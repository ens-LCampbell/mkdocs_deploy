{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Ensembl-genomio","text":"<p>A repository dedicated to pipelines used to turn basic genomic data into formatted  Ensembl core databases. Also allow users to dump core databases into various formats.</p> <p>File formats handled : FastA, gff3, JSON (following BRC4 specifications).</p> <p>Check out the usage section for further information of requirements to run ensembl-genomio pipelines.</p>"},{"location":"#ehive-pipelines","title":"Ehive pipelines","text":"<ol> <li>Genome loader: Creates an Ensembl core database from a set of flat files.</li> <li>Genome dumper: Dumps flat files from an Ensembl core database.</li> </ol>"},{"location":"#contents","title":"Contents","text":"<p>Check out installation section for further information on how  to install the project.</p> <ol> <li>Usage</li> <li>Install</li> <li>License</li> </ol>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>ensembl/\n\u251c\u2500\u2500 brc4\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 runnable\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 compare_fasta.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 compare_report.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 core_server.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 download_genbank.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 dump_stable_ids.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 extract_from_gb.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 fill_metadata.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 gff3_specifier.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 integrity.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 json_schema_factory.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 load_sequence_data.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 manifest.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 manifest_stats.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 prepare_genome.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 read_json.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 say_accession.py\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 seqregion_parser.py\n\u2514\u2500\u2500 io\n    \u2514\u2500\u2500 genomio\n        \u251c\u2500\u2500 assembly\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 get_assembly_data.py\n        \u251c\u2500\u2500 db_factory.py\n        \u251c\u2500\u2500 events\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 format_events.py\n        \u251c\u2500\u2500 events_dumper.py\n        \u251c\u2500\u2500 events_loader.py\n        \u251c\u2500\u2500 fastaprep\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 process_fasta.py\n        \u251c\u2500\u2500 genbank\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 extract_from_genbank.py\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 get_genbank.py\n        \u251c\u2500\u2500 genome_metadata\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 compare_genome_stats.py\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 dump_genome_metadata.py\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 dump_genome_stats.py\n        \u251c\u2500\u2500 gff3\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 functional_annotation.py\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 process_gff3.py\n        \u251c\u2500\u2500 integrity.py\n        \u251c\u2500\u2500 manifest_maker.py\n        \u251c\u2500\u2500 manifest_stats.py\n        \u251c\u2500\u2500 metadata\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 prepare_genome.py\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 prepare_seq_region.py\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 update_genome_metadata.py\n        \u251c\u2500\u2500 schemas\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 json_schema_factory.py\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 json_schema_validator.py\n        \u251c\u2500\u2500 seq_region_dumper.py\n        \u2514\u2500\u2500 utils\n            \u251c\u2500\u2500 archive_utils.py\n            \u2514\u2500\u2500 json_utils.py\n</code></pre>"},{"location":"#project-module-overview","title":"Project Module Overview","text":""},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>I want to thank my house plants for providing me with a negligible amount of oxygen each day. Also, I want to thank the sun for providing more than half of their nourishment free of charge.</p>"},{"location":"#indices-and-tables","title":"Indices and tables","text":"<p>"},{"location":"#mkdocs","title":"MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"install/","title":"API Setup and installation","text":""},{"location":"install/#requirements","title":"Requirements","text":"<p>An Ensembl API checkout including:</p> <ul> <li>ensembl-genomio  (export /src/perl and /src/python into PERL5LIB and PYTHONPATH respectively)</li> <li>ensembl-hive</li> <li>ensembl-production</li> <li>ensembl-analysis (on dev/hive_master branch)</li> <li>ensembl-taxonomy</li> <li>ensembl-orm</li> </ul>"},{"location":"install/#software","title":"Software","text":"<ul> <li>Python 3.8+</li> <li>Bioperl 1.6.9+</li> </ul>"},{"location":"install/#python-modules","title":"Python Modules","text":"<ul> <li>bcbio-gff</li> <li>biopython</li> <li>jsonschema</li> <li>mysql-connector-python</li> <li>requests</li> <li>python-redmine</li> </ul>"},{"location":"install/#installation","title":"Installation","text":""},{"location":"install/#directly-from-github","title":"Directly from GitHub:","text":"<pre><code>git clone https://github.com/Ensembl/ensembl-genomio\ngit clone https://github.com/Ensembl/ensembl-analysis -b dev/hive_master\ngit clone https://github.com/Ensembl/ensembl-production\ngit clone https://github.com/Ensembl/ensembl-hive\ngit clone https://github.com/Ensembl/ensembl-taxonomy\ngit clone https://github.com/Ensembl/ensembl-orm\n</code></pre>"},{"location":"license/","title":"License","text":""},{"location":"license/#license","title":"License","text":"<p>Apache License    Version 2.0, January 2004    http://www.apache.org/licenses/</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"{}\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre> <p>Copyright [yyyy] [name of copyright owner]</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"},{"location":"nextflow/","title":"Nextflow related documentation","text":""},{"location":"nextflow/#installation","title":"Installation","text":"<p>If you don't have an installed environment or you don't have nextflow itself, here's one of the ways to install it.</p> <p>Define <code>NXF_HOME</code> env variable to use a nextlow home location instead of the default one (<code>$HOME/.nextflow</code>). Everything else is unchanged from the default Nextflow installation instructions on https://www.nextflow.io/index.html#GetStarted.</p> <pre><code># add NXF_HOME env\nexport NXF_HOME=$(pwd)/dot.nextflow # or whatever\n\n# get nextflow and install almost like here: https://www.nextflow.io/index.html#GetStarted\nwget -O - https://get.nextflow.io  &gt; nextflow.install.bash\n\n# review and run\ncat nextflow.install.bash | bash -i 2&gt;&amp;1 | tee nextflow.install.log\n\n# run test, see https://www.nextflow.io/index.html#GetStarted\n./nextflow run hello\n</code></pre> <p>Configure the environment you're using if you haven't done so yet. Don't forget to add <code>NXF_HOME</code>, patch <code>PATH</code> and export them.</p> <pre><code># fix env variables, i.e.:\nexport NXF_HOME=$(pwd)/dot.nextflow\nexport PATH=$(pwd):$PATH\n</code></pre> <p>If you wish, you can set <code>NXF_WORK</code> env to be used by <code>nextflow</code>.</p> <pre><code>export NXF_WORK=...\n</code></pre> <p>Or use <code>nextflow -e.NXF_WORK=...</code> approach. Ideally, should be overridable by the <code>-work-dir</code> (<code>-w</code>) option of <code>nextflow run</code></p>"},{"location":"nextflow/#running-a-pipeline","title":"Running a pipeline","text":"<p>Once you have production (and nextflow) env ready, you can run pipelines. I.e.</p> <pre><code>CMD=&lt;dba_alias&gt;\n\nmkdir -p data\npushd data\n  data_dir=$(pwd)\n  nextflow run \\\n    -w ${data_dir}/nextflow_work \\\n    ${ENSEMBL_ROOT_DIR}/ensembl-genomio/pipelines/nextflow/workflows/dumper_pipeline/main.nf \\\n    -profile lsf \\\n    $(${CMD} details script) \\\n    --dbname_re '^drosophila_melanogaster_\\w+_57_.*$' \\\n    --output_dir ${data_dir}/dumper_output\npopd\n</code></pre> <p>Try to invoke pipelines with <code>--help</code> option to get insight on how to run them.</p>"},{"location":"nextflow/#strange-things-we-met","title":"Strange things we met","text":""},{"location":"nextflow/#channel-is-not-forked-only-one-operation-on-stream-is-allowed","title":"Channel is not forked, only one operation on stream is allowed","text":""},{"location":"nextflow/#symptoms","title":"Symptoms:","text":"<p>When running a stage or a subworkflow on a channel with a single element we expect stream to be forked, allowing us to seed several task at a time.</p> <pre><code>// create that channel with a single element\n//   calls read_json(...) in turn, see below\ndbs = from_read_json(...)\n\nDUMP_SQL(..., dbs, ...)\nDUMP_METADATA(..., dbs, ...)\n</code></pre> <p>Instead pipeline dies with</p> <pre><code>Caused by: Cannot load from object array because \"this.keys\" is null\n</code></pre> <p>and when printing this object (<code>dbs</code> in this case, with <code>println \"db: ${db}\"</code>), we see it dict surronded by the curly brackets like this</p> <pre><code>{..., \"db_name\":\"some_db_name\", ...}\n</code></pre> <p>instead of this (with square brackets)</p> <pre><code>[..., \"db_name\":\"some_db_name\", ...]\n</code></pre>"},{"location":"nextflow/#reason-solution","title":"Reason / solution","text":"<p>In our case we used the <code>read_json</code> function similar to this one:</p> <pre><code>def read_json(json_path) {\n    slurp = new JsonSlurper()\n    json_file = file(json_path)\n    text = json_file.text\n    return slurp.parseText(text) // &lt;-- problem here\n}\n</code></pre> <p>that returned some kind of a lazy evaluator/iterator/whatever(not sure).</p> <p>Replacing <code>return slurp.parseText(text)</code> with</p> <pre><code>    not_a_lazy_val = slurp.parseText(text)\n    return not_a_lazy_val\n</code></pre> <p>did help.</p>"},{"location":"pipelines/","title":"Ensembl Genomio Pipelines:","text":""},{"location":"pipelines/#genomio-prepare-pipeline","title":"Genomio prepare pipeline","text":"<p>Module [Bio::EnsEMBL::Pipeline::PipeConfig::BRC4_genome_prepare_conf]</p> <p>Genome prepare pipeline for BRC/Metazoa</p>"},{"location":"pipelines/#description","title":"Description","text":"<p>Retrieve data for a genome from INSDC and prepare the following files in a separate folder for each genome:</p> <ul> <li>FASTA for DNA sequences</li> <li>FASTA for protein sequences</li> <li>GFF gene models</li> <li>JSON functional annotation</li> <li>JSON seq_region</li> <li>JSON genome</li> <li>JSON manifest</li> </ul> <p>The JSON files follow the schemas defined in the /schemas folder.</p> <p>These files can then be fed to the Genome loader pipeline.</p>"},{"location":"pipelines/#how-to-run","title":"How to run","text":"<pre><code>init_pipeline.pl Bio::EnsEMBL::Pipeline::PipeConfig::BRC4_genome_prepare_conf \\\n    --host $HOST --port $PORT --user $USER --pass $PASS \\\n    --hive_force_init 1 \\\n    --pipeline_dir temp/prepare \\\n    --data_dir $INPUT \\\n    --output_dir $OUTPUT \\\n    ${OTHER_OPTIONS}\n</code></pre>"},{"location":"pipelines/#parameters","title":"Parameters","text":"option default value meaning <code>--pipeline_name</code> brc4_genome_prepare name of the hive pipeline <code>--pipeline_dir</code> temp directory for this pipeline run <code>--data_dir</code> directory with json files for each genome to prepare, following the format set by schemas/genome_schema.json <code>--output_dir</code> directory where the prepared files are to be stored <code>--merge_split_genes</code> 0 Sometimes the gene features are split in a gff file. Ensembl expects genes to be contiguous, so this option merge the parts into 1. <code>--exclude_seq_regions</code> Do not include those seq_regions (apply to all genomes, this should be seldom used) <code>--validate_gene_id</code> 0 Enforce a strong gene ID pattern (replace by GeneID if available) <code>--ensembl_mode</code> 0 By default, set additional metadata for BRC genomes. With this parameter, use vanilla Ensembl metadata."},{"location":"usage/","title":"Usage","text":"<p>For full details on python modules and Ensembl API repositories required see install section.</p>"},{"location":"usage/#environment-setup","title":"Environment setup","text":"<pre><code>python3.7 -m venv path/to/virtual_env\n. path/to/virtual_env/bin/activate\npip install -e .\n</code></pre> <p>Do not forget to load this environment by sourcing virtual_env/bin/activate in order to run ensembl-genomio pipeline(s).</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>src<ul> <li>ensembl<ul> <li>brc4<ul> <li>runnable<ul> <li>compare_fasta</li> <li>compare_report</li> <li>core_server</li> <li>download_genbank</li> <li>dump_stable_ids</li> <li>extract_from_gb</li> <li>fill_metadata</li> <li>gff3_specifier</li> <li>integrity</li> <li>json_schema_factory</li> <li>load_sequence_data</li> <li>manifest</li> <li>manifest_stats</li> <li>prepare_genome</li> <li>read_json</li> <li>say_accession</li> <li>seqregion_parser</li> </ul> </li> </ul> </li> <li>io<ul> <li>genomio<ul> <li>assembly<ul> <li>download</li> </ul> </li> <li>database<ul> <li>factory</li> </ul> </li> <li>events<ul> <li>dump</li> <li>format</li> <li>load</li> </ul> </li> <li>fasta<ul> <li>process</li> </ul> </li> <li>genbank<ul> <li>download</li> <li>extract_data</li> </ul> </li> <li>genome_metadata<ul> <li>dump</li> <li>extend</li> <li>prepare</li> </ul> </li> <li>genome_stats<ul> <li>compare</li> <li>dump</li> </ul> </li> <li>gff3<ul> <li>extract_annotation</li> <li>process</li> </ul> </li> <li>manifest<ul> <li>check_integrity</li> <li>compute_stats</li> <li>generate</li> </ul> </li> <li>schemas<ul> <li>json<ul> <li>factory</li> <li>validate</li> </ul> </li> </ul> </li> <li>seq_region<ul> <li>dump</li> <li>prepare</li> </ul> </li> <li>utils<ul> <li>archive_utils</li> <li>json_utils</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li>example<ul> <li>calculate<ul> <li>calculations</li> </ul> </li> <li>google_docs<ul> <li>google_docs_eg</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/ensembl/brc4/","title":"brc4","text":""},{"location":"reference/ensembl/brc4/runnable/","title":"runnable","text":""},{"location":"reference/ensembl/brc4/runnable/compare_fasta/","title":"compare_fasta","text":""},{"location":"reference/ensembl/brc4/runnable/compare_fasta/#src.ensembl.brc4.runnable.compare_fasta.compare_fasta","title":"<code>compare_fasta</code>","text":"<p>             Bases: <code>BaseRunnable</code></p> Source code in <code>src/ensembl/brc4/runnable/compare_fasta.py</code> <pre><code>class compare_fasta(eHive.BaseRunnable):\n    def param_defaults(self):\n        return {}\n\n    def run(self) -&gt; None:\n        report = self.param_required(\"report\")\n        fasta1 = self.param_required(\"fasta1\")\n        fasta2 = self.param_required(\"fasta2\")\n        map_dna_path = self.param_required(\"seq_regions\")\n        output_dir = self.param_required(\"output_dir\")\n        species = self.param_required(\"species\")\n        name = self.param_required(\"comparison_name\")\n        accession = self.param_required(\"accession\")\n\n        map_dna = self.get_map(map_dna_path)\n        seq1 = self.get_fasta(fasta1, map_dna)\n        seq2 = self.get_fasta(fasta2, map_dna)\n\n        (stats, diffs, seq_map) = self.compare_seqs(seq1, seq2)\n        # Print mapping to a file (add report data)\n        map_file = output_dir + \"/\" + species + \"_\" + name + \".map\"\n        self.print_map(seq_map, map_file, report, accession)\n\n        # Print full list of results in a file\n        output_file = output_dir + \"/\" + species + \"_\" + name + \".log\"\n        print(f\"Write results in {output_file}\")\n        with open(output_file, \"w\") as out_fh:\n            for line in diffs:\n                out_fh.write(line + \"\\n\")\n\n        # Print the stats separately\n        out = {\"species\": species, \"stats\": stats}\n        self.dataflow(out, 2)\n\n    def print_map(self, seq_map: dict, map_file: str, report_file: str, accession: str) -&gt; None:\n        report_parser = SeqregionParser()\n        report_seq = report_parser.get_report_regions(report_file, accession)\n        report = self.add_report_to_map(seq_map, report_seq)\n\n        print(f\"Write map in {map_file}\")\n        with open(map_file, \"w\") as out_fh:\n            out_fh.write(json.dumps(report, sort_keys=True, indent=4))\n\n    def add_report_to_map(self, seq_map: dict, report_seq: dict) -&gt; List[Any]:\n        accession_version = r\"\\.\\d+$\"\n        report = []\n        for insdc_name, old_name in seq_map.items():\n            if insdc_name not in report_seq:\n                raise Exception(\"No INSDC %s found in report\" % insdc_name)\n            else:\n                seqr = report_seq[insdc_name]\n                seqr[\"name\"] = old_name\n                seqr[\"EBI_seq_region_name\"] = old_name\n                brc4_name = insdc_name\n                brc4_name = re.sub(accession_version, \"\", brc4_name)\n                seqr[\"BRC4_seq_region_name\"] = brc4_name\n                syns = [{\"source\": \"INSDC\", \"name\": insdc_name}]\n                seqr[\"synonyms\"] = syns\n                report.append(seqr)\n\n        return report\n\n    def get_map(self, map_path: str) -&gt; dict:\n        print(f\"Read file {map_path}\")\n        data = self.get_json(map_path)\n\n        map_dna = {}\n\n        for seqr in data:\n            name = seqr[\"name\"]\n            if \"synonyms\" in seqr:\n                for syn in seqr[\"synonyms\"]:\n                    if syn[\"name\"] == \"INSDC\":\n                        map_dna[name] = syn[\"value\"]\n\n        return map_dna\n\n    def get_json(self, json_path: str) -&gt; dict:\n        with open(json_path) as json_file:\n            return json.load(json_file)\n\n    def build_seq_dict(self, seqs: dict) -&gt; dict:\n        \"\"\"Build a seq dict taking duplicates into account\"\"\"\n\n        seqs_dict = dict()\n        for name, seq in seqs.items():\n            if seq in seqs_dict:\n                seqs_dict[seq].add_id(name)\n            else:\n                seqs_dict[seq] = SeqGroup(seq, name)\n\n        return seqs_dict\n\n    def get_fasta(self, fasta_path: str, map_dna: dict) -&gt; dict:\n        print(f\"Read file {fasta_path}\")\n        sequences = {}\n        with open_gz_file(fasta_path) as fasta_fh:\n            for rec in SeqIO.parse(fasta_fh, \"fasta\"):\n                name = rec.id\n                if name in map_dna:\n                    name = map_dna[name]\n                sequences[name] = re.sub(r\"[^CGTA]\", \"N\", str(rec.seq.upper()))\n        return sequences\n\n    def compare_seqs(self, seq1: dict, seq2: dict) -&gt; Tuple[dict, list, dict]:\n        comp = []\n        accession = self.param_required(\"accession\")\n        diff = abs(len(seq1) - len(seq2))\n        stats = {\n            \"accession\": accession,\n            \"seq_count_1\": len(seq1),\n            \"seq_count_2\": len(seq2),\n            \"num_diff_seq\": diff,\n            \"common\": 0,\n            \"only1\": 0,\n            \"only2\": 0,\n            \"max_only1\": 0,\n            \"max_only2\": 0,\n            \"only1_200\": 0,\n            \"only1_1000\": 0,\n            \"only2_200\": 0,\n            \"only2_1000\": 0,\n            \"other_locations\": 0,\n            \"summary\": None,\n            \"organellar_summary\": None,\n            \"Assembly_level_1\": None,\n            \"Assembly_level_2\": None,\n        }\n        value = \"identical\"  # variable used for summary\n        org_value = \"no_organelles_present\"  # variable used for organellar_summary\n\n        # Compare sequences\n        seqs1 = self.build_seq_dict(seq1)\n        seqs2 = self.build_seq_dict(seq2)\n\n        # Compare number of sequences\n        if len(seq1) != len(seq2):\n            comp.append(f\"WARNING: Different number of sequences: {len(seq1)} vs {len(seq2)}\")\n        else:\n            comp.append(f\"Same number of sequences: {len(seq1)}\")\n\n        # Sequences that are not common\n        only1 = {seq: group for seq, group in seqs1.items() if not seq in seqs2}\n\n        only2 = {seq: group for seq, group in seqs2.items() if not seq in seqs1}\n\n        common, group_comp = self.find_common_groups(seqs1, seqs2)\n        comp += group_comp\n\n        if only1 or only2:\n            value = \"mismatch\"\n\n        # Gathering the organellar sequences\n        report = self.param_required(\"report\")\n        report_parser = SeqregionParser()\n        report_seq = report_parser.get_report_regions(report, accession)\n        map_dna_path = self.param_required(\"seq_regions\")\n        seq_data = self.get_json(map_dna_path)\n        org_loc = self.organellar_assembly(report_seq, seq_data)\n        INSDC_assembly_level, core_assembly_level = self.assembly_level(report_seq, seq_data)\n\n        comp.append(f\"Assembly level: {INSDC_assembly_level} vs {core_assembly_level}\")\n\n        names_length = {}\n        # sequences which have extra N at the end\n        if only1 and only2:\n            for seq_1, name1 in only1.items():\n                len1 = len(seq_1)\n                seq1_N = seq_1.count(\"N\")\n                for seq_2, name2 in only2.items():\n                    len2 = len(seq_2)\n                    seq2_N = seq_2.count(\"N\")\n                    sequence_2 = seq_2[:len1]\n                    if sequence_2 == seq_1:\n                        ignored_seq = seq_2[len1:]\n                        N = ignored_seq.count(\"N\")\n                        if len(ignored_seq) == N:\n                            comp.append(f\"Please check extra Ns added in core in {name1} and {name2}\")\n                        else:\n                            comp.append(\n                                f\"ALERT INSERTIONS at the end or diff assembly level {name1} and {name2}\"\n                            )\n                    elif len1 == len2:\n                        if seq2_N &gt; seq1_N:\n                            comp.append(f\"Core has more Ns, check {name1} and {name2}\")\n                        elif seq1_N &gt; seq2_N:\n                            comp.append(f\"INSDC has more Ns, check {name1} and {name2}\")\n                        else:\n                            names_length[name1] = name2\n                    else:\n                        continue\n\n        if names_length:\n            length = len(names_length)\n            comp.append(f\"{length} sequences have the same length\")\n            for insdc, core in names_length.items():\n                comp.append(f\"INSDC: {insdc} and coredb : {core}\")\n\n        # Remove the duplicates\n        for org_name in list(org_loc.keys()):\n            for insdc_id, core_id in common.items():\n                if org_name == core_id:\n                    org_loc.pop(org_name)\n\n        # checking for multiple entries of organellar seq\n        multi_org = [name.split(\".\")[0] for name in org_loc.keys()]\n        multi_org_acc = [j[:-1] for j in multi_org]  # similar accession\n        unique_org_id = list(set(multi_org_acc))\n        location = [location for location in org_loc.values()]\n        unique_location = location.count(\"mitochondrial_chromosome\")\n        unique_apicoplast = location.count(\"apicoplast_chromosome\")\n\n        only1_id = [str(id1) for id1 in only1.values()]\n\n        # comparing organellar sequences with common, only1 and only2\n        count = 0\n        for org_name, loc in org_loc.items():\n            if org_name == \"na\":\n                comp.append(\"MISSING accession in the report (na)\")\n            else:\n                if org_name in common.keys():\n                    count = count + 1\n                    comp.append(f\"{org_name} (both) in location: {loc}\")\n                    if count &gt; 0:\n                        org_value = \"identical\"\n                elif org_name in only1_id:\n                    count = count + 1\n                    comp.append(f\"{org_name} (only1) in  location: {loc}\")\n                    org_value = \"unknown_with_organellar\"\n                else:\n                    count = count + 1\n                    comp.append(f\"{org_name} (only2) in location: {loc}\")\n                    org_value = \"unknown_with_organellar\"\n\n        # if the mistmatch is due to added organellar sequences\n        if len(seqs1) &gt; len(seqs2):\n            greater_len = len(seq1)\n        else:\n            greater_len = len(seq2)\n\n        diff_common = greater_len - len(common)\n        diff = abs(len(only1) + len(only2))\n\n        if diff != 0:\n            if diff == count and diff_common == count:\n                org_value = \"organellar_present\"\n\n        if count == 0:\n            org_value = \"no_organelles_present\"\n\n        # checking if multiple entries of organellar sequences are present\n        if len(multi_org_acc) != len(unique_org_id):\n            if unique_location &gt; 1 or unique_apicoplast &gt; 1:\n                org_value = \"WARNING:Multiple_entry\"\n\n        # updating the stats\n        stats[\"num_diff_seq\"] = diff\n        stats[\"common\"] = len(common)\n        stats[\"only1\"] = len(only1)\n        stats[\"only2\"] = len(only2)\n        stats[\"other_locations\"] = count\n        stats[\"summary\"] = value\n        stats[\"organellar_summary\"] = org_value\n        stats[\"Assembly_level_1\"] = INSDC_assembly_level\n        stats[\"Assembly_level_2\"] = core_assembly_level\n        print(stats)\n\n        if only1:\n            stats[\"max_only1\"] = len(max(only1, key=lambda k: len(k)))\n            # Only list sequences where the length is &gt; 200\n            mini = {seq: name for seq, name in only1.items() if len(seq) &lt;= 200}\n            maxi = {seq: name for seq, name in only1.items() if len(seq) &gt; 200}\n\n            if mini and len(mini) &gt; 3000:\n                comp.append(f\"WARNING: Ignoring {len(mini)} sequences from 1 with length &lt;= 200\")\n                only1 = maxi\n\n        if only1:\n            # Only list sequences where the length is &gt; 1000\n            mini = {seq: name for seq, name in only1.items() if len(seq) &lt;= 1000}\n            maxi = {seq: name for seq, name in only1.items() if len(seq) &gt; 1000}\n            if mini and len(mini) &gt; 3000:\n                comp.append(f\"WARNING: Ignoring {len(mini)} sequences from 1 with length &lt;= 1000\")\n                only1 = maxi\n\n        if only1:\n            total = sum([len(seq) for seq in only1.keys()])\n            comp.append(f\"WARNING: Sequences only in 1: {len(only1)} ({total})\")\n            only_seq1 = {name: len(seq) for seq, name in only1.items()}\n            for name, length in sorted(only_seq1.items(), key=lambda x: x[1]):\n                comp.append(f\"\\tOnly in 1: {name} ({length})\")\n\n        if only2:\n            stats[\"max_only2\"] = len(max(only2, key=lambda k: len(k)))\n            # Only list sequences where the length is &gt; 200\n            mini = {seq: name for seq, name in only2.items() if len(seq) &lt;= 200}\n            maxi = {seq: name for seq, name in only2.items() if len(seq) &gt; 200}\n\n            if mini and len(mini) &gt; 3000:\n                comp.append(f\"WARNING: Ignoring {len(mini)} sequences from 2 with length &lt;= 200\")\n                only2 = maxi\n\n        if only2:\n            # Only list sequences where the length is &gt; 1000\n            mini = {seq: name for seq, name in only2.items() if len(seq) &lt;= 1000}\n            maxi = {seq: name for seq, name in only2.items() if len(seq) &gt; 1000}\n\n            if mini and len(mini) &gt; 3000:\n                comp.append(f\"WARNING: Ignoring {len(mini)} sequences from 2 with length &lt;= 1000\")\n                only2 = maxi\n\n        if only2:\n            total = sum([len(seq) for seq in only2.keys()])\n            comp.append(f\"WARNING: Sequences only in 2: {len(only2)} ({total})\")\n            only_seq2 = {name: len(seq) for seq, name in only2.items()}\n            for name, length in sorted(only_seq2.items(), key=lambda x: x[1]):\n                comp.append(f\"\\tOnly in 2: {name} ({length})\")\n\n        return (stats, comp, common)\n\n    def find_common_groups(self, seqs1: dict, seqs2: dict) -&gt; Tuple[dict, List[Any]]:\n        print(len(seqs1))\n        print(len(seqs2))\n        comp = []\n        common = {}\n        for seq1, group1 in seqs1.items():\n            if seq1 in seqs2:\n                group2 = seqs2[seq1]\n                # Check that the 2 groups have the same number of sequences\n                if group1.count == group2.count:\n                    if group1.count == 1:\n                        common[group1.ids[0]] = group2.ids[0]\n                    else:\n                        comp.append(f\"Matched 2 identical groups of sequences: {group1} and {group2}\")\n                        possible_id2 = \" OR \".join(group2.ids)\n                        for id1 in group1.ids:\n                            common[id1] = possible_id2\n\n                else:\n                    comp.append(\n                        f\"Matched 2 different groups of sequences ({group1.count} vs {group2.count}): {group1} and {group2}\"\n                    )\n\n        print(len(common))\n        return common, comp\n\n    def organellar_assembly(self, report_seq: dict, data: List[dict]) -&gt; dict:\n        org_loc = {}\n\n        # Gathering data from the INSDC report file and storing it into a list\n        for name1, details1 in report_seq.items():\n            if \"location\" in details1:\n                if details1[\"location\"] not in (\n                    \"chromosome\",\n                    \"nuclear_chromosome\",\n                    \"linkage_group\",\n                ):\n                    loc = details1[\"location\"]\n                    org_loc[name1] = loc\n\n        # Gathering data from Seq_json file and storing it into a list\n        for rep in data:\n            for name2, details2 in rep.items():\n                if \"location\" in name2:\n                    if details2 not in (\n                        \"chromosome\",\n                        \"nuclear_chromosome\",\n                        \"linkage_group\",\n                    ):\n                        name = rep[\"BRC4_seq_region_name\"]\n                        org_loc[name] = details2\n\n        return org_loc\n\n    def assembly_level(self, report_seq: dict, core_data: list) -&gt; Tuple[str, str]:\n        INSDC_assembly_level = []\n        core_assembly_level = []\n        core_assembly = {}\n        scaffold_INSDC = 0\n        chromosome_INSDC = 0\n        scaffold_core = 0\n        chromosome_core = 0\n\n        for name, insdc_rep in report_seq.items():\n            if insdc_rep[\"coord_system_level\"] not in (\n                \"chromosome\",\n                \"nuclear_chromosome\",\n            ):\n                scaffold_INSDC += 1\n            else:\n                chromosome_INSDC += 1\n\n        INSDC_assembly_level.extend([scaffold_INSDC, chromosome_INSDC])\n\n        for core_details in core_data:\n            name = core_details[\"BRC4_seq_region_name\"]\n            coord_system_level = core_details[\"coord_system_level\"]\n            core_assembly[name] = coord_system_level\n\n        for name, coord_level in core_assembly.items():\n            if coord_level not in (\"chromosome\", \"nuclear_chromosome\"):\n                scaffold_core += 1\n            else:\n                chromosome_core += 1\n\n        core_assembly_level.extend([scaffold_core, chromosome_core])\n\n        INSDC_assembly_level = \", \".join([str(assembly) for assembly in INSDC_assembly_level])\n        core_assembly_level = \", \".join([str(assembly) for assembly in core_assembly_level])\n\n        return INSDC_assembly_level, core_assembly_level\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/compare_fasta/#src.ensembl.brc4.runnable.compare_fasta.compare_fasta.build_seq_dict","title":"<code>build_seq_dict(seqs)</code>","text":"<p>Build a seq dict taking duplicates into account</p> Source code in <code>src/ensembl/brc4/runnable/compare_fasta.py</code> <pre><code>def build_seq_dict(self, seqs: dict) -&gt; dict:\n    \"\"\"Build a seq dict taking duplicates into account\"\"\"\n\n    seqs_dict = dict()\n    for name, seq in seqs.items():\n        if seq in seqs_dict:\n            seqs_dict[seq].add_id(name)\n        else:\n            seqs_dict[seq] = SeqGroup(seq, name)\n\n    return seqs_dict\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/compare_report/","title":"compare_report","text":""},{"location":"reference/ensembl/brc4/runnable/core_server/","title":"core_server","text":""},{"location":"reference/ensembl/brc4/runnable/core_server/#src.ensembl.brc4.runnable.core_server.CoreServer","title":"<code>CoreServer</code>","text":"<p>Basic interface to a MySQL server with core databases.</p> <p>Allows to get a list of databases and provides access to them.</p> <p>To connect to a specific database: 1) Create the core server object 2) Set the database with core_server.set_database(\"dbname\") 3) Retrieve a cursor with core_server.get_cursor()</p> Source code in <code>src/ensembl/brc4/runnable/core_server.py</code> <pre><code>class CoreServer:\n    \"\"\"Basic interface to a MySQL server with core databases.\n\n    Allows to get a list of databases and provides access to them.\n\n    Attributes:\n        host\n        port\n        user\n        password (optional)\n\n    To connect to a specific database:\n    1) Create the core server object\n    2) Set the database with core_server.set_database(\"dbname\")\n    3) Retrieve a cursor with core_server.get_cursor()\n    \"\"\"\n\n    def __init__(self, host: str, port: str, user: str, password: str = \"\") -&gt; None:\n        self.host = host\n        self.port = port\n        self.user = user\n        self.password = password\n        self._connector: Any = None\n\n        # Start a connection directly\n        self.connect()\n\n    def connect(self) -&gt; None:\n        \"\"\"Create a connection to the database.\"\"\"\n        self._connector = mysql.connector.connect(\n            user=self.user, passwd=self.password, host=self.host, port=self.port\n        )\n\n    def set_database(self, db_name: str) -&gt; None:\n        self._connector.database = db_name\n\n    def get_cursor(self):\n        return self._connector.cursor()\n\n    def get_all_cores(self) -&gt; List[str]:\n        \"\"\"Query the server and retrieve all databases that look like Ensembl cores.\"\"\"\n\n        query = \"SHOW DATABASES LIKE '%_core_%'\"\n\n        cursor = self.get_cursor()\n        cursor.execute(query)\n\n        dbs = []\n        for db in cursor:\n            dbs.append(db[0])\n        return dbs\n\n    def get_cores(\n        self, prefix: str = \"\", build: str = \"\", version: str = \"\", dbname_re: str = \"\"\n    ) -&gt; List[str]:\n        \"\"\"Provide a list of core databases, filtered if requested.\n        Args:\n            prefix: filter by prefix (no _ is added automatically)\n            build: filter by build\n            version: filter by Ensembl version\n            dbname_re: filter by dbname regular expression\n\n        Returns:\n            A list of database names\n        \"\"\"\n        dbs = []\n\n        dbs = self.get_all_cores()\n\n        if prefix:\n            dbs = [db for db in dbs if db.startswith(f\"{prefix}\")]\n        if dbname_re:\n            dbname_m = re.compile(dbname_re)\n            dbs = list(filter(dbname_m.search, dbs))\n        if build:\n            dbs = [db for db in dbs if re.search(rf\"_core_{build}_\\d+_\\d+$\", db)]\n        if version:\n            dbs = [db for db in dbs if re.search(rf\"_core_\\d+_{version}_\\d+$\", db)]\n\n        return dbs\n\n    def get_db_metadata(self) -&gt; Dict[str, List]:\n        \"\"\"Retrieve all metadata from a database.\n\n        Returns:\n            A dict of with key meta_key, and value=List of meta_value.\n\n        \"\"\"\n        query = \"SELECT meta_key, meta_value FROM meta\"\n        cursor = self.get_cursor()\n        cursor.execute(query)\n\n        metadata: Dict[str, List] = {}\n        for row in cursor:\n            meta_key, meta_value = row\n            if meta_key in metadata:\n                metadata[meta_key].append(meta_value)\n            else:\n                metadata[meta_key] = [meta_value]\n\n        return metadata\n\n    def get_table_data(self, table: str, fields: List[str], constraints: str = \"\") -&gt; List[Dict]:\n        \"\"\"Retrieve all rows from a table.\n\n        Returns:\n            A List containing a dict for each row.\n\n        \"\"\"\n        fields_str = \", \".join(fields)\n        query = f\"SELECT {fields_str} FROM {table}\"\n        if constraints:\n            query = f\"{query} WHERE {constraints}\"\n        cursor = self.get_cursor()\n        cursor.execute(query)\n\n        rows = []\n        for row in cursor:\n            row_data = dict(zip(fields, list(row)))\n            rows.append(row_data)\n\n        return rows\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/core_server/#src.ensembl.brc4.runnable.core_server.CoreServer.connect","title":"<code>connect()</code>","text":"<p>Create a connection to the database.</p> Source code in <code>src/ensembl/brc4/runnable/core_server.py</code> <pre><code>def connect(self) -&gt; None:\n    \"\"\"Create a connection to the database.\"\"\"\n    self._connector = mysql.connector.connect(\n        user=self.user, passwd=self.password, host=self.host, port=self.port\n    )\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/core_server/#src.ensembl.brc4.runnable.core_server.CoreServer.get_all_cores","title":"<code>get_all_cores()</code>","text":"<p>Query the server and retrieve all databases that look like Ensembl cores.</p> Source code in <code>src/ensembl/brc4/runnable/core_server.py</code> <pre><code>def get_all_cores(self) -&gt; List[str]:\n    \"\"\"Query the server and retrieve all databases that look like Ensembl cores.\"\"\"\n\n    query = \"SHOW DATABASES LIKE '%_core_%'\"\n\n    cursor = self.get_cursor()\n    cursor.execute(query)\n\n    dbs = []\n    for db in cursor:\n        dbs.append(db[0])\n    return dbs\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/core_server/#src.ensembl.brc4.runnable.core_server.CoreServer.get_cores","title":"<code>get_cores(prefix='', build='', version='', dbname_re='')</code>","text":"<p>Provide a list of core databases, filtered if requested. Args:     prefix: filter by prefix (no _ is added automatically)     build: filter by build     version: filter by Ensembl version     dbname_re: filter by dbname regular expression</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of database names</p> Source code in <code>src/ensembl/brc4/runnable/core_server.py</code> <pre><code>def get_cores(\n    self, prefix: str = \"\", build: str = \"\", version: str = \"\", dbname_re: str = \"\"\n) -&gt; List[str]:\n    \"\"\"Provide a list of core databases, filtered if requested.\n    Args:\n        prefix: filter by prefix (no _ is added automatically)\n        build: filter by build\n        version: filter by Ensembl version\n        dbname_re: filter by dbname regular expression\n\n    Returns:\n        A list of database names\n    \"\"\"\n    dbs = []\n\n    dbs = self.get_all_cores()\n\n    if prefix:\n        dbs = [db for db in dbs if db.startswith(f\"{prefix}\")]\n    if dbname_re:\n        dbname_m = re.compile(dbname_re)\n        dbs = list(filter(dbname_m.search, dbs))\n    if build:\n        dbs = [db for db in dbs if re.search(rf\"_core_{build}_\\d+_\\d+$\", db)]\n    if version:\n        dbs = [db for db in dbs if re.search(rf\"_core_\\d+_{version}_\\d+$\", db)]\n\n    return dbs\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/core_server/#src.ensembl.brc4.runnable.core_server.CoreServer.get_db_metadata","title":"<code>get_db_metadata()</code>","text":"<p>Retrieve all metadata from a database.</p> <p>Returns:</p> Type Description <code>Dict[str, List]</code> <p>A dict of with key meta_key, and value=List of meta_value.</p> Source code in <code>src/ensembl/brc4/runnable/core_server.py</code> <pre><code>def get_db_metadata(self) -&gt; Dict[str, List]:\n    \"\"\"Retrieve all metadata from a database.\n\n    Returns:\n        A dict of with key meta_key, and value=List of meta_value.\n\n    \"\"\"\n    query = \"SELECT meta_key, meta_value FROM meta\"\n    cursor = self.get_cursor()\n    cursor.execute(query)\n\n    metadata: Dict[str, List] = {}\n    for row in cursor:\n        meta_key, meta_value = row\n        if meta_key in metadata:\n            metadata[meta_key].append(meta_value)\n        else:\n            metadata[meta_key] = [meta_value]\n\n    return metadata\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/core_server/#src.ensembl.brc4.runnable.core_server.CoreServer.get_table_data","title":"<code>get_table_data(table, fields, constraints='')</code>","text":"<p>Retrieve all rows from a table.</p> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>A List containing a dict for each row.</p> Source code in <code>src/ensembl/brc4/runnable/core_server.py</code> <pre><code>def get_table_data(self, table: str, fields: List[str], constraints: str = \"\") -&gt; List[Dict]:\n    \"\"\"Retrieve all rows from a table.\n\n    Returns:\n        A List containing a dict for each row.\n\n    \"\"\"\n    fields_str = \", \".join(fields)\n    query = f\"SELECT {fields_str} FROM {table}\"\n    if constraints:\n        query = f\"{query} WHERE {constraints}\"\n    cursor = self.get_cursor()\n    cursor.execute(query)\n\n    rows = []\n    for row in cursor:\n        row_data = dict(zip(fields, list(row)))\n        rows.append(row_data)\n\n    return rows\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/download_genbank/","title":"download_genbank","text":""},{"location":"reference/ensembl/brc4/runnable/download_genbank/#src.ensembl.brc4.runnable.download_genbank.DownloadError","title":"<code>DownloadError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>In case a download failed.</p> Source code in <code>src/ensembl/brc4/runnable/download_genbank.py</code> <pre><code>class DownloadError(Exception):\n    \"\"\"In case a download failed.\"\"\"\n\n    def __init__(self, msg):\n        self.msg = msg\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/download_genbank/#src.ensembl.brc4.runnable.download_genbank.download_genbank","title":"<code>download_genbank</code>","text":"<p>             Bases: <code>BaseRunnable</code></p> Source code in <code>src/ensembl/brc4/runnable/download_genbank.py</code> <pre><code>class download_genbank(eHive.BaseRunnable):\n    def param_defaults(self):\n        return {}\n\n    def run(self):\n        accession = self.param_required(\"gb_accession\")\n        main_download_dir = self.param(\"download_dir\")\n        download_dir = main_download_dir + \"/\" + accession\n\n        # Set and create dedicated dir for download\n        if not os.path.isdir(download_dir):\n            os.makedirs(download_dir)\n\n        # Download the file\n        gb_path = self.download_genbank(accession, download_dir)\n\n        output = {\"gb_file\": gb_path, \"gb_accession\": accession}\n        self.dataflow(output, 2)\n\n    @staticmethod\n    def download_genbank(accession: str, dl_dir: str) -&gt; str:\n        \"\"\"\n        Given a GenBank accession, download the corresponding file in GenBank format\n        \"\"\"\n        dl_path = os.path.join(dl_dir, accession + \".gb\")\n\n        # Don't redownload the file\n        if os.path.exists(dl_path):\n            return dl_path\n\n        # Get the list of assemblies for this accession\n        e_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n        e_params = {\n            \"db\": \"nuccore\",\n            \"rettype\": \"gbwithparts\",\n            \"retmode\": \"text\",\n        }\n        e_params[\"id\"] = accession\n\n        result = requests.get(e_url, params=e_params)\n\n        if result and result.status_code == 200:\n            with open(dl_path, \"wb\") as gff:\n                gff.write(result.content)\n            print(f\"GFF file write to {dl_path}\")\n            return dl_path\n        else:\n            raise DownloadError(f\"Could not download the genbank file: {result}\")\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/download_genbank/#src.ensembl.brc4.runnable.download_genbank.download_genbank.download_genbank","title":"<code>download_genbank(accession, dl_dir)</code>  <code>staticmethod</code>","text":"<p>Given a GenBank accession, download the corresponding file in GenBank format</p> Source code in <code>src/ensembl/brc4/runnable/download_genbank.py</code> <pre><code>@staticmethod\ndef download_genbank(accession: str, dl_dir: str) -&gt; str:\n    \"\"\"\n    Given a GenBank accession, download the corresponding file in GenBank format\n    \"\"\"\n    dl_path = os.path.join(dl_dir, accession + \".gb\")\n\n    # Don't redownload the file\n    if os.path.exists(dl_path):\n        return dl_path\n\n    # Get the list of assemblies for this accession\n    e_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n    e_params = {\n        \"db\": \"nuccore\",\n        \"rettype\": \"gbwithparts\",\n        \"retmode\": \"text\",\n    }\n    e_params[\"id\"] = accession\n\n    result = requests.get(e_url, params=e_params)\n\n    if result and result.status_code == 200:\n        with open(dl_path, \"wb\") as gff:\n            gff.write(result.content)\n        print(f\"GFF file write to {dl_path}\")\n        return dl_path\n    else:\n        raise DownloadError(f\"Could not download the genbank file: {result}\")\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/dump_stable_ids/","title":"dump_stable_ids","text":""},{"location":"reference/ensembl/brc4/runnable/dump_stable_ids/#src.ensembl.brc4.runnable.dump_stable_ids.DumpStableIDs","title":"<code>DumpStableIDs</code>","text":"<p>An processor that create events from pairs of ids and can print those events out.</p> <p>Attributes:</p> Name Type Description <code>server</code> <p>a core server set to a database, to retrieve the data from.</p> Source code in <code>src/ensembl/brc4/runnable/dump_stable_ids.py</code> <pre><code>class DumpStableIDs:\n    \"\"\"An processor that create events from pairs of ids and can print those events out.\n\n    Attributes:\n        server: a core server set to a database, to retrieve the data from.\n\n    \"\"\"\n\n    def __init__(self, server: CoreServer) -&gt; None:\n        self.server = server\n\n    def get_history(self) -&gt; List:\n        \"\"\"Retrieve all events from a database.\n\n        Returns:\n            A list of all events.\n\n        \"\"\"\n\n        sessions = self.get_mapping_sessions()\n\n        events = []\n        for session in sessions:\n            print(f\"Mapping session {session['release']}\")\n            pairs = self.get_pairs(session[\"id\"])\n            session_events = self.make_events(pairs)\n            for event in session_events:\n                event.set_release(session[\"release\"])\n                event.set_date(session[\"date\"])\n            events += session_events\n\n        # Then analyse the pairs to make events\n        return events\n\n    def print_events(self, events: List[StableIdEvent], output_file: Path) -&gt; None:\n        \"\"\"Print events in a format for BRC.\n\n        Args:\n            events: list of events for a given genome.\n            output_file: where the events will be printed.\n\n        \"\"\"\n        if not events:\n            print(\"No events to print\")\n            return\n        with output_file.open(\"w\") as out_fh:\n            for event in events:\n                event_lines = event.brc_format_2()\n                for line in event_lines:\n                    out_fh.write(line + \"\\n\")\n\n    def get_mapping_sessions(self) -&gt; List[Dict]:\n        \"\"\"Retrieve the mapping sessions from the connected database.\n\n        Returns:\n            A list of sessions, as dicts: {'id: str, 'release': str, 'date': str}.\n\n        \"\"\"\n        query = \"\"\"SELECT mapping_session_id, new_release, created\n        FROM mapping_session\n        \"\"\"\n        cursor = self.server.get_cursor()\n        cursor.execute(query)\n\n        sessions = []\n        for db in cursor:\n            date = db[2]\n            session = {\"id\": db[0], \"release\": db[1], \"date\": date}\n            sessions.append(session)\n        return sessions\n\n    def get_pairs(self, session_id: int) -&gt; List[Dict]:\n        \"\"\"Retrieve all pair of ids for a given session.\n\n        Args:\n            session_id: id of a session from the connected database.\n\n        Returns:\n            A list of all pairs of ids, as dicts: {'old_id': str, 'new_id': str}.\n\n        \"\"\"\n        query = \"\"\"SELECT old_stable_id, new_stable_id\n        FROM stable_id_event\n        WHERE (old_stable_id != new_stable_id OR old_stable_id IS NULL OR new_stable_id IS NULL)\n            AND type=\"gene\"\n            AND mapping_session_id=%s\n        GROUP BY old_stable_id, new_stable_id, mapping_session_id\n        \"\"\"\n        values = (session_id,)\n        cursor = self.server.get_cursor()\n        cursor.execute(query, values)\n\n        pairs = []\n        for db in cursor:\n            pair = {\"old_id\": db[0], \"new_id\": db[1]}\n            pairs.append(pair)\n        print(f\"{len(pairs)} stable id events\")\n        return pairs\n\n    def make_events(self, pairs: List) -&gt; List:\n        \"\"\"Given a list of pairs, create events.\n\n        Args:\n            pairs: list of dicts {'old_id': str, 'new_id': str}.\n\n        Return:\n            A list of events.\n\n        \"\"\"\n\n        from_list = {}\n        to_list = {}\n        for pair in pairs:\n            old_id = pair[\"old_id\"]\n            new_id = pair[\"new_id\"]\n            if old_id == None:\n                old_id = \"\"\n            if new_id == None:\n                new_id = \"\"\n\n            if old_id in from_list:\n                from_list[old_id].add(new_id)\n            else:\n                from_list[old_id] = set([new_id])\n\n            if new_id in to_list:\n                to_list[new_id].add(old_id)\n            else:\n                to_list[new_id] = set([old_id])\n\n        # Remove empty elements\n        for from_id in from_list:\n            from_list[from_id] = StableIdEvent.clean_set(from_list[from_id])\n        for to_id in to_list:\n            to_list[to_id] = StableIdEvent.clean_set(to_list[to_id])\n\n        events: List[StableIdEvent] = []\n        for old_id in from_list:\n            if not old_id or old_id not in from_list:\n                continue\n            event = StableIdEvent([old_id], from_list[old_id])\n            (event, from_list, to_list) = self.extend_event(event, from_list, to_list)\n            event.add_pairs(pairs)\n            events.append(event)\n\n        # Remaining events should only be new genes\n        for new_id in to_list:\n            if not new_id:\n                continue\n            event = StableIdEvent(to_list[new_id], [new_id])\n            event.add_pairs(pairs)\n            events.append(event)\n\n        stats = {}\n        for event in events:\n            name = event.get_name()\n            event.clean_pairs()\n            if not name in stats:\n                stats[name] = 1\n            else:\n                stats[name] += 1\n\n        for stat in stats:\n            print(f\"\\t{stat} = {stats[stat]}\")\n\n        return events\n\n    def extend_event(\n        self, event: StableIdEvent, from_list: Dict[str, List[str]], to_list: Dict[str, List[str]]\n    ) -&gt; Tuple[StableIdEvent, List, List]:\n        \"\"\"Given an event, aggregate ids in the 'from' and 'to' sets, to connect the whole group.\n\n        Args:\n            event: the event to extend.\n            from_list: A dict a the from ids, and their corresponding to ids.\n            to_list: A dict of the to ids, and their corresponding from ids.\n\n        Returns:\n            A tuple of the extended event, and the from_list and to_list from which the ids that\n            have been added to the event have been removed.\n\n        \"\"\"\n\n        extended = True\n\n        while extended:\n            extended = False\n\n            # Extend the group in the to ids\n            for to_id in event.to_set:\n                if to_id in to_list:\n                    to_from_ids: List[str] = to_list[to_id]\n                    # Add to the from list?\n                    for to_from_id in to_from_ids:\n                        if not to_from_id in event.from_set:\n                            event.add_from(to_from_id)\n                            extended = True\n\n            # Extend the group in the from ids\n            for from_id in event.from_set:\n                if from_id in from_list:\n                    from_to_ids = from_list[from_id]\n                    # Add to the to list?\n                    for from_to_id in from_to_ids:\n                        if not from_to_id in event.to_set:\n                            event.add_to(from_to_id)\n                            extended = True\n\n        # Clean up\n        from_list = {from_id: from_list[from_id] for from_id in from_list if from_id not in event.from_set}\n        to_list = {to_id: to_list[to_id] for to_id in to_list if to_id not in event.to_set}\n\n        return (event, from_list, to_list)\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/dump_stable_ids/#src.ensembl.brc4.runnable.dump_stable_ids.DumpStableIDs.extend_event","title":"<code>extend_event(event, from_list, to_list)</code>","text":"<p>Given an event, aggregate ids in the 'from' and 'to' sets, to connect the whole group.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>StableIdEvent</code> <p>the event to extend.</p> required <code>from_list</code> <code>Dict[str, List[str]]</code> <p>A dict a the from ids, and their corresponding to ids.</p> required <code>to_list</code> <code>Dict[str, List[str]]</code> <p>A dict of the to ids, and their corresponding from ids.</p> required <p>Returns:</p> Type Description <code>StableIdEvent</code> <p>A tuple of the extended event, and the from_list and to_list from which the ids that</p> <code>List</code> <p>have been added to the event have been removed.</p> Source code in <code>src/ensembl/brc4/runnable/dump_stable_ids.py</code> <pre><code>def extend_event(\n    self, event: StableIdEvent, from_list: Dict[str, List[str]], to_list: Dict[str, List[str]]\n) -&gt; Tuple[StableIdEvent, List, List]:\n    \"\"\"Given an event, aggregate ids in the 'from' and 'to' sets, to connect the whole group.\n\n    Args:\n        event: the event to extend.\n        from_list: A dict a the from ids, and their corresponding to ids.\n        to_list: A dict of the to ids, and their corresponding from ids.\n\n    Returns:\n        A tuple of the extended event, and the from_list and to_list from which the ids that\n        have been added to the event have been removed.\n\n    \"\"\"\n\n    extended = True\n\n    while extended:\n        extended = False\n\n        # Extend the group in the to ids\n        for to_id in event.to_set:\n            if to_id in to_list:\n                to_from_ids: List[str] = to_list[to_id]\n                # Add to the from list?\n                for to_from_id in to_from_ids:\n                    if not to_from_id in event.from_set:\n                        event.add_from(to_from_id)\n                        extended = True\n\n        # Extend the group in the from ids\n        for from_id in event.from_set:\n            if from_id in from_list:\n                from_to_ids = from_list[from_id]\n                # Add to the to list?\n                for from_to_id in from_to_ids:\n                    if not from_to_id in event.to_set:\n                        event.add_to(from_to_id)\n                        extended = True\n\n    # Clean up\n    from_list = {from_id: from_list[from_id] for from_id in from_list if from_id not in event.from_set}\n    to_list = {to_id: to_list[to_id] for to_id in to_list if to_id not in event.to_set}\n\n    return (event, from_list, to_list)\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/dump_stable_ids/#src.ensembl.brc4.runnable.dump_stable_ids.DumpStableIDs.get_history","title":"<code>get_history()</code>","text":"<p>Retrieve all events from a database.</p> <p>Returns:</p> Type Description <code>List</code> <p>A list of all events.</p> Source code in <code>src/ensembl/brc4/runnable/dump_stable_ids.py</code> <pre><code>def get_history(self) -&gt; List:\n    \"\"\"Retrieve all events from a database.\n\n    Returns:\n        A list of all events.\n\n    \"\"\"\n\n    sessions = self.get_mapping_sessions()\n\n    events = []\n    for session in sessions:\n        print(f\"Mapping session {session['release']}\")\n        pairs = self.get_pairs(session[\"id\"])\n        session_events = self.make_events(pairs)\n        for event in session_events:\n            event.set_release(session[\"release\"])\n            event.set_date(session[\"date\"])\n        events += session_events\n\n    # Then analyse the pairs to make events\n    return events\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/dump_stable_ids/#src.ensembl.brc4.runnable.dump_stable_ids.DumpStableIDs.get_mapping_sessions","title":"<code>get_mapping_sessions()</code>","text":"<p>Retrieve the mapping sessions from the connected database.</p> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>A list of sessions, as dicts: {'id: str, 'release': str, 'date': str}.</p> Source code in <code>src/ensembl/brc4/runnable/dump_stable_ids.py</code> <pre><code>def get_mapping_sessions(self) -&gt; List[Dict]:\n    \"\"\"Retrieve the mapping sessions from the connected database.\n\n    Returns:\n        A list of sessions, as dicts: {'id: str, 'release': str, 'date': str}.\n\n    \"\"\"\n    query = \"\"\"SELECT mapping_session_id, new_release, created\n    FROM mapping_session\n    \"\"\"\n    cursor = self.server.get_cursor()\n    cursor.execute(query)\n\n    sessions = []\n    for db in cursor:\n        date = db[2]\n        session = {\"id\": db[0], \"release\": db[1], \"date\": date}\n        sessions.append(session)\n    return sessions\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/dump_stable_ids/#src.ensembl.brc4.runnable.dump_stable_ids.DumpStableIDs.get_pairs","title":"<code>get_pairs(session_id)</code>","text":"<p>Retrieve all pair of ids for a given session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>int</code> <p>id of a session from the connected database.</p> required <p>Returns:</p> Type Description <code>List[Dict]</code> <p>A list of all pairs of ids, as dicts: {'old_id': str, 'new_id': str}.</p> Source code in <code>src/ensembl/brc4/runnable/dump_stable_ids.py</code> <pre><code>def get_pairs(self, session_id: int) -&gt; List[Dict]:\n    \"\"\"Retrieve all pair of ids for a given session.\n\n    Args:\n        session_id: id of a session from the connected database.\n\n    Returns:\n        A list of all pairs of ids, as dicts: {'old_id': str, 'new_id': str}.\n\n    \"\"\"\n    query = \"\"\"SELECT old_stable_id, new_stable_id\n    FROM stable_id_event\n    WHERE (old_stable_id != new_stable_id OR old_stable_id IS NULL OR new_stable_id IS NULL)\n        AND type=\"gene\"\n        AND mapping_session_id=%s\n    GROUP BY old_stable_id, new_stable_id, mapping_session_id\n    \"\"\"\n    values = (session_id,)\n    cursor = self.server.get_cursor()\n    cursor.execute(query, values)\n\n    pairs = []\n    for db in cursor:\n        pair = {\"old_id\": db[0], \"new_id\": db[1]}\n        pairs.append(pair)\n    print(f\"{len(pairs)} stable id events\")\n    return pairs\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/dump_stable_ids/#src.ensembl.brc4.runnable.dump_stable_ids.DumpStableIDs.make_events","title":"<code>make_events(pairs)</code>","text":"<p>Given a list of pairs, create events.</p> <p>Parameters:</p> Name Type Description Default <code>pairs</code> <code>List</code> <p>list of dicts {'old_id': str, 'new_id': str}.</p> required Return <p>A list of events.</p> Source code in <code>src/ensembl/brc4/runnable/dump_stable_ids.py</code> <pre><code>def make_events(self, pairs: List) -&gt; List:\n    \"\"\"Given a list of pairs, create events.\n\n    Args:\n        pairs: list of dicts {'old_id': str, 'new_id': str}.\n\n    Return:\n        A list of events.\n\n    \"\"\"\n\n    from_list = {}\n    to_list = {}\n    for pair in pairs:\n        old_id = pair[\"old_id\"]\n        new_id = pair[\"new_id\"]\n        if old_id == None:\n            old_id = \"\"\n        if new_id == None:\n            new_id = \"\"\n\n        if old_id in from_list:\n            from_list[old_id].add(new_id)\n        else:\n            from_list[old_id] = set([new_id])\n\n        if new_id in to_list:\n            to_list[new_id].add(old_id)\n        else:\n            to_list[new_id] = set([old_id])\n\n    # Remove empty elements\n    for from_id in from_list:\n        from_list[from_id] = StableIdEvent.clean_set(from_list[from_id])\n    for to_id in to_list:\n        to_list[to_id] = StableIdEvent.clean_set(to_list[to_id])\n\n    events: List[StableIdEvent] = []\n    for old_id in from_list:\n        if not old_id or old_id not in from_list:\n            continue\n        event = StableIdEvent([old_id], from_list[old_id])\n        (event, from_list, to_list) = self.extend_event(event, from_list, to_list)\n        event.add_pairs(pairs)\n        events.append(event)\n\n    # Remaining events should only be new genes\n    for new_id in to_list:\n        if not new_id:\n            continue\n        event = StableIdEvent(to_list[new_id], [new_id])\n        event.add_pairs(pairs)\n        events.append(event)\n\n    stats = {}\n    for event in events:\n        name = event.get_name()\n        event.clean_pairs()\n        if not name in stats:\n            stats[name] = 1\n        else:\n            stats[name] += 1\n\n    for stat in stats:\n        print(f\"\\t{stat} = {stats[stat]}\")\n\n    return events\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/dump_stable_ids/#src.ensembl.brc4.runnable.dump_stable_ids.DumpStableIDs.print_events","title":"<code>print_events(events, output_file)</code>","text":"<p>Print events in a format for BRC.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>List[StableIdEvent]</code> <p>list of events for a given genome.</p> required <code>output_file</code> <code>Path</code> <p>where the events will be printed.</p> required Source code in <code>src/ensembl/brc4/runnable/dump_stable_ids.py</code> <pre><code>def print_events(self, events: List[StableIdEvent], output_file: Path) -&gt; None:\n    \"\"\"Print events in a format for BRC.\n\n    Args:\n        events: list of events for a given genome.\n        output_file: where the events will be printed.\n\n    \"\"\"\n    if not events:\n        print(\"No events to print\")\n        return\n    with output_file.open(\"w\") as out_fh:\n        for event in events:\n            event_lines = event.brc_format_2()\n            for line in event_lines:\n                out_fh.write(line + \"\\n\")\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/dump_stable_ids/#src.ensembl.brc4.runnable.dump_stable_ids.StableIdEvent","title":"<code>StableIdEvent</code>","text":"<p>Represents a stable id event from one gene set version to another one. Various events: - new genes - deleted genes - merged genes (several genes to one) - split genes (one gene to several) - mixed (several genes to several)</p> <p>Attributes:</p> Name Type Description <code>from_list</code> <p>List of genes the previous gene set.</p> <code>to_list</code> <p>List of genes in the new gene set.</p> <code>release</code> <p>New gene set release name.</p> <code>date</code> <p>Date of the new gene set.</p> <code>name</code> <p>Name of the event (will be updated automatically).</p> <code>pairs</code> <p>All pair of ids for this event.</p> <p>Any gene set before 2019-09 is dubbed pre-BRC4.</p> Source code in <code>src/ensembl/brc4/runnable/dump_stable_ids.py</code> <pre><code>class StableIdEvent:\n    \"\"\"Represents a stable id event from one gene set version to another one. Various events:\n    - new genes\n    - deleted genes\n    - merged genes (several genes to one)\n    - split genes (one gene to several)\n    - mixed (several genes to several)\n\n    Attributes:\n        from_list: List of genes the previous gene set.\n        to_list: List of genes in the new gene set.\n        release: New gene set release name.\n        date: Date of the new gene set.\n        name: Name of the event (will be updated automatically).\n        pairs: All pair of ids for this event.\n\n    Any gene set before 2019-09 is dubbed pre-BRC4.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        from_list: Set[str] = [],\n        to_list: Set[str] = [],\n        release: Optional[int] = None,\n        date: Optional[datetime] = None,\n    ) -&gt; None:\n        self.from_set = self.clean_set(from_list)\n        self.to_set = self.clean_set(to_list)\n        self.release = release\n        self.date = date\n        self.name = \"\"\n        self.pairs = []\n\n    def __str__(self) -&gt; str:\n        from_str = \",\".join(self.from_set)\n        to_str = \",\".join(self.to_set)\n        return f\"From {from_str} to {to_str} = {self.get_name()} in release {self.release}\"\n\n    def brc_format_1(self) -&gt; List[str]:\n        \"\"\"Returns a list events, one line per initial ID, in the following TSV format:\n        - old gene id\n        - event name\n        - release\n        - release date\n        - list of old gene ids in the event (comma-separated)\n        - list of new gene ids in the event (comma-separated)\n\n        \"\"\"\n        from_str = \",\".join(self.from_set)\n        to_str = \",\".join(self.to_set)\n        release = self.get_full_release()\n        if self.date:\n            date = self.date.strftime(\"%Y-%m\")\n        else:\n            date = \"no_date\"\n        name = self.get_name()\n        line_list = []\n        for id in self.from_set:\n            line = [\n                id,\n                name,\n                release,\n                date,\n            ]\n            if name in (\"merge\", \"split\", \"mixed\", \"change\"):\n                line.append(from_str)\n                line.append(to_str)\n            else:\n                line += [\"\", \"\"]\n            line_list.append(\"\\t\".join(line))\n\n        if self.get_name() == \"new\":\n            new_id = self.to_set[0]\n            line = (new_id, name, release, date, \"\", \"\")\n            line_list.append(\"\\t\".join(line))\n        return line_list\n\n    def brc_format_2(self) -&gt; List[str]:\n        \"\"\"Returns a list of combination of genes, one line per combination of old_id - new_ids, in the\n        following TSV format:\n        - old gene id\n        - new gene id\n        - event name\n        - release\n        - release date\n\n        \"\"\"\n        release = self.get_full_release()\n        if self.date:\n            date = self.date.strftime(\"%Y-%m\")\n        else:\n            date = \"no_date\"\n        name = self.get_name()\n        line_list = []\n\n        for pair in self.pairs:\n            line = [\n                pair[\"old_id\"],\n                pair[\"new_id\"],\n                name,\n                release,\n                date,\n            ]\n            line_list.append(\"\\t\".join(line))\n        return line_list\n\n    @staticmethod\n    def clean_set(this_list: Set) -&gt; Set:\n        \"\"\"Removes any empty elements from a list.\n\n        Args:\n            this_list: list of items, so of which can be empty/None.\n\n        Returns:\n            The cleaned list.\n\n        \"\"\"\n        return set([id for id in this_list if id])\n\n    def add_from(self, from_id: str) -&gt; None:\n        \"\"\"Store an id in the from_set.\"\"\"\n        if from_id:\n            self.from_set.add(from_id)\n\n    def add_to(self, to_id: str) -&gt; None:\n        \"\"\"Store an id in the from_set.\"\"\"\n        if to_id:\n            self.to_set.add(to_id)\n\n    def set_release(self, release: str) -&gt; None:\n        self.release = release\n\n    def set_date(self, date: datetime) -&gt; None:\n        self.date = date\n\n    def add_pair(self, pair: Dict) -&gt; None:\n        \"\"\"Keeps a record of this pair.\n\n        Args:\n            pair: Dictionary of pairs to record, with keys \"old_id\" and \"new_id\".\n\n        Raises:\n            ValueError: When no-empty value is provided for either \"old_id\" or \"new_id\".\n\n        \"\"\"\n        if \"old_id\" in pair and pair[\"old_id\"] == None:\n            pair[\"old_id\"] = \"\"\n        if \"new_id\" in pair and pair[\"new_id\"] == None:\n            pair[\"new_id\"] = \"\"\n        if not pair[\"old_id\"] and not pair[\"new_id\"]:\n            raise ValueError(f\"Expected at least one value in the given pair {pair}\")\n        self.pairs.append(pair)\n\n    def get_full_release(self) -&gt; str:\n        \"\"\"Returns the expanded release name, pre-BRC4 or `BRC4 = build`.\"\"\"\n        release = self.release\n        date = self.date\n\n        if date and date &gt; BRC4_START_DATE:\n            release = f\"build {release}\"\n        else:\n            release = f\"pre-BRC4 {release}\"\n\n        return release\n\n    def _name_event(self) -&gt; None:\n        \"\"\"Identify the event name based on the old vs new id lists.\"\"\"\n        if not self.from_set and len(self.to_set) == 1:\n            self.name = \"new\"\n        elif not self.to_set and len(self.from_set) == 1:\n            self.name = \"deletion\"\n        elif len(self.from_set) == 1 and len(self.to_set) == 1:\n            self.name = \"change\"\n        elif len(self.from_set) == 1 and len(self.to_set) &gt; 1:\n            self.name = \"split\"\n        elif len(self.from_set) &gt; 1 and len(self.to_set) == 1:\n            self.name = \"merge\"\n        elif len(self.from_set) &gt; 1 and len(self.to_set) &gt; 1:\n            self.name = \"mixed\"\n        else:\n            raise UnsupportedEvent(f\"Event {self.from_set} to {self.to_set} is not supported\")\n\n    def clean_pairs(self) -&gt; None:\n        \"\"\"Remove the empty old pairs when the event is not 'new'.\"\"\"\n        if not self.name:\n            self._name_event()\n\n        if self.name != \"new\":\n            new_pairs = []\n            for pair in self.pairs:\n                if pair.get(\"old_id\", \"\") == \"\":\n                    continue\n                new_pairs.append(pair)\n            self.pairs = new_pairs\n\n    def get_name(self) -&gt; str:\n        \"\"\"Retrieve the name for this event, update it beforehand.\"\"\"\n        self._name_event()\n        return self.name\n\n    def add_pairs(self, pairs: List[Dict[str, str]]) -&gt; None:\n        \"\"\"Provided all the pairs, keep those that are used by this event.\n\n        Args:\n            pairs: list of pairs of ids {old_id:\"\", new_id:\"\"}.\n\n        \"\"\"\n        for pair in pairs:\n            if (pair[\"old_id\"] and pair[\"old_id\"] in self.from_set) or (\n                pair[\"new_id\"] and pair[\"new_id\"] in self.to_set\n            ):\n                # Core db contains an empty line to signify that an old id has been removed\n                # in merge/split/mixed\n                name = self.get_name()\n                if (name != \"deletion\") and not pair[\"new_id\"]:\n                    continue\n                self.add_pair(pair)\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/dump_stable_ids/#src.ensembl.brc4.runnable.dump_stable_ids.StableIdEvent.add_from","title":"<code>add_from(from_id)</code>","text":"<p>Store an id in the from_set.</p> Source code in <code>src/ensembl/brc4/runnable/dump_stable_ids.py</code> <pre><code>def add_from(self, from_id: str) -&gt; None:\n    \"\"\"Store an id in the from_set.\"\"\"\n    if from_id:\n        self.from_set.add(from_id)\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/dump_stable_ids/#src.ensembl.brc4.runnable.dump_stable_ids.StableIdEvent.add_pair","title":"<code>add_pair(pair)</code>","text":"<p>Keeps a record of this pair.</p> <p>Parameters:</p> Name Type Description Default <code>pair</code> <code>Dict</code> <p>Dictionary of pairs to record, with keys \"old_id\" and \"new_id\".</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>When no-empty value is provided for either \"old_id\" or \"new_id\".</p> Source code in <code>src/ensembl/brc4/runnable/dump_stable_ids.py</code> <pre><code>def add_pair(self, pair: Dict) -&gt; None:\n    \"\"\"Keeps a record of this pair.\n\n    Args:\n        pair: Dictionary of pairs to record, with keys \"old_id\" and \"new_id\".\n\n    Raises:\n        ValueError: When no-empty value is provided for either \"old_id\" or \"new_id\".\n\n    \"\"\"\n    if \"old_id\" in pair and pair[\"old_id\"] == None:\n        pair[\"old_id\"] = \"\"\n    if \"new_id\" in pair and pair[\"new_id\"] == None:\n        pair[\"new_id\"] = \"\"\n    if not pair[\"old_id\"] and not pair[\"new_id\"]:\n        raise ValueError(f\"Expected at least one value in the given pair {pair}\")\n    self.pairs.append(pair)\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/dump_stable_ids/#src.ensembl.brc4.runnable.dump_stable_ids.StableIdEvent.add_pairs","title":"<code>add_pairs(pairs)</code>","text":"<p>Provided all the pairs, keep those that are used by this event.</p> <p>Parameters:</p> Name Type Description Default <code>pairs</code> <code>List[Dict[str, str]]</code> <p>list of pairs of ids {old_id:\"\", new_id:\"\"}.</p> required Source code in <code>src/ensembl/brc4/runnable/dump_stable_ids.py</code> <pre><code>def add_pairs(self, pairs: List[Dict[str, str]]) -&gt; None:\n    \"\"\"Provided all the pairs, keep those that are used by this event.\n\n    Args:\n        pairs: list of pairs of ids {old_id:\"\", new_id:\"\"}.\n\n    \"\"\"\n    for pair in pairs:\n        if (pair[\"old_id\"] and pair[\"old_id\"] in self.from_set) or (\n            pair[\"new_id\"] and pair[\"new_id\"] in self.to_set\n        ):\n            # Core db contains an empty line to signify that an old id has been removed\n            # in merge/split/mixed\n            name = self.get_name()\n            if (name != \"deletion\") and not pair[\"new_id\"]:\n                continue\n            self.add_pair(pair)\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/dump_stable_ids/#src.ensembl.brc4.runnable.dump_stable_ids.StableIdEvent.add_to","title":"<code>add_to(to_id)</code>","text":"<p>Store an id in the from_set.</p> Source code in <code>src/ensembl/brc4/runnable/dump_stable_ids.py</code> <pre><code>def add_to(self, to_id: str) -&gt; None:\n    \"\"\"Store an id in the from_set.\"\"\"\n    if to_id:\n        self.to_set.add(to_id)\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/dump_stable_ids/#src.ensembl.brc4.runnable.dump_stable_ids.StableIdEvent.brc_format_1","title":"<code>brc_format_1()</code>","text":"<p>Returns a list events, one line per initial ID, in the following TSV format: - old gene id - event name - release - release date - list of old gene ids in the event (comma-separated) - list of new gene ids in the event (comma-separated)</p> Source code in <code>src/ensembl/brc4/runnable/dump_stable_ids.py</code> <pre><code>def brc_format_1(self) -&gt; List[str]:\n    \"\"\"Returns a list events, one line per initial ID, in the following TSV format:\n    - old gene id\n    - event name\n    - release\n    - release date\n    - list of old gene ids in the event (comma-separated)\n    - list of new gene ids in the event (comma-separated)\n\n    \"\"\"\n    from_str = \",\".join(self.from_set)\n    to_str = \",\".join(self.to_set)\n    release = self.get_full_release()\n    if self.date:\n        date = self.date.strftime(\"%Y-%m\")\n    else:\n        date = \"no_date\"\n    name = self.get_name()\n    line_list = []\n    for id in self.from_set:\n        line = [\n            id,\n            name,\n            release,\n            date,\n        ]\n        if name in (\"merge\", \"split\", \"mixed\", \"change\"):\n            line.append(from_str)\n            line.append(to_str)\n        else:\n            line += [\"\", \"\"]\n        line_list.append(\"\\t\".join(line))\n\n    if self.get_name() == \"new\":\n        new_id = self.to_set[0]\n        line = (new_id, name, release, date, \"\", \"\")\n        line_list.append(\"\\t\".join(line))\n    return line_list\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/dump_stable_ids/#src.ensembl.brc4.runnable.dump_stable_ids.StableIdEvent.brc_format_2","title":"<code>brc_format_2()</code>","text":"<p>Returns a list of combination of genes, one line per combination of old_id - new_ids, in the following TSV format: - old gene id - new gene id - event name - release - release date</p> Source code in <code>src/ensembl/brc4/runnable/dump_stable_ids.py</code> <pre><code>def brc_format_2(self) -&gt; List[str]:\n    \"\"\"Returns a list of combination of genes, one line per combination of old_id - new_ids, in the\n    following TSV format:\n    - old gene id\n    - new gene id\n    - event name\n    - release\n    - release date\n\n    \"\"\"\n    release = self.get_full_release()\n    if self.date:\n        date = self.date.strftime(\"%Y-%m\")\n    else:\n        date = \"no_date\"\n    name = self.get_name()\n    line_list = []\n\n    for pair in self.pairs:\n        line = [\n            pair[\"old_id\"],\n            pair[\"new_id\"],\n            name,\n            release,\n            date,\n        ]\n        line_list.append(\"\\t\".join(line))\n    return line_list\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/dump_stable_ids/#src.ensembl.brc4.runnable.dump_stable_ids.StableIdEvent.clean_pairs","title":"<code>clean_pairs()</code>","text":"<p>Remove the empty old pairs when the event is not 'new'.</p> Source code in <code>src/ensembl/brc4/runnable/dump_stable_ids.py</code> <pre><code>def clean_pairs(self) -&gt; None:\n    \"\"\"Remove the empty old pairs when the event is not 'new'.\"\"\"\n    if not self.name:\n        self._name_event()\n\n    if self.name != \"new\":\n        new_pairs = []\n        for pair in self.pairs:\n            if pair.get(\"old_id\", \"\") == \"\":\n                continue\n            new_pairs.append(pair)\n        self.pairs = new_pairs\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/dump_stable_ids/#src.ensembl.brc4.runnable.dump_stable_ids.StableIdEvent.clean_set","title":"<code>clean_set(this_list)</code>  <code>staticmethod</code>","text":"<p>Removes any empty elements from a list.</p> <p>Parameters:</p> Name Type Description Default <code>this_list</code> <code>Set</code> <p>list of items, so of which can be empty/None.</p> required <p>Returns:</p> Type Description <code>Set</code> <p>The cleaned list.</p> Source code in <code>src/ensembl/brc4/runnable/dump_stable_ids.py</code> <pre><code>@staticmethod\ndef clean_set(this_list: Set) -&gt; Set:\n    \"\"\"Removes any empty elements from a list.\n\n    Args:\n        this_list: list of items, so of which can be empty/None.\n\n    Returns:\n        The cleaned list.\n\n    \"\"\"\n    return set([id for id in this_list if id])\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/dump_stable_ids/#src.ensembl.brc4.runnable.dump_stable_ids.StableIdEvent.get_full_release","title":"<code>get_full_release()</code>","text":"<p>Returns the expanded release name, pre-BRC4 or <code>BRC4 = build</code>.</p> Source code in <code>src/ensembl/brc4/runnable/dump_stable_ids.py</code> <pre><code>def get_full_release(self) -&gt; str:\n    \"\"\"Returns the expanded release name, pre-BRC4 or `BRC4 = build`.\"\"\"\n    release = self.release\n    date = self.date\n\n    if date and date &gt; BRC4_START_DATE:\n        release = f\"build {release}\"\n    else:\n        release = f\"pre-BRC4 {release}\"\n\n    return release\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/dump_stable_ids/#src.ensembl.brc4.runnable.dump_stable_ids.StableIdEvent.get_name","title":"<code>get_name()</code>","text":"<p>Retrieve the name for this event, update it beforehand.</p> Source code in <code>src/ensembl/brc4/runnable/dump_stable_ids.py</code> <pre><code>def get_name(self) -&gt; str:\n    \"\"\"Retrieve the name for this event, update it beforehand.\"\"\"\n    self._name_event()\n    return self.name\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/extract_from_gb/","title":"extract_from_gb","text":""},{"location":"reference/ensembl/brc4/runnable/extract_from_gb/#src.ensembl.brc4.runnable.extract_from_gb.FormattedFilesGenerator","title":"<code>FormattedFilesGenerator</code>","text":"<p>Contains a parser to load data from a file, and output a set of files that follow our schema for input into a core database</p> Source code in <code>src/ensembl/brc4/runnable/extract_from_gb.py</code> <pre><code>class FormattedFilesGenerator:\n    \"\"\"\n    Contains a parser to load data from a file, and output a set of files that follow our schema for input into a core database\n    \"\"\"\n\n    locations = {\n        \"mitochondrion\": \"mitochondrial_chromosome\",\n        \"apicoplast\": \"apicoplast_chromosome\",\n        \"chloroplast\": \"chloroplast_chromosome\",\n        \"chromoplast\": \"chromoplast_chromosome\",\n        \"cyanelle\": \"cyanelle_chromosome\",\n        \"leucoplast\": \"leucoplast_chromosome\",\n    }\n\n    allowed_feat_types = [\n        \"gene\",\n        \"transcript\",\n        \"tRNA\",\n        \"rRNA\",\n        \"CDS\",\n    ]\n\n    def __init__(self, output_dir, prefix=\"\"):\n        self.output_dir = output_dir\n        try:\n            os.makedirs(self.output_dir)\n        except FileExistsError:\n            pass\n\n        self.genome = os.path.join(self.output_dir, \"genome.json\")\n        self.seq_regions = os.path.join(self.output_dir, \"seq_regions.json\")\n        self.fasta_dna = os.path.join(self.output_dir, \"dna.fasta\")\n        self.fasta_pep = os.path.join(self.output_dir, \"pep.fasta\")\n        self.genes_gff = os.path.join(self.output_dir, \"genes.gff\")\n        self.prefix = prefix\n        self.seqs = []\n\n    def set_prefix(self, prefix):\n        \"\"\"\n        Define a prefix to add to the feature IDs\n        \"\"\"\n        if prefix:\n            self.prefix = prefix\n\n    def set_production_name(self, prod_name):\n        \"\"\"\n        Define a production_name for the genome\n        \"\"\"\n        if prod_name:\n            self.prod_name = prod_name\n\n    def parse_genbank(self, gb_file):\n        \"\"\"\n        Load a sequence from a Genbank file\n        \"\"\"\n\n        organella = self._get_organella(gb_file)\n\n        with open(gb_file, \"r\") as gbh:\n            for record in SeqIO.parse(gbh, \"genbank\"):\n                # We don't want the record description (especially for the fasta file)\n                record.description = \"\"\n                record.organelle = None\n                if record.id in organella:\n                    record.organelle = organella[record.id]\n                self.seqs.append(record)\n\n            self._write_genome_json()\n            self._write_genes_gff()\n            self._write_seq_regions_json()\n            self._write_fasta_dna()\n\n    def _get_organella(self, gb_file):\n        \"\"\"\n        Retrive the organelle from the genbank file, using the specific GenBank object,\n        because SeqIO does not support this field\n        \"\"\"\n        organella = {}\n        with open(gb_file, \"r\") as gbh:\n            for record in GenBank.parse(gbh):\n                accession = record.version\n                for q in record.features[0].qualifiers:\n                    if q.key == \"/organelle=\":\n                        organelle = q.value.replace('\"', \"\")\n                        organella[record.version] = organelle\n        return organella\n\n    def _write_fasta_dna(self):\n        with open(self.fasta_dna, \"w\") as fasta_fh:\n            SeqIO.write(self.seqs, fasta_fh, \"fasta\")\n\n    def _write_genes_gff(self):\n        peptides = []\n\n        with open(self.genes_gff, \"w\") as gff_fh:\n            recs = []\n            all_ids = []\n\n            for seq in self.seqs:\n                feats = {}\n\n                for feat in seq.features:\n                    if feat.type not in self.allowed_feat_types:\n                        continue\n                    gff_qualifiers = feat.qualifiers\n                    gff_feat = SeqFeature(\n                        location=feat.location,\n                        type=feat.type,\n                        strand=feat.location.strand,\n                        qualifiers=gff_qualifiers,\n                    )\n\n                    gene_name = gff_qualifiers.get(\"gene\", [None])[0]\n                    if gene_name is None:\n                        gene_name = gff_qualifiers.get(\"locus_tag\", [None])[0]\n\n                    if gene_name is not None:\n                        gene_id = self.prefix + gene_name\n\n                        if feat.type == \"gene\":\n                            if \"pseudo\" in gff_qualifiers:\n                                gff_feat.type = \"pseudogene\"\n                            gff_feat.qualifiers[\"ID\"] = gene_id\n                            gff_feat.qualifiers[\"Name\"] = gene_name\n                            if \"gene\" in gff_feat.qualifiers:\n                                del gff_feat.qualifiers[\"gene\"]\n                            if \"locus_tag\" in gff_feat.qualifiers:\n                                del gff_feat.qualifiers[\"locus_tag\"]\n                            feats[str(gene_id)] = gff_feat\n                            all_ids.append(str(gene_id))\n\n                        if feat.type == \"CDS\":\n                            if \"pseudo\" in gff_qualifiers:\n                                gff_feat.type = \"exon\"\n                            cds_id = gene_id + \"_p1\"\n                            tr_id = gene_id + \"_t1\"\n                            gff_feat.qualifiers[\"ID\"] = cds_id\n                            gff_feat.qualifiers[\"Parent\"] = tr_id\n                            if \"gene\" in gff_feat.qualifiers:\n                                del gff_feat.qualifiers[\"gene\"]\n                            if \"locus_tag\" in gff_feat.qualifiers:\n                                del gff_feat.qualifiers[\"locus_tag\"]\n\n                            # Add fasta to pep fasta file\n                            if \"translation\" in feat.qualifiers:\n                                peptides.append(SeqRecord(Seq(feat.qualifiers[\"translation\"][0]), id=cds_id))\n\n                            # Also create a parent transcript for this translation\n                            tr_qualifiers = {\"ID\": tr_id, \"Name\": gene_name, \"Parent\": gene_id}\n                            gff_tr = SeqFeature(\n                                location=feat.location,\n                                type=\"mRNA\",\n                                strand=feat.location.strand,\n                                qualifiers=tr_qualifiers,\n                            )\n                            feats[str(tr_id)] = gff_tr\n                            feats[str(cds_id)] = gff_feat\n                            all_ids.append(str(tr_id))\n                            all_ids.append(str(cds_id))\n\n                    elif feat.type in (\"tRNA\", \"rRNA\"):\n                        feat_name = gff_qualifiers[\"product\"][0]\n                        gene_id = self.prefix + feat_name\n\n                        parts = gene_id.split(\" \")\n                        if len(parts) &gt; 2:\n                            print(f\"Shortening gene_id to {parts[0]}\")\n                            gene_id = parts[0]\n                        gene_id = self._uniquify_id(gene_id, all_ids)\n\n                        feat_id = gene_id + \"_t1\"\n                        gff_feat.qualifiers[\"ID\"] = feat_id\n                        gff_feat.qualifiers[\"Name\"] = feat_name\n                        gff_feat.qualifiers[\"Parent\"] = gene_id\n\n                        # Also create a parent gene for this transcript\n                        gene_qualifiers = {\n                            \"ID\": gene_id,\n                            \"Name\": feat_name,\n                        }\n                        gff_gene = SeqFeature(\n                            location=feat.location,\n                            type=\"gene\",\n                            strand=feat.location.strand,\n                            qualifiers=gene_qualifiers,\n                        )\n                        feats[str(gene_id)] = gff_gene\n                        feats[str(feat_id)] = gff_feat\n                        all_ids.append(str(gene_id))\n                        all_ids.append(str(feat_id))\n\n                    else:\n                        raise Exception(f\"No ID for allowed feature: {feat}\")\n\n                rec = SeqRecord(seq.seq, seq.id)\n                rec.features = feats.values()\n                recs.append(rec)\n\n            GFF.write(recs, gff_fh)\n\n            with open(self.fasta_pep, \"w\") as fasta_fh:\n                SeqIO.write(peptides, fasta_fh, \"fasta\")\n\n            # Warn if some IDs are not unique\n            count = dict(Counter(all_ids))\n            num_duplicates = 0\n            for key in count:\n                if count[key] &gt; 1:\n                    num_duplicates += 1\n                    print(f\"ID {key} is duplicated {count[key]} times\")\n            if num_duplicates &gt; 0:\n                raise Exception(f\"Some {num_duplicates} IDs are duplicated\")\n\n    def _uniquify_id(self, gene_id, all_ids):\n        \"\"\"Ensure the gene id used is unique,\n        and append a number otherwise, starting at 2\n        \"\"\"\n\n        new_id = gene_id\n        num = 1\n        while new_id in all_ids:\n            print(f\"{new_id} exists, update\")\n            num += 1\n            new_id = f\"{gene_id}_{num}\"\n        print(f\"Using {new_id}\")\n\n        return new_id\n\n    def _write_seq_regions_json(self):\n        json_array = []\n\n        for seq in self.seqs:\n            codon_table = self._get_codon_table(seq)\n            if not codon_table:\n                print(\n                    f\"Warning: No codon table found. Make sure to change the codon table number in {self.seq_regions_json} manually if it is not the standard codon table\"\n                )\n\n                codon_table = 1\n            else:\n                codon_table = int(codon_table)\n            seq_obj = {\n                \"name\": seq.id,\n                \"coord_system_level\": \"chromosome\",\n                \"circular\": (seq.annotations[\"topology\"] == \"circular\"),\n                \"codon_table\": codon_table,\n                \"length\": len(seq.seq),\n            }\n            if seq.organelle:\n                seq_obj[\"location\"] = self._prepare_location(seq.organelle)\n                if not codon_table:\n                    print(\n                        f\"Warning: '{seq.organelle}' is an organelle: make sure to change the codon table number in {self.seq_regions_json} manually if it is not the standard codon table\"\n                    )\n\n            # Additional attributes for Ensembl\n            seq_obj[\"added_sequence\"] = {\n                \"accession\": seq.id,\n                \"assembly_provider\": {\n                    \"name\": \"GenBank\",\n                    \"url\": \"https://www.ncbi.nlm.nih.gov/genbank\",\n                },\n            }\n            if not seq_obj[\"added_sequence\"][\"assembly_provider\"][\"name\"]:\n                print(\n                    f\"Warning: please add the relevant provider name for the assembly in {self.seq_regions}\"\n                )\n            if not seq_obj[\"added_sequence\"][\"assembly_provider\"][\"url\"]:\n                print(f\"Warning: please add the relevant provider url for the assembly in {self.seq_regions}\")\n\n            # Additional attributes for gene set, if any\n            # TODO\n\n            json_array.append(seq_obj)\n        with open(self.seq_regions, \"w\") as seq_fh:\n            seq_fh.write(json.dumps(json_array, indent=4))\n\n    def _get_codon_table(self, seq):\n        \"\"\"\n        Look at the CDS features to see if they have a codon table\n        \"\"\"\n        for feat in seq.features:\n            if feat.type == \"CDS\":\n                quals = feat.qualifiers\n                if \"transl_table\" in quals:\n                    return quals[\"transl_table\"][0]\n                else:\n                    return\n        return\n\n    def _prepare_location(self, organelle):\n        \"\"\"\n        Given an organelle name, returns the SO term corresponding to its location\n        \"\"\"\n        if organelle in self.locations:\n            return self.locations[organelle]\n        else:\n            raise Exception(f\"Unkown organelle: {organelle}\")\n\n    def _write_genome_json(self):\n        \"\"\"\n        Write a draft for the genome json file\n        Only the production_name is needed, but the rest of the fields need to be given\n        for the validation of the json file\n        \"\"\"\n\n        prod_name = self.prod_name if self.prod_name else \"\"\n\n        genome_data = {\n            \"species\": {\n                \"production_name\": prod_name,\n                \"taxonomy_id\": 0,\n            },\n            \"assembly\": {\"accession\": \"GCA_000000000\", \"version\": 1},\n            \"added_seq\": {},\n        }\n\n        if not genome_data[\"species\"][\"production_name\"]:\n            print(f\"Warning: please add the relevant production_name for this genome in {self.genome}\")\n\n        ids = [seq.id for seq in self.seqs]\n        genome_data[\"added_seq\"][\"region_name\"] = ids\n\n        with open(self.genome, \"w\") as genome_fh:\n            genome_fh.write(json.dumps(genome_data, indent=4))\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/extract_from_gb/#src.ensembl.brc4.runnable.extract_from_gb.FormattedFilesGenerator.parse_genbank","title":"<code>parse_genbank(gb_file)</code>","text":"<p>Load a sequence from a Genbank file</p> Source code in <code>src/ensembl/brc4/runnable/extract_from_gb.py</code> <pre><code>def parse_genbank(self, gb_file):\n    \"\"\"\n    Load a sequence from a Genbank file\n    \"\"\"\n\n    organella = self._get_organella(gb_file)\n\n    with open(gb_file, \"r\") as gbh:\n        for record in SeqIO.parse(gbh, \"genbank\"):\n            # We don't want the record description (especially for the fasta file)\n            record.description = \"\"\n            record.organelle = None\n            if record.id in organella:\n                record.organelle = organella[record.id]\n            self.seqs.append(record)\n\n        self._write_genome_json()\n        self._write_genes_gff()\n        self._write_seq_regions_json()\n        self._write_fasta_dna()\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/extract_from_gb/#src.ensembl.brc4.runnable.extract_from_gb.FormattedFilesGenerator.set_prefix","title":"<code>set_prefix(prefix)</code>","text":"<p>Define a prefix to add to the feature IDs</p> Source code in <code>src/ensembl/brc4/runnable/extract_from_gb.py</code> <pre><code>def set_prefix(self, prefix):\n    \"\"\"\n    Define a prefix to add to the feature IDs\n    \"\"\"\n    if prefix:\n        self.prefix = prefix\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/extract_from_gb/#src.ensembl.brc4.runnable.extract_from_gb.FormattedFilesGenerator.set_production_name","title":"<code>set_production_name(prod_name)</code>","text":"<p>Define a production_name for the genome</p> Source code in <code>src/ensembl/brc4/runnable/extract_from_gb.py</code> <pre><code>def set_production_name(self, prod_name):\n    \"\"\"\n    Define a production_name for the genome\n    \"\"\"\n    if prod_name:\n        self.prod_name = prod_name\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/fill_metadata/","title":"fill_metadata","text":""},{"location":"reference/ensembl/brc4/runnable/gff3_specifier/","title":"gff3_specifier","text":""},{"location":"reference/ensembl/brc4/runnable/integrity/","title":"integrity","text":""},{"location":"reference/ensembl/brc4/runnable/integrity/#src.ensembl.brc4.runnable.integrity.integrity","title":"<code>integrity</code>","text":"<p>             Bases: <code>BaseRunnable</code></p> <p>Check the integrity of sequence and annotation files in the genome</p> Source code in <code>src/ensembl/brc4/runnable/integrity.py</code> <pre><code>class integrity(eHive.BaseRunnable):\n    \"\"\"Check the integrity of sequence and annotation files in the genome\"\"\"\n\n    def param_defaults(self):\n        return {\n            \"ensembl_mode\": False,\n            \"ignore_final_stops\": False,\n        }\n\n    def run(self):\n        \"\"\"Load files listed in the manifest.json and check the integrity.\n            Check if the files are correct by verifying the MD5 hash.\n            Check if translation, functional annotation and sequence region ids\n            and lengths are consistent with the information in gff.\n            Compare sequence length from fasta_dna file to seq_region.json metadata.\n\n        Args:\n            manifest: Path to the manifest file.\n            It contains a set of files fasta, json metadata\n            and optional annotation files (gff, functional_annotation).\n\n        Returns:\n            Error if any of the above checks fail.\n        \"\"\"\n\n        manifest_path = self.param_required(\"manifest\")\n        errors = []\n        # load the manisfest.json\n        with open(manifest_path) as manifest_file:\n            manifest = json.load(manifest_file)\n\n            # Use dir name from the manifest\n            for name in manifest:\n                if \"file\" in manifest[name]:\n                    file_name = manifest[name][\"file\"]\n                    file_name = path.join(path.dirname(manifest_path), file_name)\n                    # check if the md5sum is correct\n                    md5sum = manifest[name][\"md5sum\"]\n                    errors += self.check_md5sum(file_name, md5sum)\n\n                    manifest[name] = file_name\n                else:\n                    for f in manifest[name]:\n                        if \"file\" in manifest[name][f]:\n                            file_name = manifest[name][f][\"file\"]\n                            file_name = path.join(path.dirname(manifest_path), file_name)\n                            # check if the md5sum is correct\n                            md5sum = manifest[name][f][\"md5sum\"]\n                            errors += self.check_md5sum(file_name, md5sum)\n\n                            manifest[name][f] = file_name\n\n            # Get content from the manifest file and store it into following variables\n            dna = {}\n            pep = {}\n            seq_regions = {}\n            seq_lengths = {}\n            seq_circular = {}\n            gff = {}\n            func_ann = {}\n            agp_seqr = {}\n            genome = {}\n\n            if \"gff3\" in manifest:\n                print(\"Got a gff\")\n                gff = self.get_gff3(manifest[\"gff3\"])\n            if \"fasta_dna\" in manifest:\n                print(\"Got a fasta dna\")\n                # Verify if the length and id for the sequence is unique\n                dna, dna_errors = self.get_fasta_lengths(manifest[\"fasta_dna\"])\n                errors += dna_errors\n            if \"fasta_pep\" in manifest:\n                print(\"Got a fasta pep\")\n                ignore_final_stops = self.param(\"ignore_final_stops\")\n                # Verify if the length and id for the sequence is unique\n                pep, pep_errors = self.get_fasta_lengths(\n                    manifest[\"fasta_pep\"], ignore_final_stops=ignore_final_stops\n                )\n                errors += pep_errors\n            if \"seq_region\" in manifest:\n                print(\"Got a seq_regions\")\n                seq_regions = get_json(Path(manifest[\"seq_region\"]))\n                seqr_lengths = {}\n                seqr_seqlevel = {}\n                # Store the length as int\n                for seq in seq_regions:\n                    seq_lengths[seq[\"name\"]] = int(seq[\"length\"])\n                    seq_circular[seq[\"name\"]] = seq.get(\"circular\", False)\n                    if seq[\"coord_system_level\"] == \"contig\":\n                        seqr_seqlevel[seq[\"name\"]] = int(seq[\"length\"])\n            if \"functional_annotation\" in manifest:\n                print(\"Got a func_anns\")\n                func_ann = self.get_functional_annotation(manifest[\"functional_annotation\"])\n            if \"agp\" in manifest:\n                print(\"Got agp files\")\n                agp_seqr = self.get_agp_seq_regions(manifest[\"agp\"])\n            if \"genome\" in manifest:\n                print(\"Got a genome\")\n                genome = get_json(Path(manifest[\"genome\"]))\n\n            # Check if the accession is correct in genome.json\n            if genome:\n                if \"assembly\" in genome:\n                    genome_ass = genome[\"assembly\"]\n                    if \"accession\" in genome_ass:\n                        genome_acc = genome_ass[\"accession\"]\n                        if not re.match(\"GC[AF]_\\d{9}(\\.\\d+)?\", genome_acc):\n                            errors += [\"Genome assembly accession is wrong: '%s'\" % genome_acc]\n\n            # Check gff3\n            if gff:\n                # Check fasta_pep.fa integrity\n                # The sequence length and id retrieved from the fasta_pep file and compared to the translated CDS id and length in the gff\n                # We don't compare the peptide lengths because of seqedits\n                if pep:\n                    tr_errors = self.check_lengths(\n                        pep, gff[\"translations\"], \"Fasta translations vs gff\", special_diff=True\n                    )\n                    if len(tr_errors) &gt; 0:\n                        # The pseudo CDSs are included in this check\n                        # Pseudo CDSs are not translated, if the pseudo translation ids are not ignored in the gff it will give an error\n                        tr_errors = self.check_lengths(\n                            pep,\n                            gff[\"all_translations\"],\n                            \"Fasta translations vs gff (include pseudo CDS)\",\n                            special_diff=True,\n                        )\n                        errors += tr_errors\n\n                # Check functional_annotation.json integrity\n                # Gene ids, translated CDS ids and translated CDSs including pseudogenes are compared to the gff\n                if func_ann:\n                    errors += self.check_ids(func_ann[\"genes\"], gff[\"genes\"], \"Gene ids metadata vs gff\")\n                    tr_errors = self.check_ids(\n                        func_ann[\"translations\"], gff[\"translations\"], \"Translation ids metadata vs gff\"\n                    )\n                    if len(tr_errors) &gt; 0:\n                        tr_errors = self.check_ids(\n                            func_ann[\"translations\"],\n                            gff[\"all_translations\"],\n                            \"Translation ids metadata vs gff (include pseudo CDS)\",\n                        )\n                    errors += tr_errors\n                    errors += self.check_ids(\n                        func_ann[\"transposable_elements\"],\n                        gff[\"transposable_elements\"],\n                        \"TE ids metadata vs gff\",\n                    )\n\n                # Check the seq.json intregrity\n                # Compare the length and id retrieved from seq.json to the gff\n                if seq_regions:\n                    errors += self.check_seq_region_lengths(\n                        seq_lengths, gff[\"seq_region\"], \"Seq_regions metadata vs gff\", seq_circular\n                    )\n\n            # Check fasta dna and seq_region integrity\n            if dna and seq_regions:\n                errors += self.check_seq_region_lengths(seq_lengths, dna, \"seq_regions json vs dna\")\n\n            # Check agp and seq_region integrity\n            if agp_seqr and seq_lengths:\n                errors += self.check_seq_region_lengths(seq_lengths, agp_seqr, \"seq_regions json vs agps\")\n\n        if errors:\n            errors_str = \"\\n\".join(errors)\n            raise Exception(\"Integrity test failed for %s:\\n%s\" % (manifest_path, errors_str))\n\n    def check_md5sum(self, path, md5sum):\n        \"\"\"Verify the integrity of the files in manifest.json.\n\n            An MD5 hash is generated using the path provided which is then compared to the hash\n            in manifest.json.\n\n        Args:\n            Path: The path for each file in the genome.\n            md5sum: MD5 hash for the files.\n\n        Returns:\n            Error if the md5sum does not match.\n        \"\"\"\n\n        errors = []\n        with open(path, \"rb\") as f:\n            bytes = f.read()\n            readable_hash = hashlib.md5(bytes).hexdigest()\n            if readable_hash != md5sum:\n                errors.append(\"File %s has a wrong md5sum\" % path)\n\n        return errors\n\n    def get_fasta_lengths(self, fasta_path, ignore_final_stops=False):\n        \"\"\"Check if the fasta files have the correct ids and no stop codon.\n\n        Args:\n            fasta_path: Path to fasta_dna and fasta_pep files.\n\n        Returns:\n            Error if any empty ids, non-unique ids or stop codons are found in the fasta files.\n        \"\"\"\n\n        data = {}\n        non_unique = {}\n        non_unique_count = 0\n        empty_id_count = 0\n        contains_stop_codon = 0\n        for rec in SeqIO.parse(fasta_path, \"fasta\"):\n            # Flag empty ids\n            if rec.id == \"\":\n                empty_id_count += 1\n            else:\n                # Flag redundant ids\n                if rec.id in data:\n                    non_unique[rec.id] = 1\n                    non_unique_count += 1\n                # Store sequence id and length\n                data[rec.id] = len(rec.seq)\n                stops = rec.seq.count(\"*\")\n                if stops &gt; 1:\n                    contains_stop_codon += 1\n                elif stops == 1:\n                    if not rec.seq.endswith(\"*\") or not ignore_final_stops:\n                        contains_stop_codon += 1\n\n        errors = []\n        if empty_id_count &gt; 0:\n            errors.append(\"%d sequences with empty ids in %s\" % (empty_id_count, fasta_path))\n        if non_unique_count &gt; 0:\n            errors.append(\"%d non unique sequence ids in %s\" % (non_unique_count, fasta_path))\n        if contains_stop_codon &gt; 0:\n            errors.append(\"%d sequences with stop codons in %s\" % (contains_stop_codon, fasta_path))\n        return data, errors\n\n    def get_functional_annotation(self, json_path):\n        \"\"\"Load the functional annotation file to retrieve the gene_id and translation id.\n            A functional annotation file contains information about a gene.\n            The functional annotation file is stored in a json format containing\n            the description, id and object type (eg: \"gene\", \"transcript\", \"translation\").\n\n        Args:\n            json_path: Path to functional_annotation.json.\n\n        Returns:\n            dict with gene and translation ids.\n        \"\"\"\n\n        # Load the json file\n        with open(json_path) as json_file:\n            data = json.load(json_file)\n\n            # Get gene ids and translation ids\n            genes = {}\n            translations = {}\n            tes = {}\n\n            for item in data:\n                if item[\"object_type\"] == \"gene\":\n                    genes[item[\"id\"]] = 1\n                elif item[\"object_type\"] == \"translation\":\n                    translations[item[\"id\"]] = 1\n                if item[\"object_type\"] == \"transposable_element\":\n                    tes[item[\"id\"]] = 1\n\n            return {\"genes\": genes, \"translations\": translations, \"transposable_elements\": tes}\n\n    def get_gff3(self, gff3_path):\n        # Load the gff file\n        if gff3_path.endswith(\".gz\"):\n            with io.TextIOWrapper(gzip.open(gff3_path, \"r\")) as gff3_handle:\n                return self.parse_gff3(gff3_handle)\n        else:\n            with open(gff3_path, \"r\") as gff3_handle:\n                return self.parse_gff3(gff3_handle)\n\n    def parse_gff3(self, gff3_handle):\n        \"\"\"A GFF parser is used to retrieve information in the GFF file such as\n           gene and CDS ids and their corresponding lengths.\n\n        Args:\n            gff3_handle: Path to gff3 file.\n\n        Returns:\n            dict containing sequence ids, gene ids, transcript ids and translation ids\n            are stored with their corresponding lengths.\n        \"\"\"\n\n        ensembl_mode = self.param(\"ensembl_mode\")\n        seqs = {}\n        genes = {}\n        peps = {}\n        all_peps = {}\n        tes = {}\n\n        gff = GFF.parse(gff3_handle)\n        for seq in gff:\n            seqs[seq.id] = len(seq.seq)\n\n            for feat in seq.features:\n                feat_length = abs(feat.location.end - feat.location.start)\n                # Store gene id and length\n                if feat.type in [\"gene\", \"ncRNA_gene\", \"pseudogene\"]:\n                    gene_id = feat.id\n                    if ensembl_mode:\n                        gene_id = feat_length\n                    genes[gene_id] = abs(feat.location.end - feat.location.start)\n                    # Get CDS id and length\n                    for feat2 in feat.sub_features:\n                        if feat2.type in (\"mRNA\", \"pseudogenic_transcript\"):\n                            length = {}\n                            for feat3 in feat2.sub_features:\n                                if feat3.type == \"CDS\":\n                                    pep_id = feat3.id\n                                    if ensembl_mode:\n                                        pep_id = pep_id.replace(\"CDS:\", \"\")\n                                    if pep_id not in length:\n                                        length[pep_id] = 0\n                                    length[pep_id] += abs(feat3.location.end - feat3.location.start)\n                            for pep_id in length:\n                                # Store length for translations, add pseudo translations separately\n                                pep_length = floor(length[pep_id] / 3) - 1\n                                if feat.type != \"pseudogene\":\n                                    peps[pep_id] = pep_length\n                                all_peps[pep_id] = pep_length\n                if feat.type == \"transposable_element\":\n                    tes[feat.id] = feat_length\n\n        stats = {\n            \"seq_region\": seqs,\n            \"genes\": genes,\n            \"translations\": peps,\n            \"all_translations\": all_peps,\n            \"transposable_elements\": tes,\n        }\n        return stats\n\n    def get_agp_seq_regions(self, agp_dict):\n        \"\"\"AGP files describe the assembly of larger sequence objects using smaller objects.\n            Eg: describes the assembly of scaffolds from contigs.\n\n        Args:\n            agp_dict: dict containing the information about the sequence.\n\n        Note:\n            AGP file is only used in the older builds, not used for current processing.\n        \"\"\"\n\n        seqr = {}\n        for agp in agp_dict:\n            agp_path = agp_dict[agp]\n\n            with open(agp_path, \"r\") as agph:\n                for line in agph:\n                    (\n                        asm_id,\n                        asm_start,\n                        asm_end,\n                        asm_part,\n                        typ,\n                        cmp_id,\n                        cmp_start,\n                        cmp_end,\n                        cmp_strand,\n                    ) = line.split(\"\\t\")\n                    # Ignore WGS contig\n                    if typ != \"W\":\n                        continue\n\n                    # Assembled seq length\n                    if not asm_id in seqr or seqr[asm_id] &lt; int(asm_end):\n                        seqr[asm_id] = int(asm_end)\n\n                    # Composite seq length\n                    if not cmp_id in seqr or seqr[cmp_id] &lt; int(cmp_end):\n                        seqr[cmp_id] = int(cmp_end)\n\n        return seqr\n\n    def check_ids(self, list1, list2, name):\n        \"\"\"Compare the ids in list1 and list2.\n\n        Args:\n            list1: dict containing sequence ids retrieved from functional.json.\n            list2: dict containing length and id in the retrieved from the gff.\n            name:  string\n\n        Return:\n            Error if the ids in functional.json and gff do not match.\n        \"\"\"\n\n        only1 = []\n        only2 = []\n        common = []\n\n        for item_id in list1:\n            if item_id in list2:\n                common.append(item_id)\n            else:\n                only1.append(item_id)\n        for item_id in list2:\n            if item_id not in common:\n                only2.append(item_id)\n\n        errors = []\n        if common:\n            print(\"%d common elements in %s\" % (len(common), name))\n        if only1:\n            errors.append(\"%d only in first list in %s (first: %s)\" % (len(only1), name, only1[0]))\n        if only2:\n            errors.append(\"%d only in second list in %s (first: %s)\" % (len(only2), name, only2[0]))\n\n        return errors\n\n    def check_lengths(self, list1, list2, name, allowed_len_diff=None, special_diff=False):\n        \"\"\"Check the difference in ids and length between list1 and list2.\n            There are a few special cases here where we allow a certain asymmetry\n            by changing the values of the arguments.\n\n        Args:\n            list1: dict containing length and id of the sequence from fasta files.\n            list2: dict containing length and id in the retrieved from the gff.\n            name:  string\n\n        allowed_len_diff : set as None when we do not want to accept any difference in length between list1 and list2.\n            The value here can be changed based on how much difference in sequence length we are wanting to accept.\n\n        special_diff: set as False when no special length difference is expected between the lists.\n                    This can be changed if we want to report common sequences with 1 BP difference.\n\n        Returns:\n            Error if there is a difference in length or ids between the lists.\n        \"\"\"\n\n        # check list diffferences, checks if abs(values diff) &lt; allowed_len_diff\n\n        set1 = frozenset(list1)\n        set2 = frozenset(list2)\n        list1_2 = list(set1 - set2)\n        list2_1 = list(set2 - set1)\n\n        errors = []\n        if len(list1_2) &gt; 0:\n            errors.append(\"%s: %d from the first list only (i.e. %s)\" % (name, len(list1_2), list1_2[0]))\n        if len(list2_1) &gt; 0:\n            errors.append(\"%s: %d from the second list only (i.e. %s)\" % (name, len(list2_1), list2_1[0]))\n\n        common_len = 0\n        if allowed_len_diff is None:\n            common_len = len(set1 &amp; set2)\n        else:\n            # check for the sequence length difference\n            diff_len_list = []\n            diff_len_special_list = []\n            for e in set1 &amp; set2:\n                dl12 = list1[e] - list2[e]\n                if abs(dl12) &lt;= allowed_len_diff:\n                    common_len += 1\n                else:\n                    _dlist = diff_len_list\n                    # Special case: 1 AA /BP shorter,\n                    #   so assuming the stop codon is not included in the CDS (when it should be)\n                    if dl12 == 1 and special_diff:\n                        _dlist = diff_len_special_list\n                    _dlist.append(\"%s: %d vs %d\" % (e, list1[e], list2[e]))\n            if diff_len_special_list:\n                errors.append(\n                    \"%d common elements with one BP/AA length diff for %s (e.g. %s)\"\n                    % (len(diff_len_special_list), name, diff_len_special_list[0])\n                )\n            if diff_len_list:\n                errors.append(\n                    \"%d common elements with length diff for %s (e.g. %s)\"\n                    % (len(diff_len_list), name, diff_len_list[0])\n                )\n        if common_len &gt; 0:\n            print(\"%d common elements between lists for %s\" % (common_len, name), file=sys.stderr)\n\n        return errors\n\n    def check_seq_region_lengths(self, seqrs, feats, name, circular=None):\n        \"\"\"Check the integrity of seq_region.json file by comparing the length of the sequence\n            to fasta files and the gff.\n\n            Seq_region file is in json format containing the metadata of the sequence.\n            It contains sequence id, length, location and the synonyms for the sequence name from different sources.\n\n        Args:\n            seqs: Sequence name and length retrieved from seq_region.json file.\n            feats: Sequence name and length retrieved from the fasta and gff file.\n            name: String\n\n        Returns:\n            Error if there are common sequences with difference in ids\n            and if the sequences are not consistent in the files.\n        \"\"\"\n\n        only_seqr = []\n        only_feat = []\n\n        common = []\n        diff = []\n        diff_list = []\n\n        # not an error on circular for gff features\n        diff_circular = []\n        diff_circular_list = []\n\n        for seq_id in seqrs:\n            if seq_id in feats:\n                # Check that feature is within the seq_region length\n                if feats[seq_id] &gt; seqrs[seq_id]:\n                    if circular is None or not circular.get(seq_id, False):\n                        diff.append(seq_id)\n                        diff_list.append(\"%s: %d vs %d\" % (seq_id, seqrs[seq_id], feats[seq_id]))\n                    else:\n                        diff_circular.append(seq_id)\n                        diff_circular_list.append(\"%s: %d vs %d\" % (seq_id, seqrs[seq_id], feats[seq_id]))\n                else:\n                    common.append(seq_id)\n            else:\n                only_seqr.append(seq_id)\n\n        for seq_id in feats:\n            if seq_id not in common and seq_id not in diff and seq_id not in diff_circular:\n                only_feat.append(seq_id)\n\n        if common:\n            print(\"%d common elements in %s\" % (len(common), name))\n        if diff_circular:\n            print(\n                \"%d differences for circular elements in %s (e.g. %s)\"\n                % (len(diff_circular), name, diff_circular_list[0])\n            )\n\n        errors = []\n        if diff:\n            errors.append(\n                \"%d common elements with higher length in %s (e.g. %s)\" % (len(diff), name, diff_list[0])\n            )\n        if only_seqr:\n            # Not an error!\n            print(\"%d only in seq_region list in %s (first: %s)\" % (len(only_seqr), name, only_seqr[0]))\n        if only_feat:\n            errors.append(\"%d only in second list in %s (first: %s)\" % (len(only_feat), name, only_feat[0]))\n\n        return errors\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/integrity/#src.ensembl.brc4.runnable.integrity.integrity.check_ids","title":"<code>check_ids(list1, list2, name)</code>","text":"<p>Compare the ids in list1 and list2.</p> <p>Parameters:</p> Name Type Description Default <code>list1</code> <p>dict containing sequence ids retrieved from functional.json.</p> required <code>list2</code> <p>dict containing length and id in the retrieved from the gff.</p> required <code>name</code> <p>string</p> required Return <p>Error if the ids in functional.json and gff do not match.</p> Source code in <code>src/ensembl/brc4/runnable/integrity.py</code> <pre><code>def check_ids(self, list1, list2, name):\n    \"\"\"Compare the ids in list1 and list2.\n\n    Args:\n        list1: dict containing sequence ids retrieved from functional.json.\n        list2: dict containing length and id in the retrieved from the gff.\n        name:  string\n\n    Return:\n        Error if the ids in functional.json and gff do not match.\n    \"\"\"\n\n    only1 = []\n    only2 = []\n    common = []\n\n    for item_id in list1:\n        if item_id in list2:\n            common.append(item_id)\n        else:\n            only1.append(item_id)\n    for item_id in list2:\n        if item_id not in common:\n            only2.append(item_id)\n\n    errors = []\n    if common:\n        print(\"%d common elements in %s\" % (len(common), name))\n    if only1:\n        errors.append(\"%d only in first list in %s (first: %s)\" % (len(only1), name, only1[0]))\n    if only2:\n        errors.append(\"%d only in second list in %s (first: %s)\" % (len(only2), name, only2[0]))\n\n    return errors\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/integrity/#src.ensembl.brc4.runnable.integrity.integrity.check_lengths","title":"<code>check_lengths(list1, list2, name, allowed_len_diff=None, special_diff=False)</code>","text":"<p>Check the difference in ids and length between list1 and list2.     There are a few special cases here where we allow a certain asymmetry     by changing the values of the arguments.</p> <p>Parameters:</p> Name Type Description Default <code>list1</code> <p>dict containing length and id of the sequence from fasta files.</p> required <code>list2</code> <p>dict containing length and id in the retrieved from the gff.</p> required <code>name</code> <p>string</p> required set as None when we do not want to accept any difference in length between list1 and list2. <p>The value here can be changed based on how much difference in sequence length we are wanting to accept.</p> set as False when no special length difference is expected between the lists. <p>This can be changed if we want to report common sequences with 1 BP difference.</p> <p>Returns:</p> Type Description <p>Error if there is a difference in length or ids between the lists.</p> Source code in <code>src/ensembl/brc4/runnable/integrity.py</code> <pre><code>def check_lengths(self, list1, list2, name, allowed_len_diff=None, special_diff=False):\n    \"\"\"Check the difference in ids and length between list1 and list2.\n        There are a few special cases here where we allow a certain asymmetry\n        by changing the values of the arguments.\n\n    Args:\n        list1: dict containing length and id of the sequence from fasta files.\n        list2: dict containing length and id in the retrieved from the gff.\n        name:  string\n\n    allowed_len_diff : set as None when we do not want to accept any difference in length between list1 and list2.\n        The value here can be changed based on how much difference in sequence length we are wanting to accept.\n\n    special_diff: set as False when no special length difference is expected between the lists.\n                This can be changed if we want to report common sequences with 1 BP difference.\n\n    Returns:\n        Error if there is a difference in length or ids between the lists.\n    \"\"\"\n\n    # check list diffferences, checks if abs(values diff) &lt; allowed_len_diff\n\n    set1 = frozenset(list1)\n    set2 = frozenset(list2)\n    list1_2 = list(set1 - set2)\n    list2_1 = list(set2 - set1)\n\n    errors = []\n    if len(list1_2) &gt; 0:\n        errors.append(\"%s: %d from the first list only (i.e. %s)\" % (name, len(list1_2), list1_2[0]))\n    if len(list2_1) &gt; 0:\n        errors.append(\"%s: %d from the second list only (i.e. %s)\" % (name, len(list2_1), list2_1[0]))\n\n    common_len = 0\n    if allowed_len_diff is None:\n        common_len = len(set1 &amp; set2)\n    else:\n        # check for the sequence length difference\n        diff_len_list = []\n        diff_len_special_list = []\n        for e in set1 &amp; set2:\n            dl12 = list1[e] - list2[e]\n            if abs(dl12) &lt;= allowed_len_diff:\n                common_len += 1\n            else:\n                _dlist = diff_len_list\n                # Special case: 1 AA /BP shorter,\n                #   so assuming the stop codon is not included in the CDS (when it should be)\n                if dl12 == 1 and special_diff:\n                    _dlist = diff_len_special_list\n                _dlist.append(\"%s: %d vs %d\" % (e, list1[e], list2[e]))\n        if diff_len_special_list:\n            errors.append(\n                \"%d common elements with one BP/AA length diff for %s (e.g. %s)\"\n                % (len(diff_len_special_list), name, diff_len_special_list[0])\n            )\n        if diff_len_list:\n            errors.append(\n                \"%d common elements with length diff for %s (e.g. %s)\"\n                % (len(diff_len_list), name, diff_len_list[0])\n            )\n    if common_len &gt; 0:\n        print(\"%d common elements between lists for %s\" % (common_len, name), file=sys.stderr)\n\n    return errors\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/integrity/#src.ensembl.brc4.runnable.integrity.integrity.check_md5sum","title":"<code>check_md5sum(path, md5sum)</code>","text":"<p>Verify the integrity of the files in manifest.json.</p> <pre><code>An MD5 hash is generated using the path provided which is then compared to the hash\nin manifest.json.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>Path</code> <p>The path for each file in the genome.</p> required <code>md5sum</code> <p>MD5 hash for the files.</p> required <p>Returns:</p> Type Description <p>Error if the md5sum does not match.</p> Source code in <code>src/ensembl/brc4/runnable/integrity.py</code> <pre><code>def check_md5sum(self, path, md5sum):\n    \"\"\"Verify the integrity of the files in manifest.json.\n\n        An MD5 hash is generated using the path provided which is then compared to the hash\n        in manifest.json.\n\n    Args:\n        Path: The path for each file in the genome.\n        md5sum: MD5 hash for the files.\n\n    Returns:\n        Error if the md5sum does not match.\n    \"\"\"\n\n    errors = []\n    with open(path, \"rb\") as f:\n        bytes = f.read()\n        readable_hash = hashlib.md5(bytes).hexdigest()\n        if readable_hash != md5sum:\n            errors.append(\"File %s has a wrong md5sum\" % path)\n\n    return errors\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/integrity/#src.ensembl.brc4.runnable.integrity.integrity.check_seq_region_lengths","title":"<code>check_seq_region_lengths(seqrs, feats, name, circular=None)</code>","text":"<p>Check the integrity of seq_region.json file by comparing the length of the sequence     to fasta files and the gff.</p> <pre><code>Seq_region file is in json format containing the metadata of the sequence.\nIt contains sequence id, length, location and the synonyms for the sequence name from different sources.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>seqs</code> <p>Sequence name and length retrieved from seq_region.json file.</p> required <code>feats</code> <p>Sequence name and length retrieved from the fasta and gff file.</p> required <code>name</code> <p>String</p> required <p>Returns:</p> Type Description <p>Error if there are common sequences with difference in ids</p> <p>and if the sequences are not consistent in the files.</p> Source code in <code>src/ensembl/brc4/runnable/integrity.py</code> <pre><code>def check_seq_region_lengths(self, seqrs, feats, name, circular=None):\n    \"\"\"Check the integrity of seq_region.json file by comparing the length of the sequence\n        to fasta files and the gff.\n\n        Seq_region file is in json format containing the metadata of the sequence.\n        It contains sequence id, length, location and the synonyms for the sequence name from different sources.\n\n    Args:\n        seqs: Sequence name and length retrieved from seq_region.json file.\n        feats: Sequence name and length retrieved from the fasta and gff file.\n        name: String\n\n    Returns:\n        Error if there are common sequences with difference in ids\n        and if the sequences are not consistent in the files.\n    \"\"\"\n\n    only_seqr = []\n    only_feat = []\n\n    common = []\n    diff = []\n    diff_list = []\n\n    # not an error on circular for gff features\n    diff_circular = []\n    diff_circular_list = []\n\n    for seq_id in seqrs:\n        if seq_id in feats:\n            # Check that feature is within the seq_region length\n            if feats[seq_id] &gt; seqrs[seq_id]:\n                if circular is None or not circular.get(seq_id, False):\n                    diff.append(seq_id)\n                    diff_list.append(\"%s: %d vs %d\" % (seq_id, seqrs[seq_id], feats[seq_id]))\n                else:\n                    diff_circular.append(seq_id)\n                    diff_circular_list.append(\"%s: %d vs %d\" % (seq_id, seqrs[seq_id], feats[seq_id]))\n            else:\n                common.append(seq_id)\n        else:\n            only_seqr.append(seq_id)\n\n    for seq_id in feats:\n        if seq_id not in common and seq_id not in diff and seq_id not in diff_circular:\n            only_feat.append(seq_id)\n\n    if common:\n        print(\"%d common elements in %s\" % (len(common), name))\n    if diff_circular:\n        print(\n            \"%d differences for circular elements in %s (e.g. %s)\"\n            % (len(diff_circular), name, diff_circular_list[0])\n        )\n\n    errors = []\n    if diff:\n        errors.append(\n            \"%d common elements with higher length in %s (e.g. %s)\" % (len(diff), name, diff_list[0])\n        )\n    if only_seqr:\n        # Not an error!\n        print(\"%d only in seq_region list in %s (first: %s)\" % (len(only_seqr), name, only_seqr[0]))\n    if only_feat:\n        errors.append(\"%d only in second list in %s (first: %s)\" % (len(only_feat), name, only_feat[0]))\n\n    return errors\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/integrity/#src.ensembl.brc4.runnable.integrity.integrity.get_agp_seq_regions","title":"<code>get_agp_seq_regions(agp_dict)</code>","text":"<p>AGP files describe the assembly of larger sequence objects using smaller objects.     Eg: describes the assembly of scaffolds from contigs.</p> <p>Parameters:</p> Name Type Description Default <code>agp_dict</code> <p>dict containing the information about the sequence.</p> required Note <p>AGP file is only used in the older builds, not used for current processing.</p> Source code in <code>src/ensembl/brc4/runnable/integrity.py</code> <pre><code>def get_agp_seq_regions(self, agp_dict):\n    \"\"\"AGP files describe the assembly of larger sequence objects using smaller objects.\n        Eg: describes the assembly of scaffolds from contigs.\n\n    Args:\n        agp_dict: dict containing the information about the sequence.\n\n    Note:\n        AGP file is only used in the older builds, not used for current processing.\n    \"\"\"\n\n    seqr = {}\n    for agp in agp_dict:\n        agp_path = agp_dict[agp]\n\n        with open(agp_path, \"r\") as agph:\n            for line in agph:\n                (\n                    asm_id,\n                    asm_start,\n                    asm_end,\n                    asm_part,\n                    typ,\n                    cmp_id,\n                    cmp_start,\n                    cmp_end,\n                    cmp_strand,\n                ) = line.split(\"\\t\")\n                # Ignore WGS contig\n                if typ != \"W\":\n                    continue\n\n                # Assembled seq length\n                if not asm_id in seqr or seqr[asm_id] &lt; int(asm_end):\n                    seqr[asm_id] = int(asm_end)\n\n                # Composite seq length\n                if not cmp_id in seqr or seqr[cmp_id] &lt; int(cmp_end):\n                    seqr[cmp_id] = int(cmp_end)\n\n    return seqr\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/integrity/#src.ensembl.brc4.runnable.integrity.integrity.get_fasta_lengths","title":"<code>get_fasta_lengths(fasta_path, ignore_final_stops=False)</code>","text":"<p>Check if the fasta files have the correct ids and no stop codon.</p> <p>Parameters:</p> Name Type Description Default <code>fasta_path</code> <p>Path to fasta_dna and fasta_pep files.</p> required <p>Returns:</p> Type Description <p>Error if any empty ids, non-unique ids or stop codons are found in the fasta files.</p> Source code in <code>src/ensembl/brc4/runnable/integrity.py</code> <pre><code>def get_fasta_lengths(self, fasta_path, ignore_final_stops=False):\n    \"\"\"Check if the fasta files have the correct ids and no stop codon.\n\n    Args:\n        fasta_path: Path to fasta_dna and fasta_pep files.\n\n    Returns:\n        Error if any empty ids, non-unique ids or stop codons are found in the fasta files.\n    \"\"\"\n\n    data = {}\n    non_unique = {}\n    non_unique_count = 0\n    empty_id_count = 0\n    contains_stop_codon = 0\n    for rec in SeqIO.parse(fasta_path, \"fasta\"):\n        # Flag empty ids\n        if rec.id == \"\":\n            empty_id_count += 1\n        else:\n            # Flag redundant ids\n            if rec.id in data:\n                non_unique[rec.id] = 1\n                non_unique_count += 1\n            # Store sequence id and length\n            data[rec.id] = len(rec.seq)\n            stops = rec.seq.count(\"*\")\n            if stops &gt; 1:\n                contains_stop_codon += 1\n            elif stops == 1:\n                if not rec.seq.endswith(\"*\") or not ignore_final_stops:\n                    contains_stop_codon += 1\n\n    errors = []\n    if empty_id_count &gt; 0:\n        errors.append(\"%d sequences with empty ids in %s\" % (empty_id_count, fasta_path))\n    if non_unique_count &gt; 0:\n        errors.append(\"%d non unique sequence ids in %s\" % (non_unique_count, fasta_path))\n    if contains_stop_codon &gt; 0:\n        errors.append(\"%d sequences with stop codons in %s\" % (contains_stop_codon, fasta_path))\n    return data, errors\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/integrity/#src.ensembl.brc4.runnable.integrity.integrity.get_functional_annotation","title":"<code>get_functional_annotation(json_path)</code>","text":"<p>Load the functional annotation file to retrieve the gene_id and translation id.     A functional annotation file contains information about a gene.     The functional annotation file is stored in a json format containing     the description, id and object type (eg: \"gene\", \"transcript\", \"translation\").</p> <p>Parameters:</p> Name Type Description Default <code>json_path</code> <p>Path to functional_annotation.json.</p> required <p>Returns:</p> Type Description <p>dict with gene and translation ids.</p> Source code in <code>src/ensembl/brc4/runnable/integrity.py</code> <pre><code>def get_functional_annotation(self, json_path):\n    \"\"\"Load the functional annotation file to retrieve the gene_id and translation id.\n        A functional annotation file contains information about a gene.\n        The functional annotation file is stored in a json format containing\n        the description, id and object type (eg: \"gene\", \"transcript\", \"translation\").\n\n    Args:\n        json_path: Path to functional_annotation.json.\n\n    Returns:\n        dict with gene and translation ids.\n    \"\"\"\n\n    # Load the json file\n    with open(json_path) as json_file:\n        data = json.load(json_file)\n\n        # Get gene ids and translation ids\n        genes = {}\n        translations = {}\n        tes = {}\n\n        for item in data:\n            if item[\"object_type\"] == \"gene\":\n                genes[item[\"id\"]] = 1\n            elif item[\"object_type\"] == \"translation\":\n                translations[item[\"id\"]] = 1\n            if item[\"object_type\"] == \"transposable_element\":\n                tes[item[\"id\"]] = 1\n\n        return {\"genes\": genes, \"translations\": translations, \"transposable_elements\": tes}\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/integrity/#src.ensembl.brc4.runnable.integrity.integrity.parse_gff3","title":"<code>parse_gff3(gff3_handle)</code>","text":"<p>A GFF parser is used to retrieve information in the GFF file such as    gene and CDS ids and their corresponding lengths.</p> <p>Parameters:</p> Name Type Description Default <code>gff3_handle</code> <p>Path to gff3 file.</p> required <p>Returns:</p> Type Description <p>dict containing sequence ids, gene ids, transcript ids and translation ids</p> <p>are stored with their corresponding lengths.</p> Source code in <code>src/ensembl/brc4/runnable/integrity.py</code> <pre><code>def parse_gff3(self, gff3_handle):\n    \"\"\"A GFF parser is used to retrieve information in the GFF file such as\n       gene and CDS ids and their corresponding lengths.\n\n    Args:\n        gff3_handle: Path to gff3 file.\n\n    Returns:\n        dict containing sequence ids, gene ids, transcript ids and translation ids\n        are stored with their corresponding lengths.\n    \"\"\"\n\n    ensembl_mode = self.param(\"ensembl_mode\")\n    seqs = {}\n    genes = {}\n    peps = {}\n    all_peps = {}\n    tes = {}\n\n    gff = GFF.parse(gff3_handle)\n    for seq in gff:\n        seqs[seq.id] = len(seq.seq)\n\n        for feat in seq.features:\n            feat_length = abs(feat.location.end - feat.location.start)\n            # Store gene id and length\n            if feat.type in [\"gene\", \"ncRNA_gene\", \"pseudogene\"]:\n                gene_id = feat.id\n                if ensembl_mode:\n                    gene_id = feat_length\n                genes[gene_id] = abs(feat.location.end - feat.location.start)\n                # Get CDS id and length\n                for feat2 in feat.sub_features:\n                    if feat2.type in (\"mRNA\", \"pseudogenic_transcript\"):\n                        length = {}\n                        for feat3 in feat2.sub_features:\n                            if feat3.type == \"CDS\":\n                                pep_id = feat3.id\n                                if ensembl_mode:\n                                    pep_id = pep_id.replace(\"CDS:\", \"\")\n                                if pep_id not in length:\n                                    length[pep_id] = 0\n                                length[pep_id] += abs(feat3.location.end - feat3.location.start)\n                        for pep_id in length:\n                            # Store length for translations, add pseudo translations separately\n                            pep_length = floor(length[pep_id] / 3) - 1\n                            if feat.type != \"pseudogene\":\n                                peps[pep_id] = pep_length\n                            all_peps[pep_id] = pep_length\n            if feat.type == \"transposable_element\":\n                tes[feat.id] = feat_length\n\n    stats = {\n        \"seq_region\": seqs,\n        \"genes\": genes,\n        \"translations\": peps,\n        \"all_translations\": all_peps,\n        \"transposable_elements\": tes,\n    }\n    return stats\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/integrity/#src.ensembl.brc4.runnable.integrity.integrity.run","title":"<code>run()</code>","text":"<p>Load files listed in the manifest.json and check the integrity.     Check if the files are correct by verifying the MD5 hash.     Check if translation, functional annotation and sequence region ids     and lengths are consistent with the information in gff.     Compare sequence length from fasta_dna file to seq_region.json metadata.</p> <p>Parameters:</p> Name Type Description Default <code>manifest</code> <p>Path to the manifest file.</p> required <p>Returns:</p> Type Description <p>Error if any of the above checks fail.</p> Source code in <code>src/ensembl/brc4/runnable/integrity.py</code> <pre><code>def run(self):\n    \"\"\"Load files listed in the manifest.json and check the integrity.\n        Check if the files are correct by verifying the MD5 hash.\n        Check if translation, functional annotation and sequence region ids\n        and lengths are consistent with the information in gff.\n        Compare sequence length from fasta_dna file to seq_region.json metadata.\n\n    Args:\n        manifest: Path to the manifest file.\n        It contains a set of files fasta, json metadata\n        and optional annotation files (gff, functional_annotation).\n\n    Returns:\n        Error if any of the above checks fail.\n    \"\"\"\n\n    manifest_path = self.param_required(\"manifest\")\n    errors = []\n    # load the manisfest.json\n    with open(manifest_path) as manifest_file:\n        manifest = json.load(manifest_file)\n\n        # Use dir name from the manifest\n        for name in manifest:\n            if \"file\" in manifest[name]:\n                file_name = manifest[name][\"file\"]\n                file_name = path.join(path.dirname(manifest_path), file_name)\n                # check if the md5sum is correct\n                md5sum = manifest[name][\"md5sum\"]\n                errors += self.check_md5sum(file_name, md5sum)\n\n                manifest[name] = file_name\n            else:\n                for f in manifest[name]:\n                    if \"file\" in manifest[name][f]:\n                        file_name = manifest[name][f][\"file\"]\n                        file_name = path.join(path.dirname(manifest_path), file_name)\n                        # check if the md5sum is correct\n                        md5sum = manifest[name][f][\"md5sum\"]\n                        errors += self.check_md5sum(file_name, md5sum)\n\n                        manifest[name][f] = file_name\n\n        # Get content from the manifest file and store it into following variables\n        dna = {}\n        pep = {}\n        seq_regions = {}\n        seq_lengths = {}\n        seq_circular = {}\n        gff = {}\n        func_ann = {}\n        agp_seqr = {}\n        genome = {}\n\n        if \"gff3\" in manifest:\n            print(\"Got a gff\")\n            gff = self.get_gff3(manifest[\"gff3\"])\n        if \"fasta_dna\" in manifest:\n            print(\"Got a fasta dna\")\n            # Verify if the length and id for the sequence is unique\n            dna, dna_errors = self.get_fasta_lengths(manifest[\"fasta_dna\"])\n            errors += dna_errors\n        if \"fasta_pep\" in manifest:\n            print(\"Got a fasta pep\")\n            ignore_final_stops = self.param(\"ignore_final_stops\")\n            # Verify if the length and id for the sequence is unique\n            pep, pep_errors = self.get_fasta_lengths(\n                manifest[\"fasta_pep\"], ignore_final_stops=ignore_final_stops\n            )\n            errors += pep_errors\n        if \"seq_region\" in manifest:\n            print(\"Got a seq_regions\")\n            seq_regions = get_json(Path(manifest[\"seq_region\"]))\n            seqr_lengths = {}\n            seqr_seqlevel = {}\n            # Store the length as int\n            for seq in seq_regions:\n                seq_lengths[seq[\"name\"]] = int(seq[\"length\"])\n                seq_circular[seq[\"name\"]] = seq.get(\"circular\", False)\n                if seq[\"coord_system_level\"] == \"contig\":\n                    seqr_seqlevel[seq[\"name\"]] = int(seq[\"length\"])\n        if \"functional_annotation\" in manifest:\n            print(\"Got a func_anns\")\n            func_ann = self.get_functional_annotation(manifest[\"functional_annotation\"])\n        if \"agp\" in manifest:\n            print(\"Got agp files\")\n            agp_seqr = self.get_agp_seq_regions(manifest[\"agp\"])\n        if \"genome\" in manifest:\n            print(\"Got a genome\")\n            genome = get_json(Path(manifest[\"genome\"]))\n\n        # Check if the accession is correct in genome.json\n        if genome:\n            if \"assembly\" in genome:\n                genome_ass = genome[\"assembly\"]\n                if \"accession\" in genome_ass:\n                    genome_acc = genome_ass[\"accession\"]\n                    if not re.match(\"GC[AF]_\\d{9}(\\.\\d+)?\", genome_acc):\n                        errors += [\"Genome assembly accession is wrong: '%s'\" % genome_acc]\n\n        # Check gff3\n        if gff:\n            # Check fasta_pep.fa integrity\n            # The sequence length and id retrieved from the fasta_pep file and compared to the translated CDS id and length in the gff\n            # We don't compare the peptide lengths because of seqedits\n            if pep:\n                tr_errors = self.check_lengths(\n                    pep, gff[\"translations\"], \"Fasta translations vs gff\", special_diff=True\n                )\n                if len(tr_errors) &gt; 0:\n                    # The pseudo CDSs are included in this check\n                    # Pseudo CDSs are not translated, if the pseudo translation ids are not ignored in the gff it will give an error\n                    tr_errors = self.check_lengths(\n                        pep,\n                        gff[\"all_translations\"],\n                        \"Fasta translations vs gff (include pseudo CDS)\",\n                        special_diff=True,\n                    )\n                    errors += tr_errors\n\n            # Check functional_annotation.json integrity\n            # Gene ids, translated CDS ids and translated CDSs including pseudogenes are compared to the gff\n            if func_ann:\n                errors += self.check_ids(func_ann[\"genes\"], gff[\"genes\"], \"Gene ids metadata vs gff\")\n                tr_errors = self.check_ids(\n                    func_ann[\"translations\"], gff[\"translations\"], \"Translation ids metadata vs gff\"\n                )\n                if len(tr_errors) &gt; 0:\n                    tr_errors = self.check_ids(\n                        func_ann[\"translations\"],\n                        gff[\"all_translations\"],\n                        \"Translation ids metadata vs gff (include pseudo CDS)\",\n                    )\n                errors += tr_errors\n                errors += self.check_ids(\n                    func_ann[\"transposable_elements\"],\n                    gff[\"transposable_elements\"],\n                    \"TE ids metadata vs gff\",\n                )\n\n            # Check the seq.json intregrity\n            # Compare the length and id retrieved from seq.json to the gff\n            if seq_regions:\n                errors += self.check_seq_region_lengths(\n                    seq_lengths, gff[\"seq_region\"], \"Seq_regions metadata vs gff\", seq_circular\n                )\n\n        # Check fasta dna and seq_region integrity\n        if dna and seq_regions:\n            errors += self.check_seq_region_lengths(seq_lengths, dna, \"seq_regions json vs dna\")\n\n        # Check agp and seq_region integrity\n        if agp_seqr and seq_lengths:\n            errors += self.check_seq_region_lengths(seq_lengths, agp_seqr, \"seq_regions json vs agps\")\n\n    if errors:\n        errors_str = \"\\n\".join(errors)\n        raise Exception(\"Integrity test failed for %s:\\n%s\" % (manifest_path, errors_str))\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/json_schema_factory/","title":"json_schema_factory","text":""},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/","title":"load_sequence_data","text":""},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data","title":"<code>load_sequence_data</code>","text":"<p>             Bases: <code>BaseRunnable</code></p> <p>loading sequence data, seq region names, atrributes and synonyms</p> <p>eHive module to load sequnce data, seq region names, atrributes and synonyms from FASTAs AGPs and seq_region.json. Various ensembl-analysis perl scripts are used to create coord_systems, load sequences and set attributes. SQL commands through out the code to be replaces with the proper python API at some point.</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>class load_sequence_data(eHive.BaseRunnable):\n    \"\"\"\n    loading sequence data, seq region names, atrributes and synonyms\n\n    eHive module to load sequnce data, seq region names, atrributes and synonyms from FASTAs AGPs and seq_region.json.\n    Various ensembl-analysis perl scripts are used to create coord_systems, load sequences and set attributes.\n    SQL commands through out the code to be replaces with the proper python API at some point.\n    \"\"\"\n\n    def param_defaults(self):\n        \"\"\"\n        default parameter/options values\n        \"\"\"\n        return {\n            # relative order of the coord_systems types to infer their rank from\n            \"cs_order\": \"ensembl_internal,chunk,contig,supercontig,non_ref_scaffold,scaffold,primary_assembly,superscaffold,linkage_group,chromosome\",\n            \"IUPAC\": \"RYKMSWBDHV\",  # symbols to be replaced with N in the DNA sequences (ensembl core(107) doesn't support the whole IUPAC alphabet for DNA)\n            # unversion scaffold, remove \".\\d$\" from seq_region.names if there's a need\n            \"unversion_scaffolds\": 0,\n            \"versioned_sr_syn_src\": \"INSDC\",  # INSDC(50710) # if unversioning non-sequence level cs, store original name (with version) as this synonym\n            \"sr_syn_src\": \"BRC4_Community_Symbol\",  # BRC4_Community_Symbol(211) # if unversioning sequence-level cs, store original name (with version) as this synonym\n            # nullify coord_system version for the given coord_system name\n            \"nullify_cs_version_from\": \"contig\",\n            # default coord_system name for single-level (no AGPs assemblies)\n            \"noagp_cs_name_default\": \"primary_assembly\",\n            # file for additional mapping of the synonym sources to the external_db (ensembl), as used by \"get_external_db_mapping\" function below\n            \"external_db_map\": None,\n            # set \"coord_system_tag\" seq_region attribute to this value if there's a corresponding \"chromosome_display_order\" list in genome.json metadata\n            #   if None, only \"chromosome\" coord system is processed (if present)\n            #   (see add_chr_karyotype_rank definition below )\n            #   if seq_region already has `coord_system_tagi` attribute, it value updated only if \"force_update_coord_system_tag\" module param is True (see below)\n            \"cs_tag_for_ordered\": None,\n            # Force updating of the \"coord_system_tags\" attribute for seq_regions from `cs_tag_for_ordered` (see above)\n            \"force_update_coord_system_tag\": False,\n            # BRC4 compatibility mode; if on, \"(EBI|BRC4)_seq_region_name\" seq_region_attributes are added.\n            #   Blocked by the \"swap_gcf_gca\" option. In this case insertion should be done on later pipeline stage after seq_region name swapping.\n            \"brc4_mode\": True,\n            # Whether to use RefSeq names as additional seq_region synonyms (if available) or not (see add_sr_synonyms definition below)\n            #  Does not swap anything actually, just loads synonyms to be used by a later \"swapping\" stage\n            #  Disables BRC4 compatibilty mode (see the \"brc4_mode\" option comment).\n            \"swap_gcf_gca\": False,\n            # list of coord systems used in \"-ignore_coord_system\" options of the \"ensembl-analysis/scripts/assembly_loading/set_toplevel.pl\" script\n            #   part of the loading process\n            \"not_toplevel_cs\": [],  # i.e. \"contig\", \"non_ref_scaffold\"\n            # explicit list of seq_region properties (keys) to load as seq_region_attrib s (values) (see add_sr_attribs definition below)\n            #   if a dict's used as a value, treat its keys as \"json_path\" (/ as delim) map, i.e.\n            #       { \"added_sequence\" : { \"assembly_provider\" : { \"name\" : ... } } } -&gt; \"added_sequence/assembly_provider/name\"\n            #   only flattable properties can be used, no arrays\n            #   arrays should be processed separately (see `add_sr_synonyms` or `add_karyotype_bands` definitions)\n            # see schemas/seq_region_schema.json\n            \"sr_attrib_types\": {\n                \"circular\": \"circular_seq\",\n                \"codon_table\": \"codon_table\",\n                \"location\": \"sequence_location\",\n                \"non_ref\": \"non_ref\",\n                \"coord_system_level\": \"coord_system_tag\",\n                \"added_sequence\": {\n                    # json_path to attrib_type_code(str) mapping\n                    \"added_sequence/accession\": \"added_seq_accession\",\n                    \"added_sequence/assembly_provider/name\": \"added_seq_asm_pr_nam\",\n                    \"added_sequence/assembly_provider/url\": \"added_seq_asm_pr_url\",\n                    \"added_sequence/annotation_provider/name\": \"added_seq_ann_pr_nam\",\n                    \"added_sequence/annotation_provider/url\": \"added_seq_ann_pr_url\",\n                },  # added_sequence\n            },\n            # loading additional sequences to the already exsisting core db\n            \"load_additional_sequences\": 0,\n            # size of the sequence data chunk, if 0 (default), no chunking is performed\n            \"sequence_data_chunck\": 0,\n            #   min size of the sequence chunk, no chunking is done if 'sequence_data_chunck' &lt; 'sequence_data_chunck_min'\n            \"sequence_data_chunck_min_len\": 50_000,\n            # coord system name for chunks\n            \"chunk_cs_name\": \"ensembl_internal\",\n        }\n\n    def run(self):\n        \"\"\"\n        Entry point for the Ehive module. All processing is done here in this case.\n        \"\"\"\n        # params\n        work_dir = self.param_required(\"work_dir\")\n\n        # initial sequence loading, using ensembl-analysis scripts\n        self.initial_sequence_loading(work_dir)\n\n        # load data from the corresponding core db tables\n        external_db_map = self.load_map_from_core_db(\n            \"external_db\", [\"db_name\", \"external_db_id\"], work_dir\n        )  # for external_db\n        attrib_type_map = self.load_map_from_core_db(\n            \"attrib_type\", [\"code\", \"attrib_type_id\"], work_dir\n        )  # for attrib_type\n        seq_region_map = self.load_map_from_core_db(\n            \"seq_region\", [\"name\", \"seq_region_id\"], work_dir\n        )  # for seq_region\n\n        # update synonyms and seq_region_attribs\n        unversion = self.param(\"unversion_scaffolds\")\n        is_primary_assembly = self.from_param(\"manifest_data\", \"agp\", not_throw=True) is None\n        seq_region_file = self.from_param(\"manifest_data\", \"seq_region\", not_throw=True)\n\n        #   add seq_region synonyms\n        self.add_sr_synonyms(\n            seq_region_file,\n            seq_region_map,\n            external_db_map,\n            self.pjc(work_dir, \"seq_region_syns\"),\n            unversion=unversion,\n        )\n\n        #   add seq_region attributes\n        self.add_sr_attribs(\n            seq_region_file,\n            seq_region_map,\n            attrib_type_map,\n            self.pjc(work_dir, \"seq_region_attr\"),\n            unversion=unversion,\n        )\n\n        #   add seq_region EBI and BRC4 name attributes in the \"BRC4 mode\"\n        #     special case of attributes adding with default values derived from seq_region names\n        #     do not add if preparing to swap RefSeq and GeneBank ids; in this case attributes to be added at a later stage in pipeline\n        #     (easier to insert then to update)\n        if self.param(\"brc4_mode\") and not self.param(\"swap_gcf_gca\"):\n            self.add_sr_ebi_brc4_names(\n                seq_region_file,\n                seq_region_map,\n                attrib_type_map,\n                self.pjc(work_dir, \"seq_region_ebi_brc4_name\"),\n                unversion=unversion,\n            )\n\n        # add karyotype related data\n        self.add_karyotype_data(\n            seq_region_file,\n            seq_region_map,\n            attrib_type_map,\n            self.pjc(work_dir, \"karyotype\"),\n            unversion=unversion,\n        )\n\n    def initial_sequence_loading(self, work_dir: str):\n        \"\"\"\n        initial preparation and loading of AGPs and fasta data.\n\n        initial preparation and loading of AGPs and fasta data using ensembl-analysis perl scripts\n        \"\"\"\n        # preprocess FASTA with sequences\n        #   rename IUPAC to N symbols using sed\n        fasta_clean = self.from_param(\"manifest_data\", \"fasta_dna\")\n\n        # start coord system ranking and agps processing\n        agps = self.from_param(\"manifest_data\", \"agp\", not_throw=True)\n\n        # get the deafult coord_system order\n        #   use noagp_cs_name_default for \"noagp\" assemblies\n        cs_order = self.coord_sys_order(self.param(\"cs_order\"))\n        noagps_cs = self.param(\"noagp_cs_name_default\")\n\n        # remove gaps and lower_level mappings if the are coveres by higher level ones\n        #   i.e.: remove 'contigN to chromosomeZ', if 'contigN to scaffoldM' and 'scaffoldM to chromosomeZ' are in place\n        #   returns None if no agps provided\n        agps_pruned_dir = self.pjc(work_dir, \"agps_pruned\")\n        agps_pruned = self.prune_agps(agps, cs_order, agps_pruned_dir, self.param_bool(\"prune_agp\"))\n\n        # order\n        # rank cs_names, met in agps.keys (\"-\" separated, i.e. \"scaffold-contig\") based on cs_order\n        cs_rank = self.used_cs_ranks(agps_pruned, cs_order, noagps_cs)\n\n        # chunk sequence data if needed\n        #   no chunking if chunk_size &lt; 50k\n        chunk_size = int(self.param(\"sequence_data_chunck\"))\n        chunk_cs_name = self.param(\"chunk_cs_name\")\n        fasta_clean, cs_rank, agps_pruned = self.chunk_contigs(\n            fasta_clean,\n            cs_rank,\n            agps_pruned,\n            pj(work_dir, \"chuncking\"),\n            chunk_size=chunk_size,\n            chunks_cs_name=chunk_cs_name,\n        )\n\n        # empty agps_pruned ignored\n        self.load_seq_data(fasta_clean, agps_pruned, cs_rank, self.pjc(work_dir, \"load\"))\n\n        # mark all the \"contig\"s or noagp_cs as being sourced from ENA\n        if not self.param_bool(\"no_contig_ena_attrib\"):\n            # NB using original \"agps\" parameter (with no chuncking data added)\n            agps_raw = self.from_param(\"manifest_data\", \"agp\", not_throw=True)\n            if agps_raw is None:\n                self.add_contig_ena_attrib(self.pjc(work_dir, \"load\", \"set_ena\"), cs_name=noagps_cs)\n            else:\n                self.add_contig_ena_attrib(self.pjc(work_dir, \"load\", \"set_ena\"))\n\n        # unversion scaffold, remove \".\\d$\" from names if there's a need\n        if self.param_bool(\"unversion_scaffolds\"):\n            self.unversion_scaffolds(cs_rank, self.pjc(work_dir, \"unversion_scaffolds\"))\n\n        # add assembly mappings between various cs to meta table for the mapper to work properly\n        cs_pairs = agps_pruned and agps_pruned.keys() or None\n        self.add_asm_mappings(cs_pairs, self.pjc(work_dir, \"asm_mappings\"))\n\n        # set toplevel seq_region attribute\n        self.set_toplevel(self.pjc(work_dir, \"set_toplevel\"), self.param(\"not_toplevel_cs\"))\n\n        # nullify contig version and update mappings strings accordingly; ignore for \"load_additional_sequences\" mode\n        if not self.param_bool(\"load_additional_sequences\"):\n            self.nullify_ctg_cs_version(cs_order, self.pjc(work_dir, \"asm_mapping\", \"nullify_cs_versions\"))\n\n    def add_sr_synonyms(\n        self,\n        seq_region_file: str,\n        seq_region_map: dict,\n        external_db_map: dict,\n        work_dir: str,\n        unversion: bool = False,\n        unversionable_sources_set: frozenset = frozenset([\"INSDC\", \"RefSeq\"]),\n    ):\n        \"\"\"\n        Add seq_region_synonym from the seq_region_file meta data file.\n\n        Add seq_region_synonym from the schemas/seq_region_schema.json compatible meta data file.\n        Merge with the already exinsting ones in the db.\n\n        If unversion is true:\n          * the unversioned synonym would be used to get the seq_region_id from \"seq_region_map\" if possible\n          * the unversioned synonyms from the unversionable_sources_set will be added as well as the original ones\n\n        Too close to the DB schema.\n        \"\"\"\n        os.makedirs(work_dir, exist_ok=True)\n\n        # return if there's nothing to add\n        if not seq_region_file:\n            return\n\n        # get seq_region ids, names, syns from db\n        synonyms_trios_db = self.load_seq_region_synonyms_trios_from_core_db(\n            self.pjc(work_dir, \"syns_from_core\")\n        )\n\n        # form set of synonyms already present in db\n        synonyms_in_db = frozenset([trio[2] for trio in synonyms_trios_db if trio[2] != \"NULL\"])\n\n        # subset of sources to use the allowed the unversion synonyms\n        unversionable_sources = unversion and unversionable_sources_set or frozenset()\n\n        # technical / optimization. get external_db_id for \"ensembl_internal_synonym\"\n        ensembl_internal_synonym_ext_db_id = self.id_from_map_or_die(\n            \"ensembl_internal_synonym\", external_db_map, \"external_db_map\"\n        )\n\n        # get dict for additional mapping for sources to external_db names if there's one specified by \"external_db_map\" module param\n        #   not to be confused with the external_db_map function parameter above\n        additional_sources_mapping = self.get_external_db_mapping()\n\n        # load synonyms from the json file\n        synonyms_from_json = (\n            []\n        )  # [ (seq_region_id, synonym, external_db_id)... ] list of trios for inserting into db\n        with open(seq_region_file) as in_file:\n            seq_regions = list(json.load(in_file))\n            for seq_region in filter(lambda sr: sr.get(\"synonyms\", False), seq_regions):\n                # iterate through all seq_regions having \"synonyms\"\n                seq_region_name, seq_region_id, _ = self.name_and_id_from_seq_region_item(\n                    seq_region, seq_region_map, try_unversion=unversion\n                )\n\n                # fill synonyms_from_json list of trios\n                for synonym_item in list(seq_region[\"synonyms\"]):\n                    synonym_name = synonym_item[\"name\"]\n                    source = synonym_item[\"source\"]\n                    unversioned_name = \"\"\n\n                    # check if there's any addtional mapping for the source, remap if so\n                    if additional_sources_mapping:\n                        source = additional_sources_mapping.get(\n                            source, source\n                        )  # use the same name if no matches in additional_sources_mapping dict\n\n                    # try to get unversioned name if applicable\n                    if source in unversionable_sources:\n                        unversioned_name = re.sub(r\"\\.\\d+$\", \"\", synonym_name)\n\n                    # put trios if names are not already seen in db\n                    if synonym_name not in synonyms_in_db:\n                        external_db_id = self.id_from_map_or_die(source, external_db_map, \"external_db_map\")\n                        synonyms_from_json.append(\n                            (seq_region_id, self.quote_or_null(synonym_name), external_db_id)\n                        )\n\n                    #   put additional unversioned synonyms if there's a sane one\n                    if (\n                        unversioned_name\n                        and unversioned_name != synonym_name\n                        and unversioned_name not in synonyms_in_db\n                    ):\n                        synonyms_from_json.append(\n                            (\n                                seq_region_id,\n                                self.quote_or_null(unversioned_name),\n                                ensembl_internal_synonym_ext_db_id,\n                            )\n                        )\n\n        # run insertion SQL\n        self.insert_to_db(\n            synonyms_from_json,\n            \"seq_region_synonym\",\n            [\"seq_region_id\", \"synonym\", \"external_db_id\"],\n            self.pjc(work_dir, \"new_seq_region_synonyms\"),\n            ignore=True,\n        )\n\n    def add_sr_attribs(\n        self,\n        seq_region_file: str,\n        seq_region_map: dict,\n        attrib_type_map: dict,\n        work_dir,\n        unversion: bool = False,\n    ):\n        \"\"\"\n        Add seq_region_attrib(s) from the seq_region_file meta data file.\n\n        Explicit list is taken from \"sr_attrib_types\" module param.\n\n        Add seq_region_attrib(s) from the schemas/seq_region_schema.json compatible meta data file.\n        Explicit list is taken from \"sr_attrib_types\" module param.\n\n        \"sr_attrib_types\" defines { json_property -&gt; attrib_type.name } map. If the value is dict,\n        its keys are treated as \"/\"-delimetered \"json_path\" (i.e. \"added_sequence/assembly_provider/name\").\n        No arrays can be processed. Only simple or \"flattable\" types.\n\n        If unversion is true:\n          * the unversioned synonym would be used to get the seq_region_id from \"seq_region_map\" if possible\n\n        Too close to the DB schema.\n        \"\"\"\n        os.makedirs(work_dir, exist_ok=True)\n\n        # return if there's nothing to add\n        if not seq_region_file:\n            return\n\n        # technical / optimization. get atttib_type_id(s)\n        # create a smaller map with attrib_type_id(s) as values\n        properties_to_use = (\n            []\n        )  # [frozen]set with the top-level \"seq_region\" properties, that should be processed\n        path_attrib_id_map = dict()  # { \"flatterned/json/paths\" : attrib_id_map ))}\n        # fill set and map\n        for prop, attrib_type in self.param(\"sr_attrib_types\").items():\n            # adding high level properties to process\n            properties_to_use.append(prop)\n            # adding json paths (or properties themselves) to atrrib_type_id map\n            if isinstance(attrib_type, dict):  # if using json paths (delimeterd with \"/\")\n                for path, inner_attrib_type in attrib_type.items():\n                    path_attrib_id_map[path] = self.id_from_map_or_die(\n                        inner_attrib_type, attrib_type_map, \"attrib_type_map\"\n                    )\n            else:\n                path_attrib_id_map[prop] = self.id_from_map_or_die(\n                    attrib_type, attrib_type_map, \"attrib_type_map\"\n                )\n        # return if there's nothing to add\n        if not properties_to_use:\n            return\n\n        properties_to_use = frozenset(properties_to_use)\n\n        # load attributes from seq_region file\n        attrib_trios = []  # [ (seq_region_id, attrib_id, value)... ] list of trios for inserting into db\n        with open(seq_region_file) as in_file:\n            seq_regions = list(json.load(in_file))\n            for seq_region in seq_regions:\n                # get seq_region_id (perhaps, by using unversioned name)\n                seq_region_name, seq_region_id, unversioned_name = self.name_and_id_from_seq_region_item(\n                    seq_region, seq_region_map, try_unversion=unversion\n                )\n\n                # iterate through properties\n                for prop_name in properties_to_use:\n                    if prop_name not in seq_region:\n                        continue\n                    # flattern path\n                    path_attrib_id_values_list = self.flattern_seq_region_item(\n                        seq_region, prop_name, path_attrib_id_map\n                    )\n                    # fill attrib_trios\n                    for path, attrib_id, value in path_attrib_id_values_list:\n                        attrib_trios.append((seq_region_id, attrib_id, self.quote_or_null(value)))\n\n        # run insertion SQL\n        self.insert_to_db(\n            attrib_trios,\n            \"seq_region_attrib\",\n            [\"seq_region_id\", \"attrib_type_id\", \"value\"],\n            self.pjc(work_dir, \"brc4_ebi_seq_region_synonyms\"),\n            ignore=True,\n        )\n\n    def flattern_seq_region_item(\n        self, seq_region: dict, prop_name: str, path_attrib_id_map: dict, sep: str = \"/\"\n    ) -&gt; list:\n        \"\"\"\n        Flattern seq_region[property] and store corresponding [ (json_path, attrib_id, value)... ] (as list of trios).\n\n        Only works for simple properties or dicts with no arrays on the path. Basically, implemets tree traversal.\n        Utility function used by the `add_sr_attribs` method\n        \"\"\"\n        res = []\n        # is there anything to do\n        if prop_name not in seq_region:\n            return res\n\n        # set up\n        value = seq_region[prop_name]\n        paths_to_go = [\n            (prop_name, value)\n        ]  # storing path and the corresponding value, to prevent repetetive traversals\n        # iterate\n        while paths_to_go:\n            (path, value) = paths_to_go.pop()  # get last item\n            if isinstance(value, list):\n                # perhaps, it's better to raise exception then to continue silently\n                continue\n            if isinstance(value, dict):\n                # if value is a complex object, add its leaves\n                for key, val in value.items():\n                    paths_to_go.append((f\"{path}{sep}{key}\", val))\n                continue\n            # if value is simple\n            attrib_id = path_attrib_id_map.get(path, None)\n            if attrib_id:\n                res.append((path, attrib_id, value))\n        # return what ever we have\n        return res\n\n    def add_sr_ebi_brc4_names(\n        self,\n        seq_region_file: str,\n        seq_region_map: dict,\n        attrib_type_map: dict,\n        work_dir: str,\n        unversion: bool = False,\n    ):\n        \"\"\"\n        Add \"(EBI|BRC4)_seq_region_name\" seq_region_attrib(s) either from the seq_region_file meta data file, or from original seq_region names.\n\n        Add \"(EBI|BRC4)_seq_region_name\" seq_region_synonym from the schemas/seq_region_schema.json compatible meta data file or from the original seq_region_names.\n        A special case of attributes adding with default values derived from seq_region names.\n\n        If unversion is true:\n          * the unversioned synonym would be used to get the seq_region_id from \"seq_region_map\" if possible\n\n        Too close to the DB schema.\n        \"\"\"\n        os.makedirs(work_dir, exist_ok=True)\n\n        # return if there's nothing to add\n        if not seq_region_file:\n            return\n\n        # technical / optimization. get atttib_type_id(s) for \"(EBI|BRC4)_seq_region_name\"\n        tagged_sr_name_attrib_id = {\n            tag: self.id_from_map_or_die(f\"{tag}_seq_region_name\", attrib_type_map, \"attrib_type_map\")\n            for tag in [\"EBI\", \"BRC4\"]\n        }\n\n        # load BRC4/EBI name from seq_region file\n        brc4_ebi_name_attrib_trios = (\n            []\n        )  # [ (seq_region_id, attrib_id, value)... ] list of trios for inserting into db\n        with open(seq_region_file) as in_file:\n            seq_regions = list(json.load(in_file))\n            for seq_region in seq_regions:\n                # get seq_region_id (perhaps, by using unversioned name)\n                seq_region_name, seq_region_id, unversioned_name = self.name_and_id_from_seq_region_item(\n                    seq_region, seq_region_map, try_unversion=unversion\n                )\n                # append attribs to the brc4_ebi_name_attrib_trios list\n                for tag in [\"BRC4\", \"EBI\"]:\n                    attrib_name = f\"{tag}_seq_region_name\"\n                    attrib_id = tagged_sr_name_attrib_id[tag]\n                    value = seq_region.get(attrib_name, seq_region_name)\n                    brc4_ebi_name_attrib_trios.append((seq_region_id, attrib_id, self.quote_or_null(value)))\n\n        # run insertion SQL\n        self.insert_to_db(\n            brc4_ebi_name_attrib_trios,\n            \"seq_region_attrib\",\n            [\"seq_region_id\", \"attrib_type_id\", \"value\"],\n            self.pjc(work_dir, \"brc4_ebi_seq_region_synonyms\"),\n            ignore=True,\n        )\n\n    def add_karyotype_data(\n        self,\n        seq_region_file: str,\n        seq_region_map: dict,\n        attrib_type_map: dict,\n        work_dir: str,\n        unversion: bool = False,\n    ):\n        \"\"\"\n        Adds various karyotypic data from seq_region file and assembly metadata (if present).\n\n        Adds various karyotypic data from the schemas/seq_region_schema.json compatible meta data file and assembly metadata (if present).\n\n        If unversion is true:\n          * the unversioned synonym would be used to get the seq_region_id from \"seq_region_map\" if possible\n        \"\"\"\n        # add karyotyope bands data\n        regions_with_karyotype_bands = self.add_karyotype_bands(\n            seq_region_file,\n            seq_region_map,\n            attrib_type_map,\n            self.pjc(work_dir, \"karyotype_bands\"),\n            unversion=unversion,\n        )\n\n        # try to add karyotype ranks for regions listed in genome_data/assembly/chromosome_display_order metadata\n        regions_with_ranks_from_assembly_metadata = self.add_karyotype_rank_based_on_assembly_metadata(\n            seq_region_map,\n            attrib_type_map,\n            self.pjc(work_dir, \"karyotype_ranks_from_meta\"),\n            unversion=unversion,\n        )\n\n        regions_with_ranks_from_chromosome_cs = []\n        if not regions_with_ranks_from_assembly_metadata:\n            # try to add karyotype_ranks for top-level regions from the \"chromosome\" coord_system\n            regions_with_ranks_from_chromosome_cs = self.add_karyotype_rank_for_chromosomes(\n                attrib_type_map, self.pjc(work_dir, \"karyotype_ranks_for_chromosomes\")\n            )\n\n        # make sure that regions with bands have karyotype_ranks\n        self.add_karyotype_rank_from_bands_info(\n            regions_with_karyotype_bands,\n            regions_with_ranks_from_chromosome_cs + regions_with_ranks_from_assembly_metadata,\n            attrib_type_map,\n            self.pjc(work_dir, \"karyotype_ranks_from_bands\"),\n        )\n\n    def add_karyotype_bands(\n        self,\n        seq_region_file: str,\n        seq_region_map: dict,\n        attrib_type_map: dict,\n        work_dir: str,\n        unversion: bool = False,\n        karyotype_bands_property=\"karyotype_bands\",\n    ) -&gt; list:  # [ (seq_region_name, seq_region_id, unversioned_name) ]\n        \"\"\"\n        Add karyotypic data from the seq_region metafile.\n\n        Add karyotypic data from the schemas/seq_region_schema.json compatible meta data file.\n        Returns list of [ (seq_region_name, seq_region_id, unversioned_name) ] trios for seq_regions having karyotype bands info.\n\n        If unversion is true:\n          * the unversioned synonym would be used to get the seq_region_id from \"seq_region_map\" if possible\n\n        Too close to the DB schema.\n        \"\"\"\n        os.makedirs(work_dir, exist_ok=True)\n\n        # return if there's nothing to add\n        if not seq_region_file:\n            return\n\n        # resulting list of seq regions with bands\n        seq_regions_with_karyotype_bands = []  # [ ( seq_region_name, seq_region_id, unversioned_name )... ]\n\n        # load BRC4/EBI name from seq_region file\n        band_tuples = (\n            []\n        )  # [ (seq_region_id, seq_region_start, seq_region_end, band|\"NULL\", stain|\"NULL\")... ] list of tuples for inserting into db\n        with open(seq_region_file) as in_file:\n            seq_regions = list(json.load(in_file))\n            for seq_region in filter(lambda sr: sr.get(karyotype_bands_property, False), seq_regions):\n                # iterate through all seq_regions having non-empty \"karyotype_bands\"\n\n                # get seq_region_id (perhaps, by using unversioned name)\n                seq_region_name, seq_region_id, unversioned_name = self.name_and_id_from_seq_region_item(\n                    seq_region, seq_region_map, try_unversion=unversion\n                )\n\n                # append trio to the resulting list\n                seq_regions_with_karyotype_bands.append((seq_region_name, seq_region_id, unversioned_name))\n\n                # append bands to the band_tuples list\n                for band in seq_region[karyotype_bands_property]:\n                    # print(\"BAND: \" + str(band), file = sys.stderr)\n                    # coords\n                    seq_region_start = band[\"start\"]\n                    seq_region_end = band[\"end\"]\n                    # band_name and stain\n                    band_name = band.get(\"name\", None)\n                    stain = band.get(\"stain\", None)\n                    # special cases for stain\n                    structure = band.get(\"structure\", None)\n                    if structure == \"telomere\":\n                        stain = \"TEL\"\n                    elif structure == \"centromere\":\n                        stain = \"ACEN\"\n\n                    # append tuple\n                    band_tuples.append(\n                        (\n                            seq_region_id,\n                            seq_region_start,\n                            seq_region_end,\n                            self.quote_or_null(band_name),\n                            self.quote_or_null(stain),\n                        )\n                    )\n\n        # run insertion SQL\n        self.insert_to_db(\n            band_tuples,\n            \"karyotype\",\n            [\"seq_region_id\", \"seq_region_start\", \"seq_region_end\", \"band\", \"stain\"],\n            self.pjc(work_dir, \"karyotype_insertion\"),\n            ignore=True,\n        )\n\n        # return resulting list of regions with bands trios\n        return seq_regions_with_karyotype_bands\n\n    def add_karyotype_rank_based_on_assembly_metadata(\n        self, seq_region_map: dict, attrib_type_map: dict, work_dir: str, unversion: bool = True\n    ) -&gt; list:  # [ (seq_region_name, seq_region_id, unversioned_name) ]\n        \"\"\"\n        Add `karyotype_rank` attributes for seq region data from based on metadata from the \"genome_data\" module parameter.\n        Add only to the seq_regions with ids listed in the array corresponding to 'genome_data/assembly/chromosome_display_order'.\n\n        Set \"coord_system_tag\" attribute to the one listed in the \"cs_tag_for_ordered\" module param; or \"chromosome\" if param value is underfined.\n        Force updating of the \"coord_system_tags\" if `force_update_coord_system_tag` module param is True.\n\n        Returns list of [ (seq_region_name, seq_region_id, unversioned_name) ] trios for seq_regions with updated karyotype_ranks.\n\n        If unversion is true:\n          * the unversioned synonym would be used to get the seq_region_id from \"seq_region_map\" if possible\n\n        Too close to the DB schema.\n        \"\"\"\n\n        # resulting list of seq_region with karyotype_rank\n        regions_with_ranks_from_assembly_metadata = (\n            []\n        )  # [ ( seq_region_name, seq_region_id, unversioned_name )... ]\n\n        # get `chromosome_display_order` list from the assembly metadata\n        assembly_metadata = self.from_param(\"genome_data\", \"assembly\", not_throw=True) or dict()\n        chromosome_display_order_list = assembly_metadata.get(\"chromosome_display_order\", [])\n\n        # technical / optimization. get external_db_id for \"karyotype_rank\" and \"coord_system_tag\"\n        karyotype_rank_attrib_id = self.id_from_map_or_die(\n            \"karyotype_rank\", attrib_type_map, \"attrib_type_map\"\n        )\n        coord_system_tag_attrib_id = self.id_from_map_or_die(\n            \"coord_system_tag\", attrib_type_map, \"attrib_type_map\"\n        )\n        coord_system_tag = self.param(\"cs_tag_for_ordered\") or \"chromosome\"\n        force_update_coord_system_tag = self.param(\"force_update_coord_system_tag\") or False\n\n        # set/update proper attributes for `chromosome_display_order_list` regions\n        rank_insertions_trios = []\n        coord_system_tag_attrib_insertion_trios = []\n        coord_system_tag_attrib_seq_region_update_ids = []\n\n        for seq_region_name_raw in chromosome_display_order_list:\n            # wrap seq_region_name_raw into seq_region struct { \"name\": seq_region_name_raw } and\n            # get seq_region_id (perhaps, by using unversioned name)\n            seq_region_name, seq_region_id, unversioned_name = self.name_and_id_from_seq_region_item(\n                {\"name\": seq_region_name_raw}, seq_region_map, try_unversion=unversion\n            )\n\n            # append trio to the resulting list\n            regions_with_ranks_from_assembly_metadata.append(\n                (seq_region_name, seq_region_id, unversioned_name)\n            )\n\n            # filling insert lists for \"karyotype_rank\" and \"coord_system_tag\" attributes\n            rank_insertions_trios.append(\n                (seq_region_id, karyotype_rank_attrib_id, len(rank_insertions_trios) + 1)\n            )\n            coord_system_tag_attrib_insertion_trios.append(\n                (seq_region_id, coord_system_tag_attrib_id, self.quote_or_null(coord_system_tag))\n            )\n\n            # filling update list for \"coord_system_tag\" with seq_region_ids\n            coord_system_tag_attrib_seq_region_update_ids.append(seq_region_id)\n\n        # run insertion SQL for \"karyotype_rank\"\n        self.insert_to_db(\n            rank_insertions_trios,\n            \"seq_region_attrib\",\n            [\"seq_region_id\", \"attrib_type_id\", \"value\"],\n            self.pjc(work_dir, \"karyotype_rank_insertion\"),\n            ignore=True,\n        )\n\n        # run insertion SQL for \"coord_system_tag\"\n        self.insert_to_db(\n            coord_system_tag_attrib_insertion_trios,\n            \"seq_region_attrib\",\n            [\"seq_region_id\", \"attrib_type_id\", \"value\"],\n            self.pjc(work_dir, \"coord_system_tag_insertion\"),\n            ignore=True,\n        )\n\n        # forcing update of the \"coord_system_tag\"\n        if force_update_coord_system_tag and coord_system_tag_attrib_seq_region_update_ids:\n            seq_region_ids_str = \",\".join(map(str, coord_system_tag_attrib_seq_region_update_ids))\n            self.update_db_single_group(\n                {\"value\": self.quote_or_null(coord_system_tag)},\n                \"seq_region_attrib\",\n                self.pjc(work_dir, \"coord_system_tag_update\"),\n                where=f\"attrib_type_id = {coord_system_tag_attrib_id} and seq_region_id in ({seq_region_ids_str})\",\n            )\n\n        # return resulting list of regions with bands trios\n        return regions_with_ranks_from_assembly_metadata\n\n    def add_karyotype_rank_for_chromosomes(\n        self, attrib_type_map: dict, work_dir: str, chromosome_coord_system_name=\"chromosome\"\n    ) -&gt; list:  # [ (seq_region_name, seq_region_id, unversioned_name) ]\n        \"\"\"\n        Add `karyotype_rank` attributes for seq region data from the \"chromosome\" coordinate system.\n\n        Returns list of [ (seq_region_name, seq_region_id, unversioned_name) ] trios for seq_regions with updated karyotype_ranks.\n        Not altering \"coord_system_tag\" tag attributes.\n\n        If unversion is true:\n          * the unversioned synonym would be used to get the seq_region_id from \"seq_region_map\" if possible\n\n        Too close to the DB schema.\n        \"\"\"\n\n        # resulting list of seq_region with karyotype_rank\n        #   list of top level seq regions from the `chromosome_coord_system_name`\n        chromomes_seq_regions = self.get_toplevel_from_cs(\n            chromosome_coord_system_name, self.pjc(work_dir, \"chromosome_seq_regions\")\n        )\n\n        if not chromomes_seq_regions:\n            return chromomes_seq_regions\n\n        # technical / optimization. get external_db_id for \"karyotype_rank\"\n        karyotype_rank_attrib_id = self.id_from_map_or_die(\n            \"karyotype_rank\", attrib_type_map, \"attrib_type_map\"\n        )\n\n        # set/update proper attributes for \"chromomosome\" regions\n        rank_insertions_trios = []\n        for _, seq_region_id, _ in chromomes_seq_regions:\n            rank_insertions_trios.append(\n                (seq_region_id, karyotype_rank_attrib_id, len(rank_insertions_trios) + 1)\n            )\n\n        # run insertion SQL for \"karyotype_rank\"\n        #    do not alter \"coord_system_tag\" anyhow in the case of the \"chromosome\" coord_system\n        self.insert_to_db(\n            rank_insertions_trios,\n            \"seq_region_attrib\",\n            [\"seq_region_id\", \"attrib_type_id\", \"value\"],\n            self.pjc(work_dir, \"karyotype_rank_insertion\"),\n            ignore=True,\n        )\n\n        return chromomes_seq_regions\n\n    def add_karyotype_rank_from_bands_info(\n        self,\n        regions_with_karyotype_bands: list,  # [ (seq_region_name, seq_region_id, unversioned_name) ]\n        other_regions_with_ranks: list,  # [ (seq_region_name, seq_region_id, unversioned_name) ]\n        attrib_type_map: dict,\n        work_dir: str,\n    ):\n        \"\"\"\n        Add karyotype_ranks for `regions_with_karyotype_bands` (those with karyotype bands in seq_region metadata) but not present in `other_regions_with_ranks` list.\n\n        Too close to the DB schema.\n        \"\"\"\n        # form set of used seq_region_id(s)\n        regions_with_ranks = frozenset(map(lambda el: el[1], other_regions_with_ranks))\n\n        # get set of seq_region_ids with bands\n        regions_with_bands = set(map(lambda el: el[1], regions_with_karyotype_bands))\n\n        # seq_region_ids list to add ranks for\n        region_ids_with_bands_but_no_karyotype_ranks = sorted(list(regions_with_bands - regions_with_ranks))\n\n        # return if nothing to add\n        if not region_ids_with_bands_but_no_karyotype_ranks:\n            return\n\n        # technical / optimization. get external_db_id for \"karyotype_rank\"\n        karyotype_rank_attrib_id = self.id_from_map_or_die(\n            \"karyotype_rank\", attrib_type_map, \"attrib_type_map\"\n        )\n\n        # set/update proper attributes for \"chromomosome\" regions\n        rank_insertions_trios = []\n        for seq_region_id in region_ids_with_bands_but_no_karyotype_ranks:\n            rank_insertions_trios.append(\n                (\n                    seq_region_id,\n                    karyotype_rank_attrib_id,\n                    len(rank_insertions_trios) + 1 + len(regions_with_ranks),\n                )\n            )\n\n        # run insertion SQL for \"karyotype_rank\"\n        #    do not alter \"coord_system_tag\" anyhow in the case of the \"chromosome\" coord_system\n        self.insert_to_db(\n            rank_insertions_trios,\n            \"seq_region_attrib\",\n            [\"seq_region_id\", \"attrib_type_id\", \"value\"],\n            self.pjc(work_dir, \"karyotype_rank_insertion\"),\n            ignore=True,\n        )\n\n    def unversion_scaffolds(self, cs_rank, logs):\n        \"\"\"\n        Unversion scaffold, remove \".\\d$\" from seq_region.names if there's a need\n\n        Non-versioned syns for contigs (lower, sequence level), versioned for the rest.\n        \"\"\"\n        seq_cs, max_rank = max([(c, r) for c, r in cs_rank.items()], key=lambda k: k[1])\n        for cs in cs_rank:\n            if cs == seq_cs:\n                # for non-sequence level cs, store original name (with version) as \"sr_syn_src\" synonym\n                xdb = self.param(\"sr_syn_src\")\n                self.copy_sr_name_to_syn(cs, xdb, self.pjc(logs, \"cp2syn\", cs))\n                self.sr_name_unversion(cs, \"seq_region_synonym\", \"synonym\", self.pjc(logs, \"unv_srs\", cs))\n            else:\n                # for sequence-level cs, store original name (with version) as \"versioned_sr_syn_src\" synonym\n                xdb = self.param(\"versioned_sr_syn_src\")\n                self.copy_sr_name_to_syn(cs, xdb, self.pjc(logs, \"cp2syn\", cs))\n                self.sr_name_unversion(cs, \"seq_region\", \"name\", self.pjc(logs, \"unv_sr\", cs))\n\n    def coord_sys_order(self, cs_order_str):\n        cs_order_lst = map(lambda x: x.strip(), cs_order_str.split(\",\"))\n        return {e: i for i, e in enumerate(filter(lambda x: len(x) &gt; 0, cs_order_lst))}\n\n    def used_cs_ranks(self, agps, cs_order, noagp_default=None):\n        # rank cs_names, met in agps.keys (\"-\" separated), i.e. \"scaffold-contig\"\n        #   only agps keys used, values are ignored\n        #   use noagp_cs_name_default for \"noagp\" assemblies\n        if agps is None:\n            if noagp_default is None:\n                raise Exception(\"NoAGP assembly with no default coordinate system name\")\n            cs_used_set = frozenset([noagp_default])\n        else:\n            cs_used_set = frozenset(sum(map(lambda x: x.split(\"-\"), agps.keys()), []))\n\n        cs_unknown = cs_used_set.difference(cs_order.keys())\n        if len(cs_unknown) &gt; 0:\n            raise Exception(\"Unknown coordinate system(s) %s\" % {str(cs_unknown)})\n        return {e: i for i, e in enumerate(sorted(cs_used_set, key=lambda x: -cs_order[x]), start=1)}\n\n    def chunk_contigs(self, fasta, cs_ranks, agps, work_dir, chunk_size=0, chunks_cs_name=\"ensembl_internal\"):\n        \"\"\"\n        chunk dna sequence fasta\n          no chunking if chunk_size &lt; 50k\n        \"\"\"\n        chunk_size_min_len = self.param_required(\"sequence_data_chunck_min_len\")\n        if chunk_size &lt; chunk_size_min_len:\n            return fasta, cs_ranks, agps\n\n        # split using script\n        en_root = self.param_required(\"ensembl_root_dir\")\n        _splitter = pj(en_root, r\"ensembl-genomio/scripts/chunk_fasta.py\")\n\n        os.makedirs(work_dir, exist_ok=True)\n\n        _stderr = f\"{work_dir}/chunking.stderr\"\n        _out_agp = f\"{work_dir}/chunks.agp\"\n        _out_fasta = f\"{work_dir}/chunks.fasta\"\n        split_cmd = f\"python {_splitter} --chunk_size {chunk_size} --agp_out {_out_agp} --out {_out_fasta} {fasta} 2&gt; {_stderr}\"\n\n        print(f\"running {split_cmd}\", file=sys.stderr)\n        # NB throws CalledProcessError if failed\n        sp.run(split_cmd, shell=True, check=True)\n\n        # add rank for chunks\n        _cs_name, _cs_rank = sorted(cs_ranks.items(), key=lambda k: k[1])[-1]\n        cs_ranks[chunks_cs_name] = _cs_rank + 1\n\n        # add agps entry\n        if agps is None:\n            agps = dict()\n        agps[f\"{_cs_name}-{chunks_cs_name}\"] = _out_agp\n\n        return _out_fasta, cs_ranks, agps\n\n    def prune_agps(self, agps, cs_order, agps_pruned_dir, pruning=True):\n        # when loading agp sort by:\n        #   highest component (cmp) cs level (lowest rank)\n        #   lowest difference between cs ranks (asm - cmp)\n        #   i.e: chromosome-scaffold scaffold-chunk chromosome-chunk\n        #   if no agps return empty pruned result\n\n        if not agps:\n            return None\n\n        agp_levels_sorted = self.order_agp_levels(agps, cs_order)\n\n        # prune agps\n        agps_pruned = dict()\n        used_components = set()\n        if not pruning:\n            used_components = None\n        for asm_cmp in agp_levels_sorted:\n            agp_file_src = agps[asm_cmp]\n            agp_file_dst = self.pjc(agps_pruned_dir, asm_cmp + \".agp\")\n            if self.agp_prune(agp_file_src, agp_file_dst, used_components) &gt; 0:\n                agps_pruned[asm_cmp] = agp_file_dst\n        return agps_pruned\n\n    def order_agp_levels(self, agps, cs_order):\n        # sort agp for loading by:\n        #   highest component (cmp) cs level (lowest rank)\n        #   lowest difference between cs ranks (asm - cmp)\n        #   i.e: chromosome-scaffold scaffold-chunk chromosome-chunk\n        if not agps:\n            return []\n\n        agp_cs_pairs = list(map(lambda x: [x] + x.split(\"-\"), agps.keys()))\n        agp_levels = [(x[0], cs_order[x[1]], cs_order[x[2]]) for x in agp_cs_pairs]\n\n        bad_agps = list(filter(lambda x: x[1] &lt; x[2], agp_levels))\n        if len(bad_agps) &gt; 0:\n            raise Exception(\"component cs has higher order than assembled cs %s\" % (str(bad_agps)))\n\n        agp_levels_sorted = [e[0] for e in sorted(agp_levels, key=lambda x: (-x[2], x[1] - x[2]))]\n        return agp_levels_sorted\n\n    def load_seq_data(self, fasta, agps, cs_rank, log_pfx):\n        \"\"\"loads sequence data for various coordinate systems accordingly with their rank\"\"\"\n        asm_v = self.asm_name()\n\n        sequence_rank = max(cs_rank.values())\n        for cs, rank in sorted(cs_rank.items(), key=lambda p: -p[1]):\n            logs = self.pjc(log_pfx, \"%02d_%s\" % (rank, cs))\n            if rank == sequence_rank:\n                self.load_cs_data(cs, rank, \"fasta\", asm_v, fasta, logs, loaded_regions=None, seq_level=True)\n            else:\n                useful_agps = list(filter(lambda x: cs in x, agps and agps.keys() or []))\n                if len(useful_agps) == 0:\n                    raise Exception(\"non-seq_level cs %s has no agps to assemble it from\" % (cs))\n                loaded_regions = set()\n                for pair, agp_file_pruned in map(lambda k: (k, agps[k]), useful_agps):\n                    if not pair.startswith(cs + \"-\"):\n                        continue\n                    self.load_cs_data(cs, rank, pair, asm_v, agp_file_pruned, logs, loaded_regions)\n\n    def load_cs_data(self, cs, rank, pair, asm_v, src_file, log_pfx, loaded_regions=None, seq_level=False):\n        \"\"\"creates a coord_system and loads sequence or assembly(AGP) data for corresponding seqregions\n\n        doesn't load already seen sequences\n        \"\"\"\n        # NB load_seq_region.pl and load_agp.pl are not failing on parameter errors (0 exit code)\n        os.makedirs(dirname(log_pfx), exist_ok=True)\n        additional_load = self.param_bool(\"load_additional_sequences\")\n        if seq_level:\n            self.load_seq_region(cs, rank, asm_v, src_file, log_pfx, seq_level, additional_load)\n        elif loaded_regions is not None:\n            new_regions = set()\n            clean_file = src_file + \".regions_deduped\"\n            self.filter_already_loaded_regions_from_agp(src_file, clean_file, loaded_regions, new_regions)\n            self.load_seq_region(cs, rank, asm_v, clean_file, log_pfx, seq_level, additional_load)\n            loaded_regions.update(new_regions)\n        if not seq_level:\n            self.load_agp(pair, asm_v, src_file, log_pfx)\n\n    def filter_already_loaded_regions_from_agp(self, src_file, dst_file, loaded_regions, new_regions):\n        with open(src_file) as src:\n            with open(dst_file, \"w\") as dst:\n                for line in src:\n                    fields = line.strip().split(\"\\t\")\n                    (\n                        asm_id,\n                        asm_start,\n                        asm_end,\n                        asm_part,\n                        type_,\n                        cmp_id,\n                        cmp_start,\n                        cmp_end,\n                        cmp_strand,\n                    ) = fields\n                    if type_ in \"NU\" or asm_id in loaded_regions:\n                        continue\n                    new_regions.add(asm_id)\n                    print(line.strip(), file=dst)\n\n    def agp_prune(self, from_file: str, to_file: str, used: set = None):\n        \"\"\"\n        Remove already components from the AGP file if they are seen in \"used\" set\n        \"\"\"\n        # reomve used component\n        #   and GAPS as they are not used by 'ensembl-analysis/scripts/assembly_loading/load_agp.pl'\n        os.makedirs(dirname(to_file), exist_ok=True)\n        open_ = self.is_gz(from_file) and gzip.open or open\n        if used is None:\n            cmd = r\"\"\"{_cat} {_file} &gt; {_out}\"\"\".format(\n                _cat=self.is_gz(from_file) and \"zcat\" or \"cat\", _file=from_file, _out=to_file\n            )\n            print(\"running %s\" % (cmd), file=sys.stderr)\n            sp.run(cmd, shell=True, check=True)\n            return 1\n        writes = 0\n        with open_(from_file, \"r\") as src:\n            with open(to_file, \"w\") as dst:\n                for line in src:\n                    fields = line.strip().split(\"\\t\")\n                    (\n                        asm_id,\n                        asm_start,\n                        asm_end,\n                        asm_part,\n                        type_,\n                        cmp_id,\n                        cmp_start,\n                        cmp_end,\n                        cmp_strand,\n                    ) = fields\n                    if type_ in \"NU\" or cmp_id in used:\n                        continue\n                    used.add(cmp_id)\n                    print(line.strip(), file=dst)\n                    writes += 1\n        return writes\n\n    def get_external_db_mapping(self) -&gt; dict:\n        \"\"\"\n        Get a map from a file for external_dbs to Ensembl dbnames from \"external_db_map\" module(!) param\n        \"\"\"\n        external_map_path = self.param(\"external_db_map\")\n        db_map = dict()\n        if external_map_path is None:\n            return db_map\n\n        # Load the map\n        with open(external_map_path, \"r\") as map_file:\n            for line in map_file:\n                if line.startswith(\"#\"):\n                    continue\n                line = re.sub(r\"#.*\", \"\", line)\n                if re.match(r\"^\\s*$\", line):\n                    continue\n                (from_name, to_name, *rest) = line.strip().split(\"\\t\")\n                if len(rest) &gt; 0 and rest[0].upper() != \"SEQ_REGION\":\n                    continue\n                if to_name == \"_IGNORE_\":\n                    continue\n                db_map[from_name] = to_name\n        return db_map\n\n    # UTILS\n    def db_string(self):\n        return \"-dbhost {host_} -dbport {port_} -dbuser {user_} -dbpass {pass_} -dbname {dbname_} \".format(\n            host_=self.param(\"dbsrv_host\"),\n            port_=self.param(\"dbsrv_port\"),\n            user_=self.param(\"dbsrv_user\"),\n            pass_=self.param(\"dbsrv_pass\"),\n            dbname_=self.param(\"db_name\"),\n        )\n\n    def pjc(self, *parts: list) -&gt; str:\n        \"\"\"\n        Join path parts and try to create every directory but the last one.\n        \"\"\"\n        if not parts:\n            return None\n\n        parts = list(parts)\n        last = parts.pop()\n        prefix = pj(*parts)\n\n        os.makedirs(prefix, exist_ok=True)\n\n        return pj(prefix, last)\n\n    def is_gz(self, filename):\n        return filename.endswith(\".gz\")\n\n    def asm_name(self):\n        asm = self.from_param(\"genome_data\", \"assembly\")\n        if \"name\" not in asm:\n            raise Exception(\"no assembly/name in genome_data\")\n        return asm[\"name\"]\n\n    # TODO: add some metafunc setter getter\n    def from_param(self, param, key, not_throw=False):\n        data = self.param_required(param)\n        if key not in data:\n            if not_throw:\n                return None\n            else:\n                raise Exception(\"Missing required %s data: %s\" % (param, key))\n        return data[key]\n\n    def param_bool(self, param):\n        val = self.param(param)\n        return bool(val) and \"0\" != val\n\n    def load_map_from_sql_stdout(self, in_file, skip_header=False):\n        \"\"\"\n        Load map from the SQL output\n\n        Process input in_file with \"key  value\" pairs and load then\n        into the {key : value} map.\n        Skips header if skip_header.\n        \"\"\"\n        data = dict()\n        with open(in_file) as pairs_file:\n            for line in pairs_file:\n                if skip_header:\n                    skip_header = False\n                    continue\n                (key, val) = line.strip().split(\"\\t\")\n                data[key] = val\n        return data\n\n    def name_and_id_from_seq_region_item(\n        self,\n        seq_region_item: dict,\n        seq_region_map: dict,\n        try_unversion: bool = False,\n        throw_missing: bool = True,\n    ) -&gt; (str, str, str):\n        \"\"\"\n        Get (seq_region_name, seq_region_id, unversioned_name) from seq_region_item struct(dict)\n\n        Gets unversioned_name only if \"try_unversion\" is True.\n        Throws exception if not able to get seq_region_id from \"seq_region_map\" and \"throw_missing\" is true.\n        \"\"\"\n        #   get seq_region_id (perhaps, by using unversioned name)\n        seq_region_name = seq_region_item[\"name\"]\n        seq_region_id = seq_region_map.get(seq_region_name, None)\n        unversioned_name = None\n        if seq_region_id is None and try_unversion:\n            # try to get seq_region_id for the unversioned name\n            unversioned_name = re.sub(r\"\\.\\d+$\", \"\", seq_region_name)\n            seq_region_id = seq_region_map.get(unversioned_name, \"\")\n\n        # oops, we don't know such seq_region name\n        if not seq_region_id and throw_missing:\n            raise Exception(f\"Not able to find seq_region for '{seq_region_name}'\")\n\n        return (seq_region_name, seq_region_id, unversioned_name)\n\n    def id_from_map_or_die(self, key: str, map_dict: dict, name_for_panic):\n        value = map_dict.get(key, None)\n        if value is None:\n            raise Exception(f\"no such key '{key}' in '{name_for_panic}' map\")\n        return value\n\n    ## Utilities using external scripts\n    def remove_IUPAC(self, from_file: str, to_file: str):\n        \"\"\"remove non-valid symbols from FASTA file (using sed) ans store the result in a different location\"\"\"\n        IUPAC = self.param(\"IUPAC\")\n        os.makedirs(dirname(to_file), exist_ok=True)\n        cmd = r\"\"\"{_cat} {_file} | sed -r '/^[^&gt;]/ {{ s/[{_IUPAC}]/N/g; s/{_iupac}/n/g }}' &gt; {_out}\"\"\".format(\n            _cat=self.is_gz(from_file) and \"zcat\" or \"cat\",\n            _file=from_file,\n            _IUPAC=IUPAC.upper(),\n            _iupac=IUPAC.lower(),\n            _out=to_file,\n        )\n        print(\"running %s\" % (cmd), file=sys.stderr)\n        return sp.run(cmd, shell=True, check=True)\n\n    def load_seq_region(\n        self,\n        cs: str,\n        rank: str,\n        asm_v: str,\n        src_file: str,\n        log_pfx: str,\n        seq_level=False,\n        additional_load=False,\n    ):\n        \"\"\"ensembl-analysis script (load_seq_region.pl) based utility for loading seq_regions FASTA sequences\"\"\"\n        en_root = self.param_required(\"ensembl_root_dir\")\n        cmd = (\n            r\"\"\"{_loader} {_db_string} {_asm_v_flag} -default_version -ignore_ambiguous_bases \"\"\"\n            + r\"\"\"    -rank {_rank} -coord_system_name {_cs} {_sl_flag} -{_tag}_file {_file}\"\"\"\n            + r\"\"\"     &gt; {_log}.stdout 2&gt; {_log}.stderr\"\"\"\n        ).format(\n            _loader=\"perl %s\"\n            % (self.pjc(en_root, r\"ensembl-analysis/scripts/assembly_loading/load_seq_region.pl\")),\n            _db_string=self.db_string(),\n            _asm_v_flag=not additional_load and f\"-coord_system_version {asm_v}\" or \"\",\n            _rank=rank,\n            _cs=cs,\n            _sl_flag=seq_level and \"-sequence_level\" or \"\",\n            _tag=seq_level and \"fasta\" or \"agp\",\n            _file=src_file,\n            _log=\"%s_seq\" % (log_pfx),\n        )\n        print(\"running %s\" % (cmd), file=sys.stderr)\n        return sp.run(cmd, shell=True, check=True)\n\n    def load_agp(self, pair, asm_v, src_file, log_pfx):\n        \"\"\"ensembl script (load_agp.pl) based utility for loading seq_regions assembly data (AGPs)\"\"\"\n        en_root = self.param_required(\"ensembl_root_dir\")\n        (asm_n, cmp_n) = pair.strip().split(\"-\")\n        cmd = (\n            r\"\"\"{_loader} {_db_string} -assembled_version {_asm_v} \"\"\"\n            + r\"\"\"    -assembled_name {_asm} -component_name {_cmp} \"\"\"\n            + r\"\"\"    -agp_file {_file} \"\"\"\n            + r\"\"\"    &gt; {_log}.stdout 2&gt; {_log}.stderr\"\"\"\n        ).format(\n            _loader=\"perl %s\" % (self.pjc(en_root, r\"ensembl-analysis/scripts/assembly_loading/load_agp.pl\")),\n            _db_string=self.db_string(),\n            _asm_v=asm_v,\n            _asm=asm_n,\n            _cmp=cmp_n,\n            _file=src_file,\n            _log=\"%s_agp_%s\" % (log_pfx, pair.replace(\"-\", \"_\")),\n        )\n        print(\"running %s\" % (cmd), file=sys.stderr)\n        return sp.run(cmd, shell=True, check=True)\n\n    def set_toplevel(self, log_pfx, ignored_cs=[]):\n        \"\"\"\n        Set toplevel(6) seq_region_attrib using ensembl script.\n\n        Uses set_toplevel.pl ensembl script.\n        \"\"\"\n        # set top_level(6) seq_region_attrib\n        os.makedirs(dirname(log_pfx), exist_ok=True)\n        en_root = self.param_required(\"ensembl_root_dir\")\n        cmd = (\n            r\"\"\"{_set_tl} {_db_string} {_ignored_cs} \"\"\" + r\"\"\"     &gt; {_log}.stdout 2&gt; {_log}.stderr\"\"\"\n        ).format(\n            _set_tl=\"perl %s\"\n            % (self.pjc(en_root, r\"ensembl-analysis/scripts/assembly_loading/set_toplevel.pl\")),\n            _db_string=self.db_string(),\n            _ignored_cs=\" \".join(map(lambda x: \"-ignore_coord_system %s\" % (x), ignored_cs)),\n            _log=log_pfx,\n        )\n        print(\"running %s\" % (cmd), file=sys.stderr)\n        sp.run(cmd, shell=True, check=True)\n\n        # remove toplevel attribute for seq_regions that are components\n        self.remove_components_from_toplevel(log_pfx)\n\n    ## SQL executor and utilities using plain SQL\n    def run_sql_req(self, sql, log_pfx, from_file=False):\n        os.makedirs(dirname(log_pfx), exist_ok=True)\n\n        sql_option = r\"\"\" -sql '{_sql}' \"\"\".format(_sql=sql)\n        if from_file:\n            sql_option = r\"\"\" &lt; '{_sql}' \"\"\".format(_sql=sql)\n\n        cmd = r\"\"\"{_dbcmd} -url \"{_srv}{_dbname}\" {_sql_option} &gt; {_out} 2&gt; {_err}\"\"\".format(\n            _dbcmd=\"perl %s/scripts/db_cmd.pl\" % os.getenv(\"EHIVE_ROOT_DIR\"),\n            _srv=self.param(\"dbsrv_url\"),\n            _dbname=self.param(\"db_name\"),\n            _sql_option=sql_option,\n            _out=log_pfx + \".stdout\",\n            _err=log_pfx + \".stderr\",\n        )\n        print(\"running %s\" % (cmd), file=sys.stderr)\n        return sp.run(cmd, shell=True, check=True)\n\n    def add_contig_ena_attrib(self, log_pfx, cs_name=\"contig\"):\n        \"\"\"\n        Add ENA attrib for contigs if their names are ENA accessions\n\n        Nno sequence_level checks are used -- just cs name.\n        See ensembl-datacheck/lib/Bio/EnsEMBL/DataCheck/Checks/SeqRegionNamesINSDC.pm .\n        SQL code.\n        \"\"\"\n        sql = r\"\"\"insert ignore into seq_region_attrib (seq_region_id, attrib_type_id, value)\n                select\n                  sr.seq_region_id, at.attrib_type_id, \"ENA\"\n                from\n                  seq_region sr, coord_system cs, attrib_type at\n                where   sr.coord_system_id = cs.coord_system_id\n                    and cs.name = \"%s\"\n                    and at.code = \"external_db\"\n              ;\"\"\" % (\n            cs_name\n        )\n        return self.run_sql_req(sql, log_pfx)\n\n    def copy_sr_name_to_syn(self, cs, x_db, log_pfx):\n        \"\"\"\n        Store original seq_region names as seq_region_synonym\n\n        Store original seq_region names (from a given cood_systen, \"cs\" param) as seq_region_synonyms (using \"x_db\" external source name)\n        SQL code.\n        \"\"\"\n        asm_v = self.asm_name()\n        sql = r\"\"\"insert into seq_region_synonym (seq_region_id, synonym, external_db_id)\n                  select\n                      sr.seq_region_id, sr.name, xdb.external_db_id\n                  from\n                     seq_region sr, external_db xdb, coord_system cs\n                  where   xdb.db_name = \"%s\"\n                      and sr.coord_system_id = cs.coord_system_id\n                      and cs.name = \"%s\"\n                      and cs.version = \"%s\"\n                      and sr.name like \"%%._\"\n                ;\"\"\" % (\n            x_db,\n            cs,\n            asm_v,\n        )\n        return self.run_sql_req(sql, log_pfx)\n\n    def add_asm_mappings(self, cs_pairs, log_pfx):\n        \"\"\"\n        Adds \"assembly.mapping\" strings to meta table.\n\n        Nullifies asm_mappings contig versions as well, but don't nullify toplevel\n        Doesn't add mapping id there is a single CS\n        SQL code.\n        \"\"\"\n        if cs_pairs is None or len(cs_pairs) &lt; 1:\n            return\n        asm_v = self.asm_name()\n        for pair in cs_pairs:\n            higher, lower = pair.strip().split(\"-\")\n            sql = r\"\"\"insert ignore into meta (species_id, meta_key, meta_value) values\n                    (1, \"assembly.mapping\", \"{_higher}:{_v}|{_lower}:{_v}\")\n                  ;\"\"\".format(\n                _v=asm_v, _higher=higher, _lower=lower\n            )\n            self.run_sql_req(sql, self.pjc(log_pfx, pair))\n\n    def remove_components_from_toplevel(self, log_pfx):\n        \"\"\"\n        Remove toplevel attribute for seq_regions that are \"components\" (parts of different seq_regions).\n\n        SQL code.\n        \"\"\"\n\n        # get list of seq_regions that are components\n        sql_not_toplevel_list = r\"\"\"\n          select distinct a.cmp_seq_region_id, sr_c.name, cs_c.name\n            from assembly a,\n                 seq_region sr_c, seq_region sr_a,\n                 coord_system cs_c, coord_system cs_a\n            where a.cmp_seq_region_id = sr_c.seq_region_id\n              and a.asm_seq_region_id = sr_a.seq_region_id\n              and sr_c.coord_system_id = cs_c.coord_system_id\n              and sr_a.coord_system_id = cs_a.coord_system_id\n              and cs_c.attrib like \"%default_version%\"\n              and cs_a.attrib like \"%default_version%\"\n              and sr_c.seq_region_id in (\n                select distinct seq_region_id\n                  from seq_region_attrib\n                  where attrib_type_id = 6 and value = 1\n              )\n        \"\"\"\n        # perhaps, make sense to check sr_c.coord_system_id != sr_a.coord_system_id\n        self.run_sql_req(sql_not_toplevel_list, \".\".join([log_pfx, \"not_toplevel_list\"]))\n\n        # delete wrongly assigned attribs\n        sql_not_toplevel_delete = r\"\"\"\n          delete from seq_region_attrib\n            where attrib_type_id = 6 and value = 1\n              and seq_region_id in (\n                select distinct a.cmp_seq_region_id\n                  from assembly a,\n                       seq_region sr_c, seq_region sr_a,\n                       coord_system cs_c, coord_system cs_a\n                  where a.cmp_seq_region_id = sr_c.seq_region_id\n                    and a.asm_seq_region_id = sr_a.seq_region_id\n                    and sr_c.coord_system_id = cs_c.coord_system_id\n                    and sr_a.coord_system_id = cs_a.coord_system_id\n                    and cs_c.attrib like \"%default_version%\"\n                    and cs_a.attrib like \"%default_version%\"\n              );\n        \"\"\"\n        # perhaps, check sr_c.coord_system_id != sr_a.coord_system_id\n        self.run_sql_req(sql_not_toplevel_delete, \".\".join([log_pfx, \"not_toplevel_delete\"]))\n\n    def sr_name_unversion(self, cs, tbl, fld, log_pfx):\n        \"\"\"\n        Remove version suffix from the seq_region names\n\n        Removes '\\.\\d+$' suffices from the seq_region names\n        SQL code.\n        \"\"\"\n        # select synonym, substr(synonym,  1, locate(\".\", synonym, length(synonym)-2)-1)\n        #     from seq_region_synonym  where synonym like \"%._\"\n        asm_v = self.asm_name()\n        sql = r\"\"\"update {_tbl} t, seq_region sr, coord_system cs\n                    set\n                      t.{_fld} = substr(t.{_fld},  1, locate(\".\", t.{_fld}, length(t.{_fld})-2)-1)\n                    where t.{_fld} like \"%._\"\n                      and t.seq_region_id = sr.seq_region_id\n                      and sr.coord_system_id = cs.coord_system_id\n                      and cs.name = \"{_cs}\"\n                      and cs.version = \"{_asm_v}\"\n                ;\"\"\".format(\n            _tbl=tbl, _fld=fld, _cs=cs, _asm_v=asm_v\n        )\n        return self.run_sql_req(sql, log_pfx)\n\n    def nullify_ctg_cs_version(self, cs_order, log_pfx: str):\n        \"\"\"\n        Nullify every CS version with rank larger than that of \"contig\", but don't nullify toplevel ones.\n\n        SQL code\n        \"\"\"\n        asm_v = self.asm_name()\n        # get cs_info (and if they have toplevel regions)\n        sql = r\"\"\"select cs.coord_system_id as coord_system_id,\n                         cs.name, cs.rank, (tl.coord_system_id is NULL) as no_toplevel\n                    from coord_system cs\n                      left join (\n                        select distinct sr.coord_system_id\n                          from seq_region sr, seq_region_attrib sra, attrib_type at\n                          where at.code = \"toplevel\"\n                            and sra.attrib_type_id = at.attrib_type_id\n                            and sra.value = 1\n                            and sra.seq_region_id = sr.seq_region_id\n                      ) as tl on tl.coord_system_id = cs.coord_system_id\n                    where cs.version = \"{_asm_v}\"\n                    order by rank\n              ;\"\"\".format(\n            _asm_v=asm_v\n        )\n        # run_sql\n        toplvl_pfx = self.pjc(log_pfx, \"toplvl_info\")\n        self.run_sql_req(sql, toplvl_pfx)\n        # load info\n        cs_info = []\n        with open(toplvl_pfx + \".stdout\") as f:\n            header = None\n            for line in f:\n                if header is None:\n                    header = line.strip().split(\"\\t\")\n                    continue\n                cs_info.append(dict(zip(header, line.strip().split())))\n        # return if there's no coord_systems to nullify versions for\n        if not cs_info:\n            return\n        # get list of known cs from cs_order to clean version from\n        nullify_cs_version_from = self.param(\"nullify_cs_version_from\")\n        if not nullify_cs_version_from or nullify_cs_version_from not in cs_order:\n            return\n        cs_thr_index = cs_order[nullify_cs_version_from]\n        cs_names_to_keep_ver = frozenset([nm for (nm, ind) in cs_order.items() if ind &gt; cs_thr_index])\n\n        # choose cs rank threshold to start clearing version from\n        clear_lst = [\n            (cs[\"coord_system_id\"], cs[\"name\"])\n            for cs in cs_info\n            if (bool(int(cs[\"no_toplevel\"])) and cs[\"name\"] not in cs_names_to_keep_ver)\n        ]\n\n        # run sql\n        if clear_lst:\n            clear_pfx = self.pjc(log_pfx, \"clear\")\n            with open(clear_pfx + \".sql\", \"w\") as clear_sql:\n                for cs_id, cs_name in clear_lst:\n                    sql = r\"\"\"\n                        update meta set\n                            meta_value=replace(meta_value, \"|{_cs_name}:{_asm_v}\", \"|{_cs_name}\")\n                            where meta_key=\"assembly.mapping\";\n                        update coord_system set version = NULL where coord_system_id = {_cs_id};\n                    \"\"\".format(\n                        _asm_v=asm_v, _cs_name=cs_name, _cs_id=cs_id\n                    )\n                    print(sql, file=clear_sql)\n            self.run_sql_req(clear_pfx + \".sql\", clear_pfx, from_file=True)\n\n    def load_map_from_core_db(self, table, cols, work_dir) -&gt; dict:\n        \"\"\"\n        Load 2 \"cols\" from core db \"table\" as map\n\n        Load { cols[0] : cols[1] } map from the core db \"table\"\n        SQL code\n        \"\"\"\n        out_pfx = self.pjc(work_dir, f\"{table}_map\")\n        sql = f\"\"\"select {cols[0]}, {cols[1]} FROM {table};\"\"\"\n        res = self.run_sql_req(sql, out_pfx)\n\n        out_file = out_pfx + \".stdout\"\n        data = self.load_map_from_sql_stdout(out_file, skip_header=True)\n        if not data:\n            raise Exception(f\"No '{table}' map loaded from '{out_file}'\")\n        return data\n\n    def load_seq_region_synonyms_trios_from_core_db(self, work_dir: str) -&gt; list:\n        # was get_db_syns\n        \"\"\"\n        Load seq_region_synonyms from from core db into [(seq_region_id, name, synonym)...] list\n\n        SQL code\n        \"\"\"\n        out_pfx = self.pjc(work_dir, f\"seq_region_synonyms\")\n        sql = r\"\"\"select sr.seq_region_id as seq_region_id, sr.name, srs.synonym\n                 from seq_region sr left join seq_region_synonym srs\n                 on sr.seq_region_id = srs.seq_region_id\n                 order by sr.seq_region_id\n              ;\"\"\"\n\n        res = self.run_sql_req(sql, out_pfx)\n\n        syn_trios = []\n        out_file = out_pfx + \".stdout\"\n        with open(out_file) as syns_file:\n            skip_header = True\n            for line in syns_file:\n                if skip_header:\n                    skip_header = False\n                    continue\n                (sr_id, name, syn) = line.strip().split(\"\\t\")\n                syn_trios.append((sr_id, name, syn))\n        return syn_trios\n\n    def insert_to_db(\n        self, list_of_tuples: list, table_name: str, col_names: list, work_dir: str, ignore: bool = True\n    ):\n        \"\"\"\n        Insert into the core db's {table_name} tuples from {list_of_tuples} as col_names.\n\n        Use `quote_or_null` (see definition below) method for string values, when putting values into `list_of_tuples`\n        SQL code\n        \"\"\"\n        # return if nothing to do\n        if not list_of_tuples:\n            return\n\n        # prepare request parts\n        ignore_str = ignore and \"IGNORE\" or \"\"\n        cols_str = \", \".join(col_names)\n\n        # generate file with the insert SQL command\n        insert_sql_file = self.pjc(work_dir, \"insert.sql\")\n        with open(insert_sql_file, \"w\") as sql:\n            print(f\"INSERT {ignore_str} INTO {table_name} ({cols_str}) VALUES\", file=sql)\n            values_sep = \"\"\n            for tpl in list_of_tuples:\n                tpl_str = \", \".join(map(str, tpl))\n                print(f\"{values_sep}({tpl_str})\", file=sql)\n                values_sep = \", \"\n            print(\";\", file=sql)\n\n        # run insert SQL from file\n        self.run_sql_req(insert_sql_file, self.pjc(work_dir, \"insert\"), from_file=True)\n\n    def quote_or_null(self, val: str, quotes: str = \"'\", null: str = \"NULL\", strings_only=True) -&gt; str:\n        \"\"\"\n        Return `val` wrapped in `quotes` or `null` value\n\n        Quotes only strings (instances of `str`) if strings_only is True.\n        \"\"\"\n        if val is None:\n            return null\n        if strings_only and isinstance(val, str):\n            return f\"{quotes}{val}{quotes}\"\n        return val\n\n    def update_db_single_group(\n        self, dict_of_col_to_value: dict, table_name: str, work_dir: str, where: str = None\n    ):\n        \"\"\"\n        Update given `table` name in db; set `col = val` for all key/value pairs from `dict_of_cols_to_values`\n\n        If `where` condition is present its value is used for the \"WHERE\" SQL clause.\n        Use `quote_or_null` (see definition below) method for string values, when putting values into `list_of_tuples`\n\n        SQL code\n        \"\"\"\n        # return if nothing to do\n        if not dict_of_col_to_value:\n            return\n\n        # prepare request parts\n        where_str = where and f\"WHERE {where}\" or \"\"\n        col_val_str = \", \".join([f\"{col} = {val}\" for col, val in dict_of_col_to_value.items()])\n\n        # generate file with the insert SQL command\n        update_sql_file = self.pjc(work_dir, \"update.sql\")\n        with open(insert_sql_file, \"w\") as sql:\n            print(f\"UPDATE {table_name} SET {col_val_str} {where_str};\", file=sql)\n            values_sep = \"\"\n            for tpl in list_of_tuples:\n                tpl_str = \", \".join(map(str, tpl))\n                print(f\"{values_sep}({tpl_str})\", file=sql)\n                values_sep = \", \"\n            print(\";\", file=sql)\n\n        # run insert SQL from file\n        self.run_sql_req(update_sql_file, self.pjc(work_dir, \"update\"), from_file=True)\n\n    def get_toplevel_from_cs(self, coord_system_name, work_dir) -&gt; list:\n        \"\"\"\n        Returns list of [ (seq_region_name, seq_region_id, \"\") ] trios for toplevel seq_regions from coord system with `coord_system_name`\n          or having  \"coord_system_tag\" attribute with the `coord_system_name` value\n\n        SQL code\n        \"\"\"\n        out_pfx = self.pjc(work_dir, f\"toplevel_from_{coord_system_name}\")\n        sql = f\"\"\"SELECT DISTINCT sr.name, sr.seq_region_id\n                FROM seq_region sr,\n                     seq_region_attrib sra,\n                     coord_system cs,\n                     attrib_type at\n                WHERE sr.seq_region_id = sra.seq_region_id\n                  AND sr.coord_system_id = cs.coord_system_id\n                  AND sra.attrib_type_id = at.attrib_type_id\n                  AND (  ( cs.name = \"{coord_system_name}\" and at.code = \"toplevel\" )\n                      OR ( at.code = \"coord_system_tag\" and sra.value = \"{coord_system_name}\" )\n                      )\n                  ORDER BY sr.seq_region_id;\n               \"\"\"\n\n        res = self.run_sql_req(sql, out_pfx)\n\n        sr_trios = []\n        out_file = out_pfx + \".stdout\"\n        with open(out_file) as sr_file:\n            skip_header = True\n            for line in sr_file:\n                if skip_header:\n                    skip_header = False\n                    continue\n                (name, sr_id) = line.strip().split(\"\\t\")\n                sr_trios.append((name, sr_id, \"\"))\n\n        return sr_trios\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.add_asm_mappings","title":"<code>add_asm_mappings(cs_pairs, log_pfx)</code>","text":"<p>Adds \"assembly.mapping\" strings to meta table.</p> <p>Nullifies asm_mappings contig versions as well, but don't nullify toplevel Doesn't add mapping id there is a single CS SQL code.</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def add_asm_mappings(self, cs_pairs, log_pfx):\n    \"\"\"\n    Adds \"assembly.mapping\" strings to meta table.\n\n    Nullifies asm_mappings contig versions as well, but don't nullify toplevel\n    Doesn't add mapping id there is a single CS\n    SQL code.\n    \"\"\"\n    if cs_pairs is None or len(cs_pairs) &lt; 1:\n        return\n    asm_v = self.asm_name()\n    for pair in cs_pairs:\n        higher, lower = pair.strip().split(\"-\")\n        sql = r\"\"\"insert ignore into meta (species_id, meta_key, meta_value) values\n                (1, \"assembly.mapping\", \"{_higher}:{_v}|{_lower}:{_v}\")\n              ;\"\"\".format(\n            _v=asm_v, _higher=higher, _lower=lower\n        )\n        self.run_sql_req(sql, self.pjc(log_pfx, pair))\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.add_contig_ena_attrib","title":"<code>add_contig_ena_attrib(log_pfx, cs_name='contig')</code>","text":"<p>Add ENA attrib for contigs if their names are ENA accessions</p> <p>Nno sequence_level checks are used -- just cs name. See ensembl-datacheck/lib/Bio/EnsEMBL/DataCheck/Checks/SeqRegionNamesINSDC.pm . SQL code.</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def add_contig_ena_attrib(self, log_pfx, cs_name=\"contig\"):\n    \"\"\"\n    Add ENA attrib for contigs if their names are ENA accessions\n\n    Nno sequence_level checks are used -- just cs name.\n    See ensembl-datacheck/lib/Bio/EnsEMBL/DataCheck/Checks/SeqRegionNamesINSDC.pm .\n    SQL code.\n    \"\"\"\n    sql = r\"\"\"insert ignore into seq_region_attrib (seq_region_id, attrib_type_id, value)\n            select\n              sr.seq_region_id, at.attrib_type_id, \"ENA\"\n            from\n              seq_region sr, coord_system cs, attrib_type at\n            where   sr.coord_system_id = cs.coord_system_id\n                and cs.name = \"%s\"\n                and at.code = \"external_db\"\n          ;\"\"\" % (\n        cs_name\n    )\n    return self.run_sql_req(sql, log_pfx)\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.add_karyotype_bands","title":"<code>add_karyotype_bands(seq_region_file, seq_region_map, attrib_type_map, work_dir, unversion=False, karyotype_bands_property='karyotype_bands')</code>","text":"<p>Add karyotypic data from the seq_region metafile.</p> <p>Add karyotypic data from the schemas/seq_region_schema.json compatible meta data file. Returns list of [ (seq_region_name, seq_region_id, unversioned_name) ] trios for seq_regions having karyotype bands info.</p> If unversion is true <ul> <li>the unversioned synonym would be used to get the seq_region_id from \"seq_region_map\" if possible</li> </ul> <p>Too close to the DB schema.</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def add_karyotype_bands(\n    self,\n    seq_region_file: str,\n    seq_region_map: dict,\n    attrib_type_map: dict,\n    work_dir: str,\n    unversion: bool = False,\n    karyotype_bands_property=\"karyotype_bands\",\n) -&gt; list:  # [ (seq_region_name, seq_region_id, unversioned_name) ]\n    \"\"\"\n    Add karyotypic data from the seq_region metafile.\n\n    Add karyotypic data from the schemas/seq_region_schema.json compatible meta data file.\n    Returns list of [ (seq_region_name, seq_region_id, unversioned_name) ] trios for seq_regions having karyotype bands info.\n\n    If unversion is true:\n      * the unversioned synonym would be used to get the seq_region_id from \"seq_region_map\" if possible\n\n    Too close to the DB schema.\n    \"\"\"\n    os.makedirs(work_dir, exist_ok=True)\n\n    # return if there's nothing to add\n    if not seq_region_file:\n        return\n\n    # resulting list of seq regions with bands\n    seq_regions_with_karyotype_bands = []  # [ ( seq_region_name, seq_region_id, unversioned_name )... ]\n\n    # load BRC4/EBI name from seq_region file\n    band_tuples = (\n        []\n    )  # [ (seq_region_id, seq_region_start, seq_region_end, band|\"NULL\", stain|\"NULL\")... ] list of tuples for inserting into db\n    with open(seq_region_file) as in_file:\n        seq_regions = list(json.load(in_file))\n        for seq_region in filter(lambda sr: sr.get(karyotype_bands_property, False), seq_regions):\n            # iterate through all seq_regions having non-empty \"karyotype_bands\"\n\n            # get seq_region_id (perhaps, by using unversioned name)\n            seq_region_name, seq_region_id, unversioned_name = self.name_and_id_from_seq_region_item(\n                seq_region, seq_region_map, try_unversion=unversion\n            )\n\n            # append trio to the resulting list\n            seq_regions_with_karyotype_bands.append((seq_region_name, seq_region_id, unversioned_name))\n\n            # append bands to the band_tuples list\n            for band in seq_region[karyotype_bands_property]:\n                # print(\"BAND: \" + str(band), file = sys.stderr)\n                # coords\n                seq_region_start = band[\"start\"]\n                seq_region_end = band[\"end\"]\n                # band_name and stain\n                band_name = band.get(\"name\", None)\n                stain = band.get(\"stain\", None)\n                # special cases for stain\n                structure = band.get(\"structure\", None)\n                if structure == \"telomere\":\n                    stain = \"TEL\"\n                elif structure == \"centromere\":\n                    stain = \"ACEN\"\n\n                # append tuple\n                band_tuples.append(\n                    (\n                        seq_region_id,\n                        seq_region_start,\n                        seq_region_end,\n                        self.quote_or_null(band_name),\n                        self.quote_or_null(stain),\n                    )\n                )\n\n    # run insertion SQL\n    self.insert_to_db(\n        band_tuples,\n        \"karyotype\",\n        [\"seq_region_id\", \"seq_region_start\", \"seq_region_end\", \"band\", \"stain\"],\n        self.pjc(work_dir, \"karyotype_insertion\"),\n        ignore=True,\n    )\n\n    # return resulting list of regions with bands trios\n    return seq_regions_with_karyotype_bands\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.add_karyotype_data","title":"<code>add_karyotype_data(seq_region_file, seq_region_map, attrib_type_map, work_dir, unversion=False)</code>","text":"<p>Adds various karyotypic data from seq_region file and assembly metadata (if present).</p> <p>Adds various karyotypic data from the schemas/seq_region_schema.json compatible meta data file and assembly metadata (if present).</p> If unversion is true <ul> <li>the unversioned synonym would be used to get the seq_region_id from \"seq_region_map\" if possible</li> </ul> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def add_karyotype_data(\n    self,\n    seq_region_file: str,\n    seq_region_map: dict,\n    attrib_type_map: dict,\n    work_dir: str,\n    unversion: bool = False,\n):\n    \"\"\"\n    Adds various karyotypic data from seq_region file and assembly metadata (if present).\n\n    Adds various karyotypic data from the schemas/seq_region_schema.json compatible meta data file and assembly metadata (if present).\n\n    If unversion is true:\n      * the unversioned synonym would be used to get the seq_region_id from \"seq_region_map\" if possible\n    \"\"\"\n    # add karyotyope bands data\n    regions_with_karyotype_bands = self.add_karyotype_bands(\n        seq_region_file,\n        seq_region_map,\n        attrib_type_map,\n        self.pjc(work_dir, \"karyotype_bands\"),\n        unversion=unversion,\n    )\n\n    # try to add karyotype ranks for regions listed in genome_data/assembly/chromosome_display_order metadata\n    regions_with_ranks_from_assembly_metadata = self.add_karyotype_rank_based_on_assembly_metadata(\n        seq_region_map,\n        attrib_type_map,\n        self.pjc(work_dir, \"karyotype_ranks_from_meta\"),\n        unversion=unversion,\n    )\n\n    regions_with_ranks_from_chromosome_cs = []\n    if not regions_with_ranks_from_assembly_metadata:\n        # try to add karyotype_ranks for top-level regions from the \"chromosome\" coord_system\n        regions_with_ranks_from_chromosome_cs = self.add_karyotype_rank_for_chromosomes(\n            attrib_type_map, self.pjc(work_dir, \"karyotype_ranks_for_chromosomes\")\n        )\n\n    # make sure that regions with bands have karyotype_ranks\n    self.add_karyotype_rank_from_bands_info(\n        regions_with_karyotype_bands,\n        regions_with_ranks_from_chromosome_cs + regions_with_ranks_from_assembly_metadata,\n        attrib_type_map,\n        self.pjc(work_dir, \"karyotype_ranks_from_bands\"),\n    )\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.add_karyotype_rank_based_on_assembly_metadata","title":"<code>add_karyotype_rank_based_on_assembly_metadata(seq_region_map, attrib_type_map, work_dir, unversion=True)</code>","text":"<p>Add <code>karyotype_rank</code> attributes for seq region data from based on metadata from the \"genome_data\" module parameter. Add only to the seq_regions with ids listed in the array corresponding to 'genome_data/assembly/chromosome_display_order'.</p> <p>Set \"coord_system_tag\" attribute to the one listed in the \"cs_tag_for_ordered\" module param; or \"chromosome\" if param value is underfined. Force updating of the \"coord_system_tags\" if <code>force_update_coord_system_tag</code> module param is True.</p> <p>Returns list of [ (seq_region_name, seq_region_id, unversioned_name) ] trios for seq_regions with updated karyotype_ranks.</p> If unversion is true <ul> <li>the unversioned synonym would be used to get the seq_region_id from \"seq_region_map\" if possible</li> </ul> <p>Too close to the DB schema.</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def add_karyotype_rank_based_on_assembly_metadata(\n    self, seq_region_map: dict, attrib_type_map: dict, work_dir: str, unversion: bool = True\n) -&gt; list:  # [ (seq_region_name, seq_region_id, unversioned_name) ]\n    \"\"\"\n    Add `karyotype_rank` attributes for seq region data from based on metadata from the \"genome_data\" module parameter.\n    Add only to the seq_regions with ids listed in the array corresponding to 'genome_data/assembly/chromosome_display_order'.\n\n    Set \"coord_system_tag\" attribute to the one listed in the \"cs_tag_for_ordered\" module param; or \"chromosome\" if param value is underfined.\n    Force updating of the \"coord_system_tags\" if `force_update_coord_system_tag` module param is True.\n\n    Returns list of [ (seq_region_name, seq_region_id, unversioned_name) ] trios for seq_regions with updated karyotype_ranks.\n\n    If unversion is true:\n      * the unversioned synonym would be used to get the seq_region_id from \"seq_region_map\" if possible\n\n    Too close to the DB schema.\n    \"\"\"\n\n    # resulting list of seq_region with karyotype_rank\n    regions_with_ranks_from_assembly_metadata = (\n        []\n    )  # [ ( seq_region_name, seq_region_id, unversioned_name )... ]\n\n    # get `chromosome_display_order` list from the assembly metadata\n    assembly_metadata = self.from_param(\"genome_data\", \"assembly\", not_throw=True) or dict()\n    chromosome_display_order_list = assembly_metadata.get(\"chromosome_display_order\", [])\n\n    # technical / optimization. get external_db_id for \"karyotype_rank\" and \"coord_system_tag\"\n    karyotype_rank_attrib_id = self.id_from_map_or_die(\n        \"karyotype_rank\", attrib_type_map, \"attrib_type_map\"\n    )\n    coord_system_tag_attrib_id = self.id_from_map_or_die(\n        \"coord_system_tag\", attrib_type_map, \"attrib_type_map\"\n    )\n    coord_system_tag = self.param(\"cs_tag_for_ordered\") or \"chromosome\"\n    force_update_coord_system_tag = self.param(\"force_update_coord_system_tag\") or False\n\n    # set/update proper attributes for `chromosome_display_order_list` regions\n    rank_insertions_trios = []\n    coord_system_tag_attrib_insertion_trios = []\n    coord_system_tag_attrib_seq_region_update_ids = []\n\n    for seq_region_name_raw in chromosome_display_order_list:\n        # wrap seq_region_name_raw into seq_region struct { \"name\": seq_region_name_raw } and\n        # get seq_region_id (perhaps, by using unversioned name)\n        seq_region_name, seq_region_id, unversioned_name = self.name_and_id_from_seq_region_item(\n            {\"name\": seq_region_name_raw}, seq_region_map, try_unversion=unversion\n        )\n\n        # append trio to the resulting list\n        regions_with_ranks_from_assembly_metadata.append(\n            (seq_region_name, seq_region_id, unversioned_name)\n        )\n\n        # filling insert lists for \"karyotype_rank\" and \"coord_system_tag\" attributes\n        rank_insertions_trios.append(\n            (seq_region_id, karyotype_rank_attrib_id, len(rank_insertions_trios) + 1)\n        )\n        coord_system_tag_attrib_insertion_trios.append(\n            (seq_region_id, coord_system_tag_attrib_id, self.quote_or_null(coord_system_tag))\n        )\n\n        # filling update list for \"coord_system_tag\" with seq_region_ids\n        coord_system_tag_attrib_seq_region_update_ids.append(seq_region_id)\n\n    # run insertion SQL for \"karyotype_rank\"\n    self.insert_to_db(\n        rank_insertions_trios,\n        \"seq_region_attrib\",\n        [\"seq_region_id\", \"attrib_type_id\", \"value\"],\n        self.pjc(work_dir, \"karyotype_rank_insertion\"),\n        ignore=True,\n    )\n\n    # run insertion SQL for \"coord_system_tag\"\n    self.insert_to_db(\n        coord_system_tag_attrib_insertion_trios,\n        \"seq_region_attrib\",\n        [\"seq_region_id\", \"attrib_type_id\", \"value\"],\n        self.pjc(work_dir, \"coord_system_tag_insertion\"),\n        ignore=True,\n    )\n\n    # forcing update of the \"coord_system_tag\"\n    if force_update_coord_system_tag and coord_system_tag_attrib_seq_region_update_ids:\n        seq_region_ids_str = \",\".join(map(str, coord_system_tag_attrib_seq_region_update_ids))\n        self.update_db_single_group(\n            {\"value\": self.quote_or_null(coord_system_tag)},\n            \"seq_region_attrib\",\n            self.pjc(work_dir, \"coord_system_tag_update\"),\n            where=f\"attrib_type_id = {coord_system_tag_attrib_id} and seq_region_id in ({seq_region_ids_str})\",\n        )\n\n    # return resulting list of regions with bands trios\n    return regions_with_ranks_from_assembly_metadata\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.add_karyotype_rank_for_chromosomes","title":"<code>add_karyotype_rank_for_chromosomes(attrib_type_map, work_dir, chromosome_coord_system_name='chromosome')</code>","text":"<p>Add <code>karyotype_rank</code> attributes for seq region data from the \"chromosome\" coordinate system.</p> <p>Returns list of [ (seq_region_name, seq_region_id, unversioned_name) ] trios for seq_regions with updated karyotype_ranks. Not altering \"coord_system_tag\" tag attributes.</p> If unversion is true <ul> <li>the unversioned synonym would be used to get the seq_region_id from \"seq_region_map\" if possible</li> </ul> <p>Too close to the DB schema.</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def add_karyotype_rank_for_chromosomes(\n    self, attrib_type_map: dict, work_dir: str, chromosome_coord_system_name=\"chromosome\"\n) -&gt; list:  # [ (seq_region_name, seq_region_id, unversioned_name) ]\n    \"\"\"\n    Add `karyotype_rank` attributes for seq region data from the \"chromosome\" coordinate system.\n\n    Returns list of [ (seq_region_name, seq_region_id, unversioned_name) ] trios for seq_regions with updated karyotype_ranks.\n    Not altering \"coord_system_tag\" tag attributes.\n\n    If unversion is true:\n      * the unversioned synonym would be used to get the seq_region_id from \"seq_region_map\" if possible\n\n    Too close to the DB schema.\n    \"\"\"\n\n    # resulting list of seq_region with karyotype_rank\n    #   list of top level seq regions from the `chromosome_coord_system_name`\n    chromomes_seq_regions = self.get_toplevel_from_cs(\n        chromosome_coord_system_name, self.pjc(work_dir, \"chromosome_seq_regions\")\n    )\n\n    if not chromomes_seq_regions:\n        return chromomes_seq_regions\n\n    # technical / optimization. get external_db_id for \"karyotype_rank\"\n    karyotype_rank_attrib_id = self.id_from_map_or_die(\n        \"karyotype_rank\", attrib_type_map, \"attrib_type_map\"\n    )\n\n    # set/update proper attributes for \"chromomosome\" regions\n    rank_insertions_trios = []\n    for _, seq_region_id, _ in chromomes_seq_regions:\n        rank_insertions_trios.append(\n            (seq_region_id, karyotype_rank_attrib_id, len(rank_insertions_trios) + 1)\n        )\n\n    # run insertion SQL for \"karyotype_rank\"\n    #    do not alter \"coord_system_tag\" anyhow in the case of the \"chromosome\" coord_system\n    self.insert_to_db(\n        rank_insertions_trios,\n        \"seq_region_attrib\",\n        [\"seq_region_id\", \"attrib_type_id\", \"value\"],\n        self.pjc(work_dir, \"karyotype_rank_insertion\"),\n        ignore=True,\n    )\n\n    return chromomes_seq_regions\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.add_karyotype_rank_from_bands_info","title":"<code>add_karyotype_rank_from_bands_info(regions_with_karyotype_bands, other_regions_with_ranks, attrib_type_map, work_dir)</code>","text":"<p>Add karyotype_ranks for <code>regions_with_karyotype_bands</code> (those with karyotype bands in seq_region metadata) but not present in <code>other_regions_with_ranks</code> list.</p> <p>Too close to the DB schema.</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def add_karyotype_rank_from_bands_info(\n    self,\n    regions_with_karyotype_bands: list,  # [ (seq_region_name, seq_region_id, unversioned_name) ]\n    other_regions_with_ranks: list,  # [ (seq_region_name, seq_region_id, unversioned_name) ]\n    attrib_type_map: dict,\n    work_dir: str,\n):\n    \"\"\"\n    Add karyotype_ranks for `regions_with_karyotype_bands` (those with karyotype bands in seq_region metadata) but not present in `other_regions_with_ranks` list.\n\n    Too close to the DB schema.\n    \"\"\"\n    # form set of used seq_region_id(s)\n    regions_with_ranks = frozenset(map(lambda el: el[1], other_regions_with_ranks))\n\n    # get set of seq_region_ids with bands\n    regions_with_bands = set(map(lambda el: el[1], regions_with_karyotype_bands))\n\n    # seq_region_ids list to add ranks for\n    region_ids_with_bands_but_no_karyotype_ranks = sorted(list(regions_with_bands - regions_with_ranks))\n\n    # return if nothing to add\n    if not region_ids_with_bands_but_no_karyotype_ranks:\n        return\n\n    # technical / optimization. get external_db_id for \"karyotype_rank\"\n    karyotype_rank_attrib_id = self.id_from_map_or_die(\n        \"karyotype_rank\", attrib_type_map, \"attrib_type_map\"\n    )\n\n    # set/update proper attributes for \"chromomosome\" regions\n    rank_insertions_trios = []\n    for seq_region_id in region_ids_with_bands_but_no_karyotype_ranks:\n        rank_insertions_trios.append(\n            (\n                seq_region_id,\n                karyotype_rank_attrib_id,\n                len(rank_insertions_trios) + 1 + len(regions_with_ranks),\n            )\n        )\n\n    # run insertion SQL for \"karyotype_rank\"\n    #    do not alter \"coord_system_tag\" anyhow in the case of the \"chromosome\" coord_system\n    self.insert_to_db(\n        rank_insertions_trios,\n        \"seq_region_attrib\",\n        [\"seq_region_id\", \"attrib_type_id\", \"value\"],\n        self.pjc(work_dir, \"karyotype_rank_insertion\"),\n        ignore=True,\n    )\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.add_sr_attribs","title":"<code>add_sr_attribs(seq_region_file, seq_region_map, attrib_type_map, work_dir, unversion=False)</code>","text":"<p>Add seq_region_attrib(s) from the seq_region_file meta data file.</p> <p>Explicit list is taken from \"sr_attrib_types\" module param.</p> <p>Add seq_region_attrib(s) from the schemas/seq_region_schema.json compatible meta data file. Explicit list is taken from \"sr_attrib_types\" module param.</p> <p>\"sr_attrib_types\" defines { json_property -&gt; attrib_type.name } map. If the value is dict, its keys are treated as \"/\"-delimetered \"json_path\" (i.e. \"added_sequence/assembly_provider/name\"). No arrays can be processed. Only simple or \"flattable\" types.</p> If unversion is true <ul> <li>the unversioned synonym would be used to get the seq_region_id from \"seq_region_map\" if possible</li> </ul> <p>Too close to the DB schema.</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def add_sr_attribs(\n    self,\n    seq_region_file: str,\n    seq_region_map: dict,\n    attrib_type_map: dict,\n    work_dir,\n    unversion: bool = False,\n):\n    \"\"\"\n    Add seq_region_attrib(s) from the seq_region_file meta data file.\n\n    Explicit list is taken from \"sr_attrib_types\" module param.\n\n    Add seq_region_attrib(s) from the schemas/seq_region_schema.json compatible meta data file.\n    Explicit list is taken from \"sr_attrib_types\" module param.\n\n    \"sr_attrib_types\" defines { json_property -&gt; attrib_type.name } map. If the value is dict,\n    its keys are treated as \"/\"-delimetered \"json_path\" (i.e. \"added_sequence/assembly_provider/name\").\n    No arrays can be processed. Only simple or \"flattable\" types.\n\n    If unversion is true:\n      * the unversioned synonym would be used to get the seq_region_id from \"seq_region_map\" if possible\n\n    Too close to the DB schema.\n    \"\"\"\n    os.makedirs(work_dir, exist_ok=True)\n\n    # return if there's nothing to add\n    if not seq_region_file:\n        return\n\n    # technical / optimization. get atttib_type_id(s)\n    # create a smaller map with attrib_type_id(s) as values\n    properties_to_use = (\n        []\n    )  # [frozen]set with the top-level \"seq_region\" properties, that should be processed\n    path_attrib_id_map = dict()  # { \"flatterned/json/paths\" : attrib_id_map ))}\n    # fill set and map\n    for prop, attrib_type in self.param(\"sr_attrib_types\").items():\n        # adding high level properties to process\n        properties_to_use.append(prop)\n        # adding json paths (or properties themselves) to atrrib_type_id map\n        if isinstance(attrib_type, dict):  # if using json paths (delimeterd with \"/\")\n            for path, inner_attrib_type in attrib_type.items():\n                path_attrib_id_map[path] = self.id_from_map_or_die(\n                    inner_attrib_type, attrib_type_map, \"attrib_type_map\"\n                )\n        else:\n            path_attrib_id_map[prop] = self.id_from_map_or_die(\n                attrib_type, attrib_type_map, \"attrib_type_map\"\n            )\n    # return if there's nothing to add\n    if not properties_to_use:\n        return\n\n    properties_to_use = frozenset(properties_to_use)\n\n    # load attributes from seq_region file\n    attrib_trios = []  # [ (seq_region_id, attrib_id, value)... ] list of trios for inserting into db\n    with open(seq_region_file) as in_file:\n        seq_regions = list(json.load(in_file))\n        for seq_region in seq_regions:\n            # get seq_region_id (perhaps, by using unversioned name)\n            seq_region_name, seq_region_id, unversioned_name = self.name_and_id_from_seq_region_item(\n                seq_region, seq_region_map, try_unversion=unversion\n            )\n\n            # iterate through properties\n            for prop_name in properties_to_use:\n                if prop_name not in seq_region:\n                    continue\n                # flattern path\n                path_attrib_id_values_list = self.flattern_seq_region_item(\n                    seq_region, prop_name, path_attrib_id_map\n                )\n                # fill attrib_trios\n                for path, attrib_id, value in path_attrib_id_values_list:\n                    attrib_trios.append((seq_region_id, attrib_id, self.quote_or_null(value)))\n\n    # run insertion SQL\n    self.insert_to_db(\n        attrib_trios,\n        \"seq_region_attrib\",\n        [\"seq_region_id\", \"attrib_type_id\", \"value\"],\n        self.pjc(work_dir, \"brc4_ebi_seq_region_synonyms\"),\n        ignore=True,\n    )\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.add_sr_ebi_brc4_names","title":"<code>add_sr_ebi_brc4_names(seq_region_file, seq_region_map, attrib_type_map, work_dir, unversion=False)</code>","text":"<p>Add \"(EBI|BRC4)_seq_region_name\" seq_region_attrib(s) either from the seq_region_file meta data file, or from original seq_region names.</p> <p>Add \"(EBI|BRC4)_seq_region_name\" seq_region_synonym from the schemas/seq_region_schema.json compatible meta data file or from the original seq_region_names. A special case of attributes adding with default values derived from seq_region names.</p> If unversion is true <ul> <li>the unversioned synonym would be used to get the seq_region_id from \"seq_region_map\" if possible</li> </ul> <p>Too close to the DB schema.</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def add_sr_ebi_brc4_names(\n    self,\n    seq_region_file: str,\n    seq_region_map: dict,\n    attrib_type_map: dict,\n    work_dir: str,\n    unversion: bool = False,\n):\n    \"\"\"\n    Add \"(EBI|BRC4)_seq_region_name\" seq_region_attrib(s) either from the seq_region_file meta data file, or from original seq_region names.\n\n    Add \"(EBI|BRC4)_seq_region_name\" seq_region_synonym from the schemas/seq_region_schema.json compatible meta data file or from the original seq_region_names.\n    A special case of attributes adding with default values derived from seq_region names.\n\n    If unversion is true:\n      * the unversioned synonym would be used to get the seq_region_id from \"seq_region_map\" if possible\n\n    Too close to the DB schema.\n    \"\"\"\n    os.makedirs(work_dir, exist_ok=True)\n\n    # return if there's nothing to add\n    if not seq_region_file:\n        return\n\n    # technical / optimization. get atttib_type_id(s) for \"(EBI|BRC4)_seq_region_name\"\n    tagged_sr_name_attrib_id = {\n        tag: self.id_from_map_or_die(f\"{tag}_seq_region_name\", attrib_type_map, \"attrib_type_map\")\n        for tag in [\"EBI\", \"BRC4\"]\n    }\n\n    # load BRC4/EBI name from seq_region file\n    brc4_ebi_name_attrib_trios = (\n        []\n    )  # [ (seq_region_id, attrib_id, value)... ] list of trios for inserting into db\n    with open(seq_region_file) as in_file:\n        seq_regions = list(json.load(in_file))\n        for seq_region in seq_regions:\n            # get seq_region_id (perhaps, by using unversioned name)\n            seq_region_name, seq_region_id, unversioned_name = self.name_and_id_from_seq_region_item(\n                seq_region, seq_region_map, try_unversion=unversion\n            )\n            # append attribs to the brc4_ebi_name_attrib_trios list\n            for tag in [\"BRC4\", \"EBI\"]:\n                attrib_name = f\"{tag}_seq_region_name\"\n                attrib_id = tagged_sr_name_attrib_id[tag]\n                value = seq_region.get(attrib_name, seq_region_name)\n                brc4_ebi_name_attrib_trios.append((seq_region_id, attrib_id, self.quote_or_null(value)))\n\n    # run insertion SQL\n    self.insert_to_db(\n        brc4_ebi_name_attrib_trios,\n        \"seq_region_attrib\",\n        [\"seq_region_id\", \"attrib_type_id\", \"value\"],\n        self.pjc(work_dir, \"brc4_ebi_seq_region_synonyms\"),\n        ignore=True,\n    )\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.add_sr_synonyms","title":"<code>add_sr_synonyms(seq_region_file, seq_region_map, external_db_map, work_dir, unversion=False, unversionable_sources_set=frozenset(['INSDC', 'RefSeq']))</code>","text":"<p>Add seq_region_synonym from the seq_region_file meta data file.</p> <p>Add seq_region_synonym from the schemas/seq_region_schema.json compatible meta data file. Merge with the already exinsting ones in the db.</p> If unversion is true <ul> <li>the unversioned synonym would be used to get the seq_region_id from \"seq_region_map\" if possible</li> <li>the unversioned synonyms from the unversionable_sources_set will be added as well as the original ones</li> </ul> <p>Too close to the DB schema.</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def add_sr_synonyms(\n    self,\n    seq_region_file: str,\n    seq_region_map: dict,\n    external_db_map: dict,\n    work_dir: str,\n    unversion: bool = False,\n    unversionable_sources_set: frozenset = frozenset([\"INSDC\", \"RefSeq\"]),\n):\n    \"\"\"\n    Add seq_region_synonym from the seq_region_file meta data file.\n\n    Add seq_region_synonym from the schemas/seq_region_schema.json compatible meta data file.\n    Merge with the already exinsting ones in the db.\n\n    If unversion is true:\n      * the unversioned synonym would be used to get the seq_region_id from \"seq_region_map\" if possible\n      * the unversioned synonyms from the unversionable_sources_set will be added as well as the original ones\n\n    Too close to the DB schema.\n    \"\"\"\n    os.makedirs(work_dir, exist_ok=True)\n\n    # return if there's nothing to add\n    if not seq_region_file:\n        return\n\n    # get seq_region ids, names, syns from db\n    synonyms_trios_db = self.load_seq_region_synonyms_trios_from_core_db(\n        self.pjc(work_dir, \"syns_from_core\")\n    )\n\n    # form set of synonyms already present in db\n    synonyms_in_db = frozenset([trio[2] for trio in synonyms_trios_db if trio[2] != \"NULL\"])\n\n    # subset of sources to use the allowed the unversion synonyms\n    unversionable_sources = unversion and unversionable_sources_set or frozenset()\n\n    # technical / optimization. get external_db_id for \"ensembl_internal_synonym\"\n    ensembl_internal_synonym_ext_db_id = self.id_from_map_or_die(\n        \"ensembl_internal_synonym\", external_db_map, \"external_db_map\"\n    )\n\n    # get dict for additional mapping for sources to external_db names if there's one specified by \"external_db_map\" module param\n    #   not to be confused with the external_db_map function parameter above\n    additional_sources_mapping = self.get_external_db_mapping()\n\n    # load synonyms from the json file\n    synonyms_from_json = (\n        []\n    )  # [ (seq_region_id, synonym, external_db_id)... ] list of trios for inserting into db\n    with open(seq_region_file) as in_file:\n        seq_regions = list(json.load(in_file))\n        for seq_region in filter(lambda sr: sr.get(\"synonyms\", False), seq_regions):\n            # iterate through all seq_regions having \"synonyms\"\n            seq_region_name, seq_region_id, _ = self.name_and_id_from_seq_region_item(\n                seq_region, seq_region_map, try_unversion=unversion\n            )\n\n            # fill synonyms_from_json list of trios\n            for synonym_item in list(seq_region[\"synonyms\"]):\n                synonym_name = synonym_item[\"name\"]\n                source = synonym_item[\"source\"]\n                unversioned_name = \"\"\n\n                # check if there's any addtional mapping for the source, remap if so\n                if additional_sources_mapping:\n                    source = additional_sources_mapping.get(\n                        source, source\n                    )  # use the same name if no matches in additional_sources_mapping dict\n\n                # try to get unversioned name if applicable\n                if source in unversionable_sources:\n                    unversioned_name = re.sub(r\"\\.\\d+$\", \"\", synonym_name)\n\n                # put trios if names are not already seen in db\n                if synonym_name not in synonyms_in_db:\n                    external_db_id = self.id_from_map_or_die(source, external_db_map, \"external_db_map\")\n                    synonyms_from_json.append(\n                        (seq_region_id, self.quote_or_null(synonym_name), external_db_id)\n                    )\n\n                #   put additional unversioned synonyms if there's a sane one\n                if (\n                    unversioned_name\n                    and unversioned_name != synonym_name\n                    and unversioned_name not in synonyms_in_db\n                ):\n                    synonyms_from_json.append(\n                        (\n                            seq_region_id,\n                            self.quote_or_null(unversioned_name),\n                            ensembl_internal_synonym_ext_db_id,\n                        )\n                    )\n\n    # run insertion SQL\n    self.insert_to_db(\n        synonyms_from_json,\n        \"seq_region_synonym\",\n        [\"seq_region_id\", \"synonym\", \"external_db_id\"],\n        self.pjc(work_dir, \"new_seq_region_synonyms\"),\n        ignore=True,\n    )\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.agp_prune","title":"<code>agp_prune(from_file, to_file, used=None)</code>","text":"<p>Remove already components from the AGP file if they are seen in \"used\" set</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def agp_prune(self, from_file: str, to_file: str, used: set = None):\n    \"\"\"\n    Remove already components from the AGP file if they are seen in \"used\" set\n    \"\"\"\n    # reomve used component\n    #   and GAPS as they are not used by 'ensembl-analysis/scripts/assembly_loading/load_agp.pl'\n    os.makedirs(dirname(to_file), exist_ok=True)\n    open_ = self.is_gz(from_file) and gzip.open or open\n    if used is None:\n        cmd = r\"\"\"{_cat} {_file} &gt; {_out}\"\"\".format(\n            _cat=self.is_gz(from_file) and \"zcat\" or \"cat\", _file=from_file, _out=to_file\n        )\n        print(\"running %s\" % (cmd), file=sys.stderr)\n        sp.run(cmd, shell=True, check=True)\n        return 1\n    writes = 0\n    with open_(from_file, \"r\") as src:\n        with open(to_file, \"w\") as dst:\n            for line in src:\n                fields = line.strip().split(\"\\t\")\n                (\n                    asm_id,\n                    asm_start,\n                    asm_end,\n                    asm_part,\n                    type_,\n                    cmp_id,\n                    cmp_start,\n                    cmp_end,\n                    cmp_strand,\n                ) = fields\n                if type_ in \"NU\" or cmp_id in used:\n                    continue\n                used.add(cmp_id)\n                print(line.strip(), file=dst)\n                writes += 1\n    return writes\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.chunk_contigs","title":"<code>chunk_contigs(fasta, cs_ranks, agps, work_dir, chunk_size=0, chunks_cs_name='ensembl_internal')</code>","text":"<p>chunk dna sequence fasta   no chunking if chunk_size &lt; 50k</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def chunk_contigs(self, fasta, cs_ranks, agps, work_dir, chunk_size=0, chunks_cs_name=\"ensembl_internal\"):\n    \"\"\"\n    chunk dna sequence fasta\n      no chunking if chunk_size &lt; 50k\n    \"\"\"\n    chunk_size_min_len = self.param_required(\"sequence_data_chunck_min_len\")\n    if chunk_size &lt; chunk_size_min_len:\n        return fasta, cs_ranks, agps\n\n    # split using script\n    en_root = self.param_required(\"ensembl_root_dir\")\n    _splitter = pj(en_root, r\"ensembl-genomio/scripts/chunk_fasta.py\")\n\n    os.makedirs(work_dir, exist_ok=True)\n\n    _stderr = f\"{work_dir}/chunking.stderr\"\n    _out_agp = f\"{work_dir}/chunks.agp\"\n    _out_fasta = f\"{work_dir}/chunks.fasta\"\n    split_cmd = f\"python {_splitter} --chunk_size {chunk_size} --agp_out {_out_agp} --out {_out_fasta} {fasta} 2&gt; {_stderr}\"\n\n    print(f\"running {split_cmd}\", file=sys.stderr)\n    # NB throws CalledProcessError if failed\n    sp.run(split_cmd, shell=True, check=True)\n\n    # add rank for chunks\n    _cs_name, _cs_rank = sorted(cs_ranks.items(), key=lambda k: k[1])[-1]\n    cs_ranks[chunks_cs_name] = _cs_rank + 1\n\n    # add agps entry\n    if agps is None:\n        agps = dict()\n    agps[f\"{_cs_name}-{chunks_cs_name}\"] = _out_agp\n\n    return _out_fasta, cs_ranks, agps\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.copy_sr_name_to_syn","title":"<code>copy_sr_name_to_syn(cs, x_db, log_pfx)</code>","text":"<p>Store original seq_region names as seq_region_synonym</p> <p>Store original seq_region names (from a given cood_systen, \"cs\" param) as seq_region_synonyms (using \"x_db\" external source name) SQL code.</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def copy_sr_name_to_syn(self, cs, x_db, log_pfx):\n    \"\"\"\n    Store original seq_region names as seq_region_synonym\n\n    Store original seq_region names (from a given cood_systen, \"cs\" param) as seq_region_synonyms (using \"x_db\" external source name)\n    SQL code.\n    \"\"\"\n    asm_v = self.asm_name()\n    sql = r\"\"\"insert into seq_region_synonym (seq_region_id, synonym, external_db_id)\n              select\n                  sr.seq_region_id, sr.name, xdb.external_db_id\n              from\n                 seq_region sr, external_db xdb, coord_system cs\n              where   xdb.db_name = \"%s\"\n                  and sr.coord_system_id = cs.coord_system_id\n                  and cs.name = \"%s\"\n                  and cs.version = \"%s\"\n                  and sr.name like \"%%._\"\n            ;\"\"\" % (\n        x_db,\n        cs,\n        asm_v,\n    )\n    return self.run_sql_req(sql, log_pfx)\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.flattern_seq_region_item","title":"<code>flattern_seq_region_item(seq_region, prop_name, path_attrib_id_map, sep='/')</code>","text":"<p>Flattern seq_region[property] and store corresponding [ (json_path, attrib_id, value)... ] (as list of trios).</p> <p>Only works for simple properties or dicts with no arrays on the path. Basically, implemets tree traversal. Utility function used by the <code>add_sr_attribs</code> method</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def flattern_seq_region_item(\n    self, seq_region: dict, prop_name: str, path_attrib_id_map: dict, sep: str = \"/\"\n) -&gt; list:\n    \"\"\"\n    Flattern seq_region[property] and store corresponding [ (json_path, attrib_id, value)... ] (as list of trios).\n\n    Only works for simple properties or dicts with no arrays on the path. Basically, implemets tree traversal.\n    Utility function used by the `add_sr_attribs` method\n    \"\"\"\n    res = []\n    # is there anything to do\n    if prop_name not in seq_region:\n        return res\n\n    # set up\n    value = seq_region[prop_name]\n    paths_to_go = [\n        (prop_name, value)\n    ]  # storing path and the corresponding value, to prevent repetetive traversals\n    # iterate\n    while paths_to_go:\n        (path, value) = paths_to_go.pop()  # get last item\n        if isinstance(value, list):\n            # perhaps, it's better to raise exception then to continue silently\n            continue\n        if isinstance(value, dict):\n            # if value is a complex object, add its leaves\n            for key, val in value.items():\n                paths_to_go.append((f\"{path}{sep}{key}\", val))\n            continue\n        # if value is simple\n        attrib_id = path_attrib_id_map.get(path, None)\n        if attrib_id:\n            res.append((path, attrib_id, value))\n    # return what ever we have\n    return res\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.get_external_db_mapping","title":"<code>get_external_db_mapping()</code>","text":"<p>Get a map from a file for external_dbs to Ensembl dbnames from \"external_db_map\" module(!) param</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def get_external_db_mapping(self) -&gt; dict:\n    \"\"\"\n    Get a map from a file for external_dbs to Ensembl dbnames from \"external_db_map\" module(!) param\n    \"\"\"\n    external_map_path = self.param(\"external_db_map\")\n    db_map = dict()\n    if external_map_path is None:\n        return db_map\n\n    # Load the map\n    with open(external_map_path, \"r\") as map_file:\n        for line in map_file:\n            if line.startswith(\"#\"):\n                continue\n            line = re.sub(r\"#.*\", \"\", line)\n            if re.match(r\"^\\s*$\", line):\n                continue\n            (from_name, to_name, *rest) = line.strip().split(\"\\t\")\n            if len(rest) &gt; 0 and rest[0].upper() != \"SEQ_REGION\":\n                continue\n            if to_name == \"_IGNORE_\":\n                continue\n            db_map[from_name] = to_name\n    return db_map\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.get_toplevel_from_cs","title":"<code>get_toplevel_from_cs(coord_system_name, work_dir)</code>","text":"<p>Returns list of [ (seq_region_name, seq_region_id, \"\") ] trios for toplevel seq_regions from coord system with <code>coord_system_name</code>   or having  \"coord_system_tag\" attribute with the <code>coord_system_name</code> value</p> <p>SQL code</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def get_toplevel_from_cs(self, coord_system_name, work_dir) -&gt; list:\n    \"\"\"\n    Returns list of [ (seq_region_name, seq_region_id, \"\") ] trios for toplevel seq_regions from coord system with `coord_system_name`\n      or having  \"coord_system_tag\" attribute with the `coord_system_name` value\n\n    SQL code\n    \"\"\"\n    out_pfx = self.pjc(work_dir, f\"toplevel_from_{coord_system_name}\")\n    sql = f\"\"\"SELECT DISTINCT sr.name, sr.seq_region_id\n            FROM seq_region sr,\n                 seq_region_attrib sra,\n                 coord_system cs,\n                 attrib_type at\n            WHERE sr.seq_region_id = sra.seq_region_id\n              AND sr.coord_system_id = cs.coord_system_id\n              AND sra.attrib_type_id = at.attrib_type_id\n              AND (  ( cs.name = \"{coord_system_name}\" and at.code = \"toplevel\" )\n                  OR ( at.code = \"coord_system_tag\" and sra.value = \"{coord_system_name}\" )\n                  )\n              ORDER BY sr.seq_region_id;\n           \"\"\"\n\n    res = self.run_sql_req(sql, out_pfx)\n\n    sr_trios = []\n    out_file = out_pfx + \".stdout\"\n    with open(out_file) as sr_file:\n        skip_header = True\n        for line in sr_file:\n            if skip_header:\n                skip_header = False\n                continue\n            (name, sr_id) = line.strip().split(\"\\t\")\n            sr_trios.append((name, sr_id, \"\"))\n\n    return sr_trios\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.initial_sequence_loading","title":"<code>initial_sequence_loading(work_dir)</code>","text":"<p>initial preparation and loading of AGPs and fasta data.</p> <p>initial preparation and loading of AGPs and fasta data using ensembl-analysis perl scripts</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def initial_sequence_loading(self, work_dir: str):\n    \"\"\"\n    initial preparation and loading of AGPs and fasta data.\n\n    initial preparation and loading of AGPs and fasta data using ensembl-analysis perl scripts\n    \"\"\"\n    # preprocess FASTA with sequences\n    #   rename IUPAC to N symbols using sed\n    fasta_clean = self.from_param(\"manifest_data\", \"fasta_dna\")\n\n    # start coord system ranking and agps processing\n    agps = self.from_param(\"manifest_data\", \"agp\", not_throw=True)\n\n    # get the deafult coord_system order\n    #   use noagp_cs_name_default for \"noagp\" assemblies\n    cs_order = self.coord_sys_order(self.param(\"cs_order\"))\n    noagps_cs = self.param(\"noagp_cs_name_default\")\n\n    # remove gaps and lower_level mappings if the are coveres by higher level ones\n    #   i.e.: remove 'contigN to chromosomeZ', if 'contigN to scaffoldM' and 'scaffoldM to chromosomeZ' are in place\n    #   returns None if no agps provided\n    agps_pruned_dir = self.pjc(work_dir, \"agps_pruned\")\n    agps_pruned = self.prune_agps(agps, cs_order, agps_pruned_dir, self.param_bool(\"prune_agp\"))\n\n    # order\n    # rank cs_names, met in agps.keys (\"-\" separated, i.e. \"scaffold-contig\") based on cs_order\n    cs_rank = self.used_cs_ranks(agps_pruned, cs_order, noagps_cs)\n\n    # chunk sequence data if needed\n    #   no chunking if chunk_size &lt; 50k\n    chunk_size = int(self.param(\"sequence_data_chunck\"))\n    chunk_cs_name = self.param(\"chunk_cs_name\")\n    fasta_clean, cs_rank, agps_pruned = self.chunk_contigs(\n        fasta_clean,\n        cs_rank,\n        agps_pruned,\n        pj(work_dir, \"chuncking\"),\n        chunk_size=chunk_size,\n        chunks_cs_name=chunk_cs_name,\n    )\n\n    # empty agps_pruned ignored\n    self.load_seq_data(fasta_clean, agps_pruned, cs_rank, self.pjc(work_dir, \"load\"))\n\n    # mark all the \"contig\"s or noagp_cs as being sourced from ENA\n    if not self.param_bool(\"no_contig_ena_attrib\"):\n        # NB using original \"agps\" parameter (with no chuncking data added)\n        agps_raw = self.from_param(\"manifest_data\", \"agp\", not_throw=True)\n        if agps_raw is None:\n            self.add_contig_ena_attrib(self.pjc(work_dir, \"load\", \"set_ena\"), cs_name=noagps_cs)\n        else:\n            self.add_contig_ena_attrib(self.pjc(work_dir, \"load\", \"set_ena\"))\n\n    # unversion scaffold, remove \".\\d$\" from names if there's a need\n    if self.param_bool(\"unversion_scaffolds\"):\n        self.unversion_scaffolds(cs_rank, self.pjc(work_dir, \"unversion_scaffolds\"))\n\n    # add assembly mappings between various cs to meta table for the mapper to work properly\n    cs_pairs = agps_pruned and agps_pruned.keys() or None\n    self.add_asm_mappings(cs_pairs, self.pjc(work_dir, \"asm_mappings\"))\n\n    # set toplevel seq_region attribute\n    self.set_toplevel(self.pjc(work_dir, \"set_toplevel\"), self.param(\"not_toplevel_cs\"))\n\n    # nullify contig version and update mappings strings accordingly; ignore for \"load_additional_sequences\" mode\n    if not self.param_bool(\"load_additional_sequences\"):\n        self.nullify_ctg_cs_version(cs_order, self.pjc(work_dir, \"asm_mapping\", \"nullify_cs_versions\"))\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.insert_to_db","title":"<code>insert_to_db(list_of_tuples, table_name, col_names, work_dir, ignore=True)</code>","text":"<p>Insert into the core db's {table_name} tuples from {list_of_tuples} as col_names.</p> <p>Use <code>quote_or_null</code> (see definition below) method for string values, when putting values into <code>list_of_tuples</code> SQL code</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def insert_to_db(\n    self, list_of_tuples: list, table_name: str, col_names: list, work_dir: str, ignore: bool = True\n):\n    \"\"\"\n    Insert into the core db's {table_name} tuples from {list_of_tuples} as col_names.\n\n    Use `quote_or_null` (see definition below) method for string values, when putting values into `list_of_tuples`\n    SQL code\n    \"\"\"\n    # return if nothing to do\n    if not list_of_tuples:\n        return\n\n    # prepare request parts\n    ignore_str = ignore and \"IGNORE\" or \"\"\n    cols_str = \", \".join(col_names)\n\n    # generate file with the insert SQL command\n    insert_sql_file = self.pjc(work_dir, \"insert.sql\")\n    with open(insert_sql_file, \"w\") as sql:\n        print(f\"INSERT {ignore_str} INTO {table_name} ({cols_str}) VALUES\", file=sql)\n        values_sep = \"\"\n        for tpl in list_of_tuples:\n            tpl_str = \", \".join(map(str, tpl))\n            print(f\"{values_sep}({tpl_str})\", file=sql)\n            values_sep = \", \"\n        print(\";\", file=sql)\n\n    # run insert SQL from file\n    self.run_sql_req(insert_sql_file, self.pjc(work_dir, \"insert\"), from_file=True)\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.load_agp","title":"<code>load_agp(pair, asm_v, src_file, log_pfx)</code>","text":"<p>ensembl script (load_agp.pl) based utility for loading seq_regions assembly data (AGPs)</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def load_agp(self, pair, asm_v, src_file, log_pfx):\n    \"\"\"ensembl script (load_agp.pl) based utility for loading seq_regions assembly data (AGPs)\"\"\"\n    en_root = self.param_required(\"ensembl_root_dir\")\n    (asm_n, cmp_n) = pair.strip().split(\"-\")\n    cmd = (\n        r\"\"\"{_loader} {_db_string} -assembled_version {_asm_v} \"\"\"\n        + r\"\"\"    -assembled_name {_asm} -component_name {_cmp} \"\"\"\n        + r\"\"\"    -agp_file {_file} \"\"\"\n        + r\"\"\"    &gt; {_log}.stdout 2&gt; {_log}.stderr\"\"\"\n    ).format(\n        _loader=\"perl %s\" % (self.pjc(en_root, r\"ensembl-analysis/scripts/assembly_loading/load_agp.pl\")),\n        _db_string=self.db_string(),\n        _asm_v=asm_v,\n        _asm=asm_n,\n        _cmp=cmp_n,\n        _file=src_file,\n        _log=\"%s_agp_%s\" % (log_pfx, pair.replace(\"-\", \"_\")),\n    )\n    print(\"running %s\" % (cmd), file=sys.stderr)\n    return sp.run(cmd, shell=True, check=True)\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.load_cs_data","title":"<code>load_cs_data(cs, rank, pair, asm_v, src_file, log_pfx, loaded_regions=None, seq_level=False)</code>","text":"<p>creates a coord_system and loads sequence or assembly(AGP) data for corresponding seqregions</p> <p>doesn't load already seen sequences</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def load_cs_data(self, cs, rank, pair, asm_v, src_file, log_pfx, loaded_regions=None, seq_level=False):\n    \"\"\"creates a coord_system and loads sequence or assembly(AGP) data for corresponding seqregions\n\n    doesn't load already seen sequences\n    \"\"\"\n    # NB load_seq_region.pl and load_agp.pl are not failing on parameter errors (0 exit code)\n    os.makedirs(dirname(log_pfx), exist_ok=True)\n    additional_load = self.param_bool(\"load_additional_sequences\")\n    if seq_level:\n        self.load_seq_region(cs, rank, asm_v, src_file, log_pfx, seq_level, additional_load)\n    elif loaded_regions is not None:\n        new_regions = set()\n        clean_file = src_file + \".regions_deduped\"\n        self.filter_already_loaded_regions_from_agp(src_file, clean_file, loaded_regions, new_regions)\n        self.load_seq_region(cs, rank, asm_v, clean_file, log_pfx, seq_level, additional_load)\n        loaded_regions.update(new_regions)\n    if not seq_level:\n        self.load_agp(pair, asm_v, src_file, log_pfx)\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.load_map_from_core_db","title":"<code>load_map_from_core_db(table, cols, work_dir)</code>","text":"<p>Load 2 \"cols\" from core db \"table\" as map</p> <p>Load { cols[0] : cols[1] } map from the core db \"table\" SQL code</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def load_map_from_core_db(self, table, cols, work_dir) -&gt; dict:\n    \"\"\"\n    Load 2 \"cols\" from core db \"table\" as map\n\n    Load { cols[0] : cols[1] } map from the core db \"table\"\n    SQL code\n    \"\"\"\n    out_pfx = self.pjc(work_dir, f\"{table}_map\")\n    sql = f\"\"\"select {cols[0]}, {cols[1]} FROM {table};\"\"\"\n    res = self.run_sql_req(sql, out_pfx)\n\n    out_file = out_pfx + \".stdout\"\n    data = self.load_map_from_sql_stdout(out_file, skip_header=True)\n    if not data:\n        raise Exception(f\"No '{table}' map loaded from '{out_file}'\")\n    return data\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.load_map_from_sql_stdout","title":"<code>load_map_from_sql_stdout(in_file, skip_header=False)</code>","text":"<p>Load map from the SQL output</p> <p>Process input in_file with \"key  value\" pairs and load then into the {key : value} map. Skips header if skip_header.</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def load_map_from_sql_stdout(self, in_file, skip_header=False):\n    \"\"\"\n    Load map from the SQL output\n\n    Process input in_file with \"key  value\" pairs and load then\n    into the {key : value} map.\n    Skips header if skip_header.\n    \"\"\"\n    data = dict()\n    with open(in_file) as pairs_file:\n        for line in pairs_file:\n            if skip_header:\n                skip_header = False\n                continue\n            (key, val) = line.strip().split(\"\\t\")\n            data[key] = val\n    return data\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.load_seq_data","title":"<code>load_seq_data(fasta, agps, cs_rank, log_pfx)</code>","text":"<p>loads sequence data for various coordinate systems accordingly with their rank</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def load_seq_data(self, fasta, agps, cs_rank, log_pfx):\n    \"\"\"loads sequence data for various coordinate systems accordingly with their rank\"\"\"\n    asm_v = self.asm_name()\n\n    sequence_rank = max(cs_rank.values())\n    for cs, rank in sorted(cs_rank.items(), key=lambda p: -p[1]):\n        logs = self.pjc(log_pfx, \"%02d_%s\" % (rank, cs))\n        if rank == sequence_rank:\n            self.load_cs_data(cs, rank, \"fasta\", asm_v, fasta, logs, loaded_regions=None, seq_level=True)\n        else:\n            useful_agps = list(filter(lambda x: cs in x, agps and agps.keys() or []))\n            if len(useful_agps) == 0:\n                raise Exception(\"non-seq_level cs %s has no agps to assemble it from\" % (cs))\n            loaded_regions = set()\n            for pair, agp_file_pruned in map(lambda k: (k, agps[k]), useful_agps):\n                if not pair.startswith(cs + \"-\"):\n                    continue\n                self.load_cs_data(cs, rank, pair, asm_v, agp_file_pruned, logs, loaded_regions)\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.load_seq_region","title":"<code>load_seq_region(cs, rank, asm_v, src_file, log_pfx, seq_level=False, additional_load=False)</code>","text":"<p>ensembl-analysis script (load_seq_region.pl) based utility for loading seq_regions FASTA sequences</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def load_seq_region(\n    self,\n    cs: str,\n    rank: str,\n    asm_v: str,\n    src_file: str,\n    log_pfx: str,\n    seq_level=False,\n    additional_load=False,\n):\n    \"\"\"ensembl-analysis script (load_seq_region.pl) based utility for loading seq_regions FASTA sequences\"\"\"\n    en_root = self.param_required(\"ensembl_root_dir\")\n    cmd = (\n        r\"\"\"{_loader} {_db_string} {_asm_v_flag} -default_version -ignore_ambiguous_bases \"\"\"\n        + r\"\"\"    -rank {_rank} -coord_system_name {_cs} {_sl_flag} -{_tag}_file {_file}\"\"\"\n        + r\"\"\"     &gt; {_log}.stdout 2&gt; {_log}.stderr\"\"\"\n    ).format(\n        _loader=\"perl %s\"\n        % (self.pjc(en_root, r\"ensembl-analysis/scripts/assembly_loading/load_seq_region.pl\")),\n        _db_string=self.db_string(),\n        _asm_v_flag=not additional_load and f\"-coord_system_version {asm_v}\" or \"\",\n        _rank=rank,\n        _cs=cs,\n        _sl_flag=seq_level and \"-sequence_level\" or \"\",\n        _tag=seq_level and \"fasta\" or \"agp\",\n        _file=src_file,\n        _log=\"%s_seq\" % (log_pfx),\n    )\n    print(\"running %s\" % (cmd), file=sys.stderr)\n    return sp.run(cmd, shell=True, check=True)\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.load_seq_region_synonyms_trios_from_core_db","title":"<code>load_seq_region_synonyms_trios_from_core_db(work_dir)</code>","text":"<p>Load seq_region_synonyms from from core db into [(seq_region_id, name, synonym)...] list</p> <p>SQL code</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def load_seq_region_synonyms_trios_from_core_db(self, work_dir: str) -&gt; list:\n    # was get_db_syns\n    \"\"\"\n    Load seq_region_synonyms from from core db into [(seq_region_id, name, synonym)...] list\n\n    SQL code\n    \"\"\"\n    out_pfx = self.pjc(work_dir, f\"seq_region_synonyms\")\n    sql = r\"\"\"select sr.seq_region_id as seq_region_id, sr.name, srs.synonym\n             from seq_region sr left join seq_region_synonym srs\n             on sr.seq_region_id = srs.seq_region_id\n             order by sr.seq_region_id\n          ;\"\"\"\n\n    res = self.run_sql_req(sql, out_pfx)\n\n    syn_trios = []\n    out_file = out_pfx + \".stdout\"\n    with open(out_file) as syns_file:\n        skip_header = True\n        for line in syns_file:\n            if skip_header:\n                skip_header = False\n                continue\n            (sr_id, name, syn) = line.strip().split(\"\\t\")\n            syn_trios.append((sr_id, name, syn))\n    return syn_trios\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.name_and_id_from_seq_region_item","title":"<code>name_and_id_from_seq_region_item(seq_region_item, seq_region_map, try_unversion=False, throw_missing=True)</code>","text":"<p>Get (seq_region_name, seq_region_id, unversioned_name) from seq_region_item struct(dict)</p> <p>Gets unversioned_name only if \"try_unversion\" is True. Throws exception if not able to get seq_region_id from \"seq_region_map\" and \"throw_missing\" is true.</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def name_and_id_from_seq_region_item(\n    self,\n    seq_region_item: dict,\n    seq_region_map: dict,\n    try_unversion: bool = False,\n    throw_missing: bool = True,\n) -&gt; (str, str, str):\n    \"\"\"\n    Get (seq_region_name, seq_region_id, unversioned_name) from seq_region_item struct(dict)\n\n    Gets unversioned_name only if \"try_unversion\" is True.\n    Throws exception if not able to get seq_region_id from \"seq_region_map\" and \"throw_missing\" is true.\n    \"\"\"\n    #   get seq_region_id (perhaps, by using unversioned name)\n    seq_region_name = seq_region_item[\"name\"]\n    seq_region_id = seq_region_map.get(seq_region_name, None)\n    unversioned_name = None\n    if seq_region_id is None and try_unversion:\n        # try to get seq_region_id for the unversioned name\n        unversioned_name = re.sub(r\"\\.\\d+$\", \"\", seq_region_name)\n        seq_region_id = seq_region_map.get(unversioned_name, \"\")\n\n    # oops, we don't know such seq_region name\n    if not seq_region_id and throw_missing:\n        raise Exception(f\"Not able to find seq_region for '{seq_region_name}'\")\n\n    return (seq_region_name, seq_region_id, unversioned_name)\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.nullify_ctg_cs_version","title":"<code>nullify_ctg_cs_version(cs_order, log_pfx)</code>","text":"<p>Nullify every CS version with rank larger than that of \"contig\", but don't nullify toplevel ones.</p> <p>SQL code</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def nullify_ctg_cs_version(self, cs_order, log_pfx: str):\n    \"\"\"\n    Nullify every CS version with rank larger than that of \"contig\", but don't nullify toplevel ones.\n\n    SQL code\n    \"\"\"\n    asm_v = self.asm_name()\n    # get cs_info (and if they have toplevel regions)\n    sql = r\"\"\"select cs.coord_system_id as coord_system_id,\n                     cs.name, cs.rank, (tl.coord_system_id is NULL) as no_toplevel\n                from coord_system cs\n                  left join (\n                    select distinct sr.coord_system_id\n                      from seq_region sr, seq_region_attrib sra, attrib_type at\n                      where at.code = \"toplevel\"\n                        and sra.attrib_type_id = at.attrib_type_id\n                        and sra.value = 1\n                        and sra.seq_region_id = sr.seq_region_id\n                  ) as tl on tl.coord_system_id = cs.coord_system_id\n                where cs.version = \"{_asm_v}\"\n                order by rank\n          ;\"\"\".format(\n        _asm_v=asm_v\n    )\n    # run_sql\n    toplvl_pfx = self.pjc(log_pfx, \"toplvl_info\")\n    self.run_sql_req(sql, toplvl_pfx)\n    # load info\n    cs_info = []\n    with open(toplvl_pfx + \".stdout\") as f:\n        header = None\n        for line in f:\n            if header is None:\n                header = line.strip().split(\"\\t\")\n                continue\n            cs_info.append(dict(zip(header, line.strip().split())))\n    # return if there's no coord_systems to nullify versions for\n    if not cs_info:\n        return\n    # get list of known cs from cs_order to clean version from\n    nullify_cs_version_from = self.param(\"nullify_cs_version_from\")\n    if not nullify_cs_version_from or nullify_cs_version_from not in cs_order:\n        return\n    cs_thr_index = cs_order[nullify_cs_version_from]\n    cs_names_to_keep_ver = frozenset([nm for (nm, ind) in cs_order.items() if ind &gt; cs_thr_index])\n\n    # choose cs rank threshold to start clearing version from\n    clear_lst = [\n        (cs[\"coord_system_id\"], cs[\"name\"])\n        for cs in cs_info\n        if (bool(int(cs[\"no_toplevel\"])) and cs[\"name\"] not in cs_names_to_keep_ver)\n    ]\n\n    # run sql\n    if clear_lst:\n        clear_pfx = self.pjc(log_pfx, \"clear\")\n        with open(clear_pfx + \".sql\", \"w\") as clear_sql:\n            for cs_id, cs_name in clear_lst:\n                sql = r\"\"\"\n                    update meta set\n                        meta_value=replace(meta_value, \"|{_cs_name}:{_asm_v}\", \"|{_cs_name}\")\n                        where meta_key=\"assembly.mapping\";\n                    update coord_system set version = NULL where coord_system_id = {_cs_id};\n                \"\"\".format(\n                    _asm_v=asm_v, _cs_name=cs_name, _cs_id=cs_id\n                )\n                print(sql, file=clear_sql)\n        self.run_sql_req(clear_pfx + \".sql\", clear_pfx, from_file=True)\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.param_defaults","title":"<code>param_defaults()</code>","text":"<p>default parameter/options values</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def param_defaults(self):\n    \"\"\"\n    default parameter/options values\n    \"\"\"\n    return {\n        # relative order of the coord_systems types to infer their rank from\n        \"cs_order\": \"ensembl_internal,chunk,contig,supercontig,non_ref_scaffold,scaffold,primary_assembly,superscaffold,linkage_group,chromosome\",\n        \"IUPAC\": \"RYKMSWBDHV\",  # symbols to be replaced with N in the DNA sequences (ensembl core(107) doesn't support the whole IUPAC alphabet for DNA)\n        # unversion scaffold, remove \".\\d$\" from seq_region.names if there's a need\n        \"unversion_scaffolds\": 0,\n        \"versioned_sr_syn_src\": \"INSDC\",  # INSDC(50710) # if unversioning non-sequence level cs, store original name (with version) as this synonym\n        \"sr_syn_src\": \"BRC4_Community_Symbol\",  # BRC4_Community_Symbol(211) # if unversioning sequence-level cs, store original name (with version) as this synonym\n        # nullify coord_system version for the given coord_system name\n        \"nullify_cs_version_from\": \"contig\",\n        # default coord_system name for single-level (no AGPs assemblies)\n        \"noagp_cs_name_default\": \"primary_assembly\",\n        # file for additional mapping of the synonym sources to the external_db (ensembl), as used by \"get_external_db_mapping\" function below\n        \"external_db_map\": None,\n        # set \"coord_system_tag\" seq_region attribute to this value if there's a corresponding \"chromosome_display_order\" list in genome.json metadata\n        #   if None, only \"chromosome\" coord system is processed (if present)\n        #   (see add_chr_karyotype_rank definition below )\n        #   if seq_region already has `coord_system_tagi` attribute, it value updated only if \"force_update_coord_system_tag\" module param is True (see below)\n        \"cs_tag_for_ordered\": None,\n        # Force updating of the \"coord_system_tags\" attribute for seq_regions from `cs_tag_for_ordered` (see above)\n        \"force_update_coord_system_tag\": False,\n        # BRC4 compatibility mode; if on, \"(EBI|BRC4)_seq_region_name\" seq_region_attributes are added.\n        #   Blocked by the \"swap_gcf_gca\" option. In this case insertion should be done on later pipeline stage after seq_region name swapping.\n        \"brc4_mode\": True,\n        # Whether to use RefSeq names as additional seq_region synonyms (if available) or not (see add_sr_synonyms definition below)\n        #  Does not swap anything actually, just loads synonyms to be used by a later \"swapping\" stage\n        #  Disables BRC4 compatibilty mode (see the \"brc4_mode\" option comment).\n        \"swap_gcf_gca\": False,\n        # list of coord systems used in \"-ignore_coord_system\" options of the \"ensembl-analysis/scripts/assembly_loading/set_toplevel.pl\" script\n        #   part of the loading process\n        \"not_toplevel_cs\": [],  # i.e. \"contig\", \"non_ref_scaffold\"\n        # explicit list of seq_region properties (keys) to load as seq_region_attrib s (values) (see add_sr_attribs definition below)\n        #   if a dict's used as a value, treat its keys as \"json_path\" (/ as delim) map, i.e.\n        #       { \"added_sequence\" : { \"assembly_provider\" : { \"name\" : ... } } } -&gt; \"added_sequence/assembly_provider/name\"\n        #   only flattable properties can be used, no arrays\n        #   arrays should be processed separately (see `add_sr_synonyms` or `add_karyotype_bands` definitions)\n        # see schemas/seq_region_schema.json\n        \"sr_attrib_types\": {\n            \"circular\": \"circular_seq\",\n            \"codon_table\": \"codon_table\",\n            \"location\": \"sequence_location\",\n            \"non_ref\": \"non_ref\",\n            \"coord_system_level\": \"coord_system_tag\",\n            \"added_sequence\": {\n                # json_path to attrib_type_code(str) mapping\n                \"added_sequence/accession\": \"added_seq_accession\",\n                \"added_sequence/assembly_provider/name\": \"added_seq_asm_pr_nam\",\n                \"added_sequence/assembly_provider/url\": \"added_seq_asm_pr_url\",\n                \"added_sequence/annotation_provider/name\": \"added_seq_ann_pr_nam\",\n                \"added_sequence/annotation_provider/url\": \"added_seq_ann_pr_url\",\n            },  # added_sequence\n        },\n        # loading additional sequences to the already exsisting core db\n        \"load_additional_sequences\": 0,\n        # size of the sequence data chunk, if 0 (default), no chunking is performed\n        \"sequence_data_chunck\": 0,\n        #   min size of the sequence chunk, no chunking is done if 'sequence_data_chunck' &lt; 'sequence_data_chunck_min'\n        \"sequence_data_chunck_min_len\": 50_000,\n        # coord system name for chunks\n        \"chunk_cs_name\": \"ensembl_internal\",\n    }\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.pjc","title":"<code>pjc(*parts)</code>","text":"<p>Join path parts and try to create every directory but the last one.</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def pjc(self, *parts: list) -&gt; str:\n    \"\"\"\n    Join path parts and try to create every directory but the last one.\n    \"\"\"\n    if not parts:\n        return None\n\n    parts = list(parts)\n    last = parts.pop()\n    prefix = pj(*parts)\n\n    os.makedirs(prefix, exist_ok=True)\n\n    return pj(prefix, last)\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.quote_or_null","title":"<code>quote_or_null(val, quotes=\"'\", null='NULL', strings_only=True)</code>","text":"<p>Return <code>val</code> wrapped in <code>quotes</code> or <code>null</code> value</p> <p>Quotes only strings (instances of <code>str</code>) if strings_only is True.</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def quote_or_null(self, val: str, quotes: str = \"'\", null: str = \"NULL\", strings_only=True) -&gt; str:\n    \"\"\"\n    Return `val` wrapped in `quotes` or `null` value\n\n    Quotes only strings (instances of `str`) if strings_only is True.\n    \"\"\"\n    if val is None:\n        return null\n    if strings_only and isinstance(val, str):\n        return f\"{quotes}{val}{quotes}\"\n    return val\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.remove_IUPAC","title":"<code>remove_IUPAC(from_file, to_file)</code>","text":"<p>remove non-valid symbols from FASTA file (using sed) ans store the result in a different location</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def remove_IUPAC(self, from_file: str, to_file: str):\n    \"\"\"remove non-valid symbols from FASTA file (using sed) ans store the result in a different location\"\"\"\n    IUPAC = self.param(\"IUPAC\")\n    os.makedirs(dirname(to_file), exist_ok=True)\n    cmd = r\"\"\"{_cat} {_file} | sed -r '/^[^&gt;]/ {{ s/[{_IUPAC}]/N/g; s/{_iupac}/n/g }}' &gt; {_out}\"\"\".format(\n        _cat=self.is_gz(from_file) and \"zcat\" or \"cat\",\n        _file=from_file,\n        _IUPAC=IUPAC.upper(),\n        _iupac=IUPAC.lower(),\n        _out=to_file,\n    )\n    print(\"running %s\" % (cmd), file=sys.stderr)\n    return sp.run(cmd, shell=True, check=True)\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.remove_components_from_toplevel","title":"<code>remove_components_from_toplevel(log_pfx)</code>","text":"<p>Remove toplevel attribute for seq_regions that are \"components\" (parts of different seq_regions).</p> <p>SQL code.</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def remove_components_from_toplevel(self, log_pfx):\n    \"\"\"\n    Remove toplevel attribute for seq_regions that are \"components\" (parts of different seq_regions).\n\n    SQL code.\n    \"\"\"\n\n    # get list of seq_regions that are components\n    sql_not_toplevel_list = r\"\"\"\n      select distinct a.cmp_seq_region_id, sr_c.name, cs_c.name\n        from assembly a,\n             seq_region sr_c, seq_region sr_a,\n             coord_system cs_c, coord_system cs_a\n        where a.cmp_seq_region_id = sr_c.seq_region_id\n          and a.asm_seq_region_id = sr_a.seq_region_id\n          and sr_c.coord_system_id = cs_c.coord_system_id\n          and sr_a.coord_system_id = cs_a.coord_system_id\n          and cs_c.attrib like \"%default_version%\"\n          and cs_a.attrib like \"%default_version%\"\n          and sr_c.seq_region_id in (\n            select distinct seq_region_id\n              from seq_region_attrib\n              where attrib_type_id = 6 and value = 1\n          )\n    \"\"\"\n    # perhaps, make sense to check sr_c.coord_system_id != sr_a.coord_system_id\n    self.run_sql_req(sql_not_toplevel_list, \".\".join([log_pfx, \"not_toplevel_list\"]))\n\n    # delete wrongly assigned attribs\n    sql_not_toplevel_delete = r\"\"\"\n      delete from seq_region_attrib\n        where attrib_type_id = 6 and value = 1\n          and seq_region_id in (\n            select distinct a.cmp_seq_region_id\n              from assembly a,\n                   seq_region sr_c, seq_region sr_a,\n                   coord_system cs_c, coord_system cs_a\n              where a.cmp_seq_region_id = sr_c.seq_region_id\n                and a.asm_seq_region_id = sr_a.seq_region_id\n                and sr_c.coord_system_id = cs_c.coord_system_id\n                and sr_a.coord_system_id = cs_a.coord_system_id\n                and cs_c.attrib like \"%default_version%\"\n                and cs_a.attrib like \"%default_version%\"\n          );\n    \"\"\"\n    # perhaps, check sr_c.coord_system_id != sr_a.coord_system_id\n    self.run_sql_req(sql_not_toplevel_delete, \".\".join([log_pfx, \"not_toplevel_delete\"]))\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.run","title":"<code>run()</code>","text":"<p>Entry point for the Ehive module. All processing is done here in this case.</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def run(self):\n    \"\"\"\n    Entry point for the Ehive module. All processing is done here in this case.\n    \"\"\"\n    # params\n    work_dir = self.param_required(\"work_dir\")\n\n    # initial sequence loading, using ensembl-analysis scripts\n    self.initial_sequence_loading(work_dir)\n\n    # load data from the corresponding core db tables\n    external_db_map = self.load_map_from_core_db(\n        \"external_db\", [\"db_name\", \"external_db_id\"], work_dir\n    )  # for external_db\n    attrib_type_map = self.load_map_from_core_db(\n        \"attrib_type\", [\"code\", \"attrib_type_id\"], work_dir\n    )  # for attrib_type\n    seq_region_map = self.load_map_from_core_db(\n        \"seq_region\", [\"name\", \"seq_region_id\"], work_dir\n    )  # for seq_region\n\n    # update synonyms and seq_region_attribs\n    unversion = self.param(\"unversion_scaffolds\")\n    is_primary_assembly = self.from_param(\"manifest_data\", \"agp\", not_throw=True) is None\n    seq_region_file = self.from_param(\"manifest_data\", \"seq_region\", not_throw=True)\n\n    #   add seq_region synonyms\n    self.add_sr_synonyms(\n        seq_region_file,\n        seq_region_map,\n        external_db_map,\n        self.pjc(work_dir, \"seq_region_syns\"),\n        unversion=unversion,\n    )\n\n    #   add seq_region attributes\n    self.add_sr_attribs(\n        seq_region_file,\n        seq_region_map,\n        attrib_type_map,\n        self.pjc(work_dir, \"seq_region_attr\"),\n        unversion=unversion,\n    )\n\n    #   add seq_region EBI and BRC4 name attributes in the \"BRC4 mode\"\n    #     special case of attributes adding with default values derived from seq_region names\n    #     do not add if preparing to swap RefSeq and GeneBank ids; in this case attributes to be added at a later stage in pipeline\n    #     (easier to insert then to update)\n    if self.param(\"brc4_mode\") and not self.param(\"swap_gcf_gca\"):\n        self.add_sr_ebi_brc4_names(\n            seq_region_file,\n            seq_region_map,\n            attrib_type_map,\n            self.pjc(work_dir, \"seq_region_ebi_brc4_name\"),\n            unversion=unversion,\n        )\n\n    # add karyotype related data\n    self.add_karyotype_data(\n        seq_region_file,\n        seq_region_map,\n        attrib_type_map,\n        self.pjc(work_dir, \"karyotype\"),\n        unversion=unversion,\n    )\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.set_toplevel","title":"<code>set_toplevel(log_pfx, ignored_cs=[])</code>","text":"<p>Set toplevel(6) seq_region_attrib using ensembl script.</p> <p>Uses set_toplevel.pl ensembl script.</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def set_toplevel(self, log_pfx, ignored_cs=[]):\n    \"\"\"\n    Set toplevel(6) seq_region_attrib using ensembl script.\n\n    Uses set_toplevel.pl ensembl script.\n    \"\"\"\n    # set top_level(6) seq_region_attrib\n    os.makedirs(dirname(log_pfx), exist_ok=True)\n    en_root = self.param_required(\"ensembl_root_dir\")\n    cmd = (\n        r\"\"\"{_set_tl} {_db_string} {_ignored_cs} \"\"\" + r\"\"\"     &gt; {_log}.stdout 2&gt; {_log}.stderr\"\"\"\n    ).format(\n        _set_tl=\"perl %s\"\n        % (self.pjc(en_root, r\"ensembl-analysis/scripts/assembly_loading/set_toplevel.pl\")),\n        _db_string=self.db_string(),\n        _ignored_cs=\" \".join(map(lambda x: \"-ignore_coord_system %s\" % (x), ignored_cs)),\n        _log=log_pfx,\n    )\n    print(\"running %s\" % (cmd), file=sys.stderr)\n    sp.run(cmd, shell=True, check=True)\n\n    # remove toplevel attribute for seq_regions that are components\n    self.remove_components_from_toplevel(log_pfx)\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.sr_name_unversion","title":"<code>sr_name_unversion(cs, tbl, fld, log_pfx)</code>","text":"<p>Remove version suffix from the seq_region names</p> <p>Removes '.\\d+$' suffices from the seq_region names SQL code.</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def sr_name_unversion(self, cs, tbl, fld, log_pfx):\n    \"\"\"\n    Remove version suffix from the seq_region names\n\n    Removes '\\.\\d+$' suffices from the seq_region names\n    SQL code.\n    \"\"\"\n    # select synonym, substr(synonym,  1, locate(\".\", synonym, length(synonym)-2)-1)\n    #     from seq_region_synonym  where synonym like \"%._\"\n    asm_v = self.asm_name()\n    sql = r\"\"\"update {_tbl} t, seq_region sr, coord_system cs\n                set\n                  t.{_fld} = substr(t.{_fld},  1, locate(\".\", t.{_fld}, length(t.{_fld})-2)-1)\n                where t.{_fld} like \"%._\"\n                  and t.seq_region_id = sr.seq_region_id\n                  and sr.coord_system_id = cs.coord_system_id\n                  and cs.name = \"{_cs}\"\n                  and cs.version = \"{_asm_v}\"\n            ;\"\"\".format(\n        _tbl=tbl, _fld=fld, _cs=cs, _asm_v=asm_v\n    )\n    return self.run_sql_req(sql, log_pfx)\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.unversion_scaffolds","title":"<code>unversion_scaffolds(cs_rank, logs)</code>","text":"<p>Unversion scaffold, remove \".\\d$\" from seq_region.names if there's a need</p> <p>Non-versioned syns for contigs (lower, sequence level), versioned for the rest.</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def unversion_scaffolds(self, cs_rank, logs):\n    \"\"\"\n    Unversion scaffold, remove \".\\d$\" from seq_region.names if there's a need\n\n    Non-versioned syns for contigs (lower, sequence level), versioned for the rest.\n    \"\"\"\n    seq_cs, max_rank = max([(c, r) for c, r in cs_rank.items()], key=lambda k: k[1])\n    for cs in cs_rank:\n        if cs == seq_cs:\n            # for non-sequence level cs, store original name (with version) as \"sr_syn_src\" synonym\n            xdb = self.param(\"sr_syn_src\")\n            self.copy_sr_name_to_syn(cs, xdb, self.pjc(logs, \"cp2syn\", cs))\n            self.sr_name_unversion(cs, \"seq_region_synonym\", \"synonym\", self.pjc(logs, \"unv_srs\", cs))\n        else:\n            # for sequence-level cs, store original name (with version) as \"versioned_sr_syn_src\" synonym\n            xdb = self.param(\"versioned_sr_syn_src\")\n            self.copy_sr_name_to_syn(cs, xdb, self.pjc(logs, \"cp2syn\", cs))\n            self.sr_name_unversion(cs, \"seq_region\", \"name\", self.pjc(logs, \"unv_sr\", cs))\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/load_sequence_data/#src.ensembl.brc4.runnable.load_sequence_data.load_sequence_data.update_db_single_group","title":"<code>update_db_single_group(dict_of_col_to_value, table_name, work_dir, where=None)</code>","text":"<p>Update given <code>table</code> name in db; set <code>col = val</code> for all key/value pairs from <code>dict_of_cols_to_values</code></p> <p>If <code>where</code> condition is present its value is used for the \"WHERE\" SQL clause. Use <code>quote_or_null</code> (see definition below) method for string values, when putting values into <code>list_of_tuples</code></p> <p>SQL code</p> Source code in <code>src/ensembl/brc4/runnable/load_sequence_data.py</code> <pre><code>def update_db_single_group(\n    self, dict_of_col_to_value: dict, table_name: str, work_dir: str, where: str = None\n):\n    \"\"\"\n    Update given `table` name in db; set `col = val` for all key/value pairs from `dict_of_cols_to_values`\n\n    If `where` condition is present its value is used for the \"WHERE\" SQL clause.\n    Use `quote_or_null` (see definition below) method for string values, when putting values into `list_of_tuples`\n\n    SQL code\n    \"\"\"\n    # return if nothing to do\n    if not dict_of_col_to_value:\n        return\n\n    # prepare request parts\n    where_str = where and f\"WHERE {where}\" or \"\"\n    col_val_str = \", \".join([f\"{col} = {val}\" for col, val in dict_of_col_to_value.items()])\n\n    # generate file with the insert SQL command\n    update_sql_file = self.pjc(work_dir, \"update.sql\")\n    with open(insert_sql_file, \"w\") as sql:\n        print(f\"UPDATE {table_name} SET {col_val_str} {where_str};\", file=sql)\n        values_sep = \"\"\n        for tpl in list_of_tuples:\n            tpl_str = \", \".join(map(str, tpl))\n            print(f\"{values_sep}({tpl_str})\", file=sql)\n            values_sep = \", \"\n        print(\";\", file=sql)\n\n    # run insert SQL from file\n    self.run_sql_req(update_sql_file, self.pjc(work_dir, \"update\"), from_file=True)\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/manifest/","title":"manifest","text":""},{"location":"reference/ensembl/brc4/runnable/manifest_stats/","title":"manifest_stats","text":""},{"location":"reference/ensembl/brc4/runnable/manifest_stats/#src.ensembl.brc4.runnable.manifest_stats.manifest_stats","title":"<code>manifest_stats</code>","text":"<p>             Bases: <code>BaseRunnable</code></p> Source code in <code>src/ensembl/brc4/runnable/manifest_stats.py</code> <pre><code>class manifest_stats(eHive.BaseRunnable):\n    def param_defaults(self):\n        return {\n            \"datasets_bin\": \"datasets\",\n        }\n\n    def run(self):\n        manifest_path = Path(self.param_required(\"manifest\"))\n        manifest = self.get_manifest(manifest_path)\n\n        stats = [self.param(\"accession\")]\n\n        self.param(\"error\", False)\n        if \"gff3\" in manifest:\n            stats += self.get_gff3_stats(Path(manifest[\"gff3\"]))\n        if \"seq_region\" in manifest:\n            stats += self.get_seq_region_stats(Path(manifest[\"seq_region\"]))\n\n        stats_path = manifest_path.parent / \"stats.txt\"\n        print(stats_path)\n        with stats_path.open(\"w\") as stats_out:\n            stats_out.write(\"\\n\".join(stats))\n\n        # Flow out if errors in stats comparison\n        if self.param(\"error\"):\n            raise Exception(f\"Stats count errors, check the file {stats_path}\")\n\n    def get_manifest(self, manifest_path: Path) -&gt; Dict:\n        manifest = get_json(manifest_path)\n        manifest_root = manifest_path.parent\n\n        # Use dir name from the manifest\n        for name in manifest:\n            if \"file\" in manifest[name]:\n                file_name = manifest[name][\"file\"]\n                file_name = manifest_root / file_name\n                manifest[name] = file_name\n            else:\n                for f in manifest[name]:\n                    if \"file\" in manifest[name][f]:\n                        file_name = manifest[name][f][\"file\"]\n                        file_name = manifest_root, file_name\n                        manifest[name][f] = file_name\n\n        return manifest\n\n    def get_seq_region_stats(self, seq_region_path: Path) -&gt; List:\n        seq_regions = get_json(seq_region_path)\n\n        # Get basic data\n        coord_systems: Dict[str, List[int]] = {}\n        circular = 0\n        locations = []\n        codon_tables = []\n        for seqr in seq_regions:\n            # Get readable seq_region name\n            genbank = \"synonyms\" in seqr and [x for x in seqr[\"synonyms\"] if x[\"source\"] == \"GenBank\"]\n            seqr_name = genbank and genbank[0][\"name\"] or seqr[\"name\"]\n\n            coord_level = seqr[\"coord_system_level\"]\n            if not coord_level in coord_systems:\n                coord_systems[coord_level] = []\n            coord_systems[coord_level].append(int(seqr[\"length\"]))\n\n            if \"circular\" in seqr:\n                circular += 1\n            if \"codon_table\" in seqr:\n                codon_tables.append(\"%s = %s\" % (seqr_name, seqr[\"codon_table\"]))\n            if \"location\" in seqr:\n                locations.append(\"%s = %s\" % (seqr_name, seqr[\"location\"]))\n\n        # Stats\n        stats = []\n        stats.append(seq_region_path.name)\n        stats.append(\"Total coord_systems %d\" % len(coord_systems))\n        for coord_name, lengths in coord_systems.items():\n            stats.append(\"\\nCoord_system: %s\" % coord_name)\n\n            stat_counts: Dict[str, float] = {}\n            stat_counts[\"Number of sequences\"] = len(lengths)\n            stat_counts[\"Sequence length sum\"] = sum(lengths)\n            stat_counts[\"Sequence length minimum\"] = min(lengths)\n            stat_counts[\"Sequence length mean\"] = mean(lengths)\n            stat_counts[\"Sequence length maximum\"] = max(lengths)\n\n            for name, count in stat_counts.items():\n                stats.append(\"%9d\\t%s\" % (count, name))\n\n        # Special\n        if circular or locations:\n            stats.append(\"\\nSpecial\")\n            if circular:\n                stats.append(\"%9d\\t%s\" % (circular, \"circular sequences\"))\n            if locations:\n                stats.append(\"%9d\\t%s\" % (len(locations), \"sequences with location\"))\n                for loc in locations:\n                    stats.append(\"\\t\\t\\t%s\" % loc)\n            if codon_tables:\n                stats.append(\"%9d\\t%s\" % (len(codon_tables), \"sequences with codon_table\"))\n                for table in codon_tables:\n                    stats.append(\"\\t\\t\\t%s\" % table)\n\n        stats.append(\"\\n\")\n\n        return stats\n\n    def get_gff3_stats(self, gff3_path: Path) -&gt; List:\n        stats = []\n        stats.append(gff3_path.name)\n        if gff3_path.name.endswith(\".gz\"):\n            with gzip.open(gff3_path, \"rt\") as gff3_handle:\n                stats += self.parse_gff3(gff3_handle)\n        else:\n            with gff3_path.open(\"r\") as gff3_handle:\n                stats += self.parse_gff3(gff3_handle)\n        stats.append(\"\\n\")\n\n        return stats\n\n    def parse_gff3(self, gff3_handle: TextIO) -&gt; List:\n        biotypes = BiotypeCollection()\n\n        for rec in GFF.parse(gff3_handle):\n            for feat1 in rec.features:\n                # Check if the gene contains proteins (CDSs),\n                # and keep a count of all hierarchies (e.g. gene-mRNA-CDS)\n                is_protein = False\n                for feat2 in feat1.sub_features:\n                    if feat2.type == \"mRNA\":\n                        types2 = {f.type for f in feat2.sub_features}\n                        if \"CDS\" in types2:\n                            is_protein = True\n                    biotypes.add_feature(feat2.id, f\"{feat1.type}-{feat2.type}\")\n                    for feat3 in feat2.sub_features:\n                        if feat3.type == \"exon\":\n                            continue\n                        biotypes.add_feature(feat1.id, f\"{feat2.type}-{feat3.type}\")\n\n                # Main categories counts\n                if feat1.type == \"pseudogene\":\n                    biotypes.add_feature(feat1.id, \"pseudogene\")\n                elif is_protein:\n                    biotypes.add_feature(feat1.id, f\"PROT_{feat1.type}\")\n                else:\n                    # Special case, undefined gene-transcript\n                    if (\n                        feat1.type == \"gene\"\n                        and feat1.sub_features\n                        and feat1.sub_features[0].type == \"transcript\"\n                    ):\n                        biotypes.add_feature(feat1.id, \"OTHER\")\n                    else:\n                        biotypes.add_feature(feat1.id, f\"NONPROT_{feat1.type}\")\n\n                # Total\n                if feat1.type in (\"gene\", \"pseudogene\"):\n                    biotypes.add_feature(feat1.id, f\"ALL_GENES\")\n        stats = biotypes.sorted_stats()\n\n        # Check against NCBI stats\n        stats += self.check_ncbi_stats(biotypes, self.param(\"accession\"))\n\n        return stats\n\n    def check_ncbi_stats(self, biotypes: BiotypeCollection, accession: str) -&gt; List:\n        \"\"\"Use the dataset tool from NCBI to get stats and compare with what we have\"\"\"\n\n        stats: List[str] = []\n\n        datasets_bin = self.param(\"datasets_bin\")\n        if not which(datasets_bin):\n            return stats\n\n        # Get the dataset summary from NCBI\n        command = [datasets_bin, \"summary\", \"genome\", \"accession\", accession]\n        result_out = subprocess.run(command, stdout=subprocess.PIPE)\n        result = json.loads(result_out.stdout)\n\n        # Get stats\n        if \"reports\" in result:\n            genome = result[\"reports\"][0]\n            if \"annotation_info\" in genome and \"stats\" in genome[\"annotation_info\"]:\n                ncbi_stats = genome[\"annotation_info\"][\"stats\"]\n\n                if \"gene_counts\" in ncbi_stats:\n                    counts = ncbi_stats[\"gene_counts\"]\n                    stats = self.compare_ncbi_counts(biotypes, counts)\n        return stats\n\n    def compare_ncbi_counts(self, prepared: BiotypeCollection, ncbi: Dict) -&gt; List:\n        \"\"\"Compare specific gene stats from NCBI\"\"\"\n        stats = []\n\n        maps = [\n            [\"total\", \"ALL_GENES\"],\n            [\"protein_coding\", \"PROT_gene\"],\n            [\"pseudogene\", \"pseudogene\"],\n            [\"non_coding\", \"NONPROT_gene\"],\n            [\"other\", \"OTHER\"],\n        ]\n\n        for map in maps:\n            ncbi_name, prep_name = map\n            ncbi_count = ncbi.get(ncbi_name, 0)\n            prep_count = prepared.get_count(prep_name)\n\n            if prep_count != ncbi_count:\n                diff = prep_count - ncbi_count\n                stats.append(f\"DIFF gene count for {map}: {prep_count} - {ncbi_count} = {diff}\")\n                self.param(\"error\", True)\n            else:\n                stats.append(f\"Same count for {map}: {prep_count}\")\n\n        return stats\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/manifest_stats/#src.ensembl.brc4.runnable.manifest_stats.manifest_stats.check_ncbi_stats","title":"<code>check_ncbi_stats(biotypes, accession)</code>","text":"<p>Use the dataset tool from NCBI to get stats and compare with what we have</p> Source code in <code>src/ensembl/brc4/runnable/manifest_stats.py</code> <pre><code>def check_ncbi_stats(self, biotypes: BiotypeCollection, accession: str) -&gt; List:\n    \"\"\"Use the dataset tool from NCBI to get stats and compare with what we have\"\"\"\n\n    stats: List[str] = []\n\n    datasets_bin = self.param(\"datasets_bin\")\n    if not which(datasets_bin):\n        return stats\n\n    # Get the dataset summary from NCBI\n    command = [datasets_bin, \"summary\", \"genome\", \"accession\", accession]\n    result_out = subprocess.run(command, stdout=subprocess.PIPE)\n    result = json.loads(result_out.stdout)\n\n    # Get stats\n    if \"reports\" in result:\n        genome = result[\"reports\"][0]\n        if \"annotation_info\" in genome and \"stats\" in genome[\"annotation_info\"]:\n            ncbi_stats = genome[\"annotation_info\"][\"stats\"]\n\n            if \"gene_counts\" in ncbi_stats:\n                counts = ncbi_stats[\"gene_counts\"]\n                stats = self.compare_ncbi_counts(biotypes, counts)\n    return stats\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/manifest_stats/#src.ensembl.brc4.runnable.manifest_stats.manifest_stats.compare_ncbi_counts","title":"<code>compare_ncbi_counts(prepared, ncbi)</code>","text":"<p>Compare specific gene stats from NCBI</p> Source code in <code>src/ensembl/brc4/runnable/manifest_stats.py</code> <pre><code>def compare_ncbi_counts(self, prepared: BiotypeCollection, ncbi: Dict) -&gt; List:\n    \"\"\"Compare specific gene stats from NCBI\"\"\"\n    stats = []\n\n    maps = [\n        [\"total\", \"ALL_GENES\"],\n        [\"protein_coding\", \"PROT_gene\"],\n        [\"pseudogene\", \"pseudogene\"],\n        [\"non_coding\", \"NONPROT_gene\"],\n        [\"other\", \"OTHER\"],\n    ]\n\n    for map in maps:\n        ncbi_name, prep_name = map\n        ncbi_count = ncbi.get(ncbi_name, 0)\n        prep_count = prepared.get_count(prep_name)\n\n        if prep_count != ncbi_count:\n            diff = prep_count - ncbi_count\n            stats.append(f\"DIFF gene count for {map}: {prep_count} - {ncbi_count} = {diff}\")\n            self.param(\"error\", True)\n        else:\n            stats.append(f\"Same count for {map}: {prep_count}\")\n\n    return stats\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/prepare_genome/","title":"prepare_genome","text":""},{"location":"reference/ensembl/brc4/runnable/read_json/","title":"read_json","text":""},{"location":"reference/ensembl/brc4/runnable/read_json/#src.ensembl.brc4.runnable.read_json.read_json","title":"<code>read_json</code>","text":"<p>             Bases: <code>BaseRunnable</code></p> <p>Read a json data from a file, and flow it out for the pipeline to use.</p> <p>Parameters:</p> Name Type Description Default <code>json_path</code> <p>json to load</p> required <code>name</code> <p>key of the dataflow object associated with the json data</p> required Dataflows <p>2: one Dict record with the name as key, and the json data as value</p> Source code in <code>src/ensembl/brc4/runnable/read_json.py</code> <pre><code>class read_json(eHive.BaseRunnable):\n    \"\"\"Read a json data from a file, and flow it out for the pipeline to use.\n\n    Args:\n        json_path: json to load\n        name: key of the dataflow object associated with the json data\n\n    Dataflows:\n        2: one Dict record with the name as key, and the json data as value\n    \"\"\"\n\n    def run(self):\n        json_path = self.param_required(\"json_path\")\n        name = self.param_required(\"name\")\n\n        with open(json_path) as json_file:\n            data = json.load(json_file)\n            self.dataflow({name: data}, 2)\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/say_accession/","title":"say_accession","text":""},{"location":"reference/ensembl/brc4/runnable/say_accession/#src.ensembl.brc4.runnable.say_accession.say_accession","title":"<code>say_accession</code>","text":"<p>             Bases: <code>BaseRunnable</code></p> <p>Simple runnable to bring out the accession value for the pipeline to use.</p> <p>Parameters:</p> Name Type Description Default <code>genome_data</code> <p>a dict from genome_data following the schema from schemas/genome_schema.json</p> required Dataflows <p>2: a single value named accession</p> Source code in <code>src/ensembl/brc4/runnable/say_accession.py</code> <pre><code>class say_accession(eHive.BaseRunnable):\n    \"\"\"Simple runnable to bring out the accession value for the pipeline to use.\n\n    Args:\n        genome_data: a dict from genome_data following the schema from schemas/genome_schema.json\n\n    Dataflows:\n        2: a single value named accession\n    \"\"\"\n\n    def run(self):\n        genome_data = self.param_required(\"genome_data\")\n        accession = genome_data[\"assembly\"][\"accession\"]\n        self.dataflow({\"accession\": accession}, 2)\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/seqregion_parser/","title":"seqregion_parser","text":""},{"location":"reference/ensembl/brc4/runnable/seqregion_parser/#src.ensembl.brc4.runnable.seqregion_parser.SeqregionParser","title":"<code>SeqregionParser</code>","text":"<p>Parser of a seq_region report from INSDC/RefSeq.</p> <p>The main method of the Parser is get_report_regions, which returns a Dict of seq_regions, where the keys are the names.</p> Source code in <code>src/ensembl/brc4/runnable/seqregion_parser.py</code> <pre><code>class SeqregionParser:\n    \"\"\"Parser of a seq_region report from INSDC/RefSeq.\n\n    The main method of the Parser is get_report_regions, which returns a Dict of seq_regions,\n    where the keys are the names.\n    \"\"\"\n\n    synonym_map = {\n        \"Sequence-Name\": \"INSDC_submitted_name\",\n        \"GenBank-Accn\": \"INSDC\",\n        \"RefSeq-Accn\": \"RefSeq\",\n        \"Assigned-Molecule\": \"GenBank\",\n    }\n\n    molecule_location = {\n        \"chromosome\": \"nuclear_chromosome\",\n        \"mitochondrion\": \"mitochondrial_chromosome\",\n        \"apicoplast\": \"apicoplast_chromosome\",\n        \"plasmid\": \"plasmid\",\n        \"linkage group\": \"linkage_group\",\n        \"kinetoplast\": \"kinetoplast\",\n    }\n\n    def get_report_regions(\n        self, report_path: Path, accession: str, use_refseq: bool = False\n    ) -&gt; Dict[str, dict]:\n        \"\"\"Get seq_region data from report file.\n\n        Args:\n            report_path: Path to the INSDC seq_region report.\n            use_refseq: Expect a RefSeq seq_region report.\n        Returns:\n            A dict of seq_regions dicts, with their name as the key\n        \"\"\"\n        if accession.startswith(\"GCF\"):\n            use_refseq = True\n        # Get the report in a CSV format, easier to manipulate\n        report_csv, metadata = self.report_to_csv(report_path)\n\n        # Feed the csv string to the CSV reader\n        reader = csv.DictReader(report_csv.splitlines(), delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n\n        # Metadata\n        assembly_level = \"contig\"\n        if \"Assembly level\" in metadata:\n            assembly_level = metadata[\"Assembly level\"].lower()\n\n        # Create the seq_regions\n        seq_regions = {}\n        for row in reader:\n            seq_region = self.make_seq_region(row, assembly_level, use_refseq)\n            name = seq_region[\"name\"]\n            seq_regions[name] = seq_region\n\n        return seq_regions\n\n    def report_to_csv(self, report_path: Path) -&gt; Tuple[str, dict]:\n        \"\"\"Load an assembly report as a csv string.\n\n        Args:\n            report_path: Path to the INSDC seq_region report.\n        Returns:\n            The csv as a string, and the head metadata as a dict.\n        \"\"\"\n\n        with open_gz_file(report_path) as report:\n            data = \"\"\n            metadata = {}\n            last_head = \"\"\n            for line in report:\n                # Ignore header\n                if line.startswith(\"#\"):\n                    # Get metadata values if possible\n                    match = re.search(\"# (.+?): (.+?)$\", line)\n                    if match:\n                        metadata[match.group(1)] = match.group(2)\n                    last_head = line\n                    continue\n                else:\n                    if last_head:\n                        data += last_head[2:].strip() + \"\\n\"\n                        last_head = \"\"\n                    data += line\n            return data, metadata\n\n    def make_seq_region(self, row: Dict[str, str], assembly_level: str, use_refseq: bool) -&gt; Dict[str, Any]:\n        \"\"\"From a row of the report, create one seq_region dict.\n\n        Args:\n            row: A seq_region row from the INSDC report.\n            assembly_level: what level is the seq_region (chromosome, contig, etc.)\n            use_refseq: Expect a RefSeq seq_region report.\n\n        Returns:\n            A seq_region dict.\n        \"\"\"\n        seq_region: Dict[str, Any] = {}\n\n        # Map the fields to their synonym name\n        synonym_map = self.synonym_map\n        molecule_location = self.molecule_location\n\n        # Synonyms\n        synonyms = []\n        for field, source in synonym_map.items():\n            if field in row and row[field].lower() != \"na\":\n                synonym = {\"source\": source, \"name\": row[field]}\n                synonyms.append(synonym)\n        if len(synonyms) &gt; 0:\n            synonyms.sort(key=lambda x: x[\"source\"])\n            seq_region[\"synonyms\"] = synonyms\n\n        # Length\n        field = \"Sequence-Length\"\n        if field in row and row[field].lower() != \"na\":\n            seq_region[\"length\"] = int(row[field])\n\n        if use_refseq:\n            if \"RefSeq-Accn\" in row:\n                seq_region[\"name\"] = row[\"RefSeq-Accn\"]\n            else:\n                raise Exception(f\"No RefSeq name for {row['Sequence-Name']}\")\n\n        else:\n            if \"GenBank-Accn\" in row:\n                seq_region[\"name\"] = row[\"GenBank-Accn\"]\n            else:\n                raise Exception(f\"No INSDC name for {row['Sequence-Name']}\")\n\n        # Coord system and location\n        seq_role = row[\"Sequence-Role\"]\n\n        # Scaffold?\n        if seq_role in (\"unplaced-scaffold\", \"unlocalized-scaffold\", \"alt-scaffold\"):\n            seq_region[\"coord_system_level\"] = \"scaffold\"\n\n        # Chromosome? Check location\n        elif seq_role == \"assembled-molecule\":\n            seq_region[\"coord_system_level\"] = \"chromosome\"\n            location = row[\"Assigned-Molecule-Location/Type\"].lower()\n\n            # Get location metadata\n            if location in molecule_location:\n                seq_location = molecule_location[location]\n                seq_region[\"location\"] = seq_location\n            else:\n                raise Exception(f\"Unrecognized sequence location: {location}\")\n        else:\n            raise Exception(f\"Unrecognized sequence role: {seq_role}\")\n        return seq_region\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/seqregion_parser/#src.ensembl.brc4.runnable.seqregion_parser.SeqregionParser.get_report_regions","title":"<code>get_report_regions(report_path, accession, use_refseq=False)</code>","text":"<p>Get seq_region data from report file.</p> <p>Parameters:</p> Name Type Description Default <code>report_path</code> <code>Path</code> <p>Path to the INSDC seq_region report.</p> required <code>use_refseq</code> <code>bool</code> <p>Expect a RefSeq seq_region report.</p> <code>False</code> <p>Returns:     A dict of seq_regions dicts, with their name as the key</p> Source code in <code>src/ensembl/brc4/runnable/seqregion_parser.py</code> <pre><code>def get_report_regions(\n    self, report_path: Path, accession: str, use_refseq: bool = False\n) -&gt; Dict[str, dict]:\n    \"\"\"Get seq_region data from report file.\n\n    Args:\n        report_path: Path to the INSDC seq_region report.\n        use_refseq: Expect a RefSeq seq_region report.\n    Returns:\n        A dict of seq_regions dicts, with their name as the key\n    \"\"\"\n    if accession.startswith(\"GCF\"):\n        use_refseq = True\n    # Get the report in a CSV format, easier to manipulate\n    report_csv, metadata = self.report_to_csv(report_path)\n\n    # Feed the csv string to the CSV reader\n    reader = csv.DictReader(report_csv.splitlines(), delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n\n    # Metadata\n    assembly_level = \"contig\"\n    if \"Assembly level\" in metadata:\n        assembly_level = metadata[\"Assembly level\"].lower()\n\n    # Create the seq_regions\n    seq_regions = {}\n    for row in reader:\n        seq_region = self.make_seq_region(row, assembly_level, use_refseq)\n        name = seq_region[\"name\"]\n        seq_regions[name] = seq_region\n\n    return seq_regions\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/seqregion_parser/#src.ensembl.brc4.runnable.seqregion_parser.SeqregionParser.make_seq_region","title":"<code>make_seq_region(row, assembly_level, use_refseq)</code>","text":"<p>From a row of the report, create one seq_region dict.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Dict[str, str]</code> <p>A seq_region row from the INSDC report.</p> required <code>assembly_level</code> <code>str</code> <p>what level is the seq_region (chromosome, contig, etc.)</p> required <code>use_refseq</code> <code>bool</code> <p>Expect a RefSeq seq_region report.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A seq_region dict.</p> Source code in <code>src/ensembl/brc4/runnable/seqregion_parser.py</code> <pre><code>def make_seq_region(self, row: Dict[str, str], assembly_level: str, use_refseq: bool) -&gt; Dict[str, Any]:\n    \"\"\"From a row of the report, create one seq_region dict.\n\n    Args:\n        row: A seq_region row from the INSDC report.\n        assembly_level: what level is the seq_region (chromosome, contig, etc.)\n        use_refseq: Expect a RefSeq seq_region report.\n\n    Returns:\n        A seq_region dict.\n    \"\"\"\n    seq_region: Dict[str, Any] = {}\n\n    # Map the fields to their synonym name\n    synonym_map = self.synonym_map\n    molecule_location = self.molecule_location\n\n    # Synonyms\n    synonyms = []\n    for field, source in synonym_map.items():\n        if field in row and row[field].lower() != \"na\":\n            synonym = {\"source\": source, \"name\": row[field]}\n            synonyms.append(synonym)\n    if len(synonyms) &gt; 0:\n        synonyms.sort(key=lambda x: x[\"source\"])\n        seq_region[\"synonyms\"] = synonyms\n\n    # Length\n    field = \"Sequence-Length\"\n    if field in row and row[field].lower() != \"na\":\n        seq_region[\"length\"] = int(row[field])\n\n    if use_refseq:\n        if \"RefSeq-Accn\" in row:\n            seq_region[\"name\"] = row[\"RefSeq-Accn\"]\n        else:\n            raise Exception(f\"No RefSeq name for {row['Sequence-Name']}\")\n\n    else:\n        if \"GenBank-Accn\" in row:\n            seq_region[\"name\"] = row[\"GenBank-Accn\"]\n        else:\n            raise Exception(f\"No INSDC name for {row['Sequence-Name']}\")\n\n    # Coord system and location\n    seq_role = row[\"Sequence-Role\"]\n\n    # Scaffold?\n    if seq_role in (\"unplaced-scaffold\", \"unlocalized-scaffold\", \"alt-scaffold\"):\n        seq_region[\"coord_system_level\"] = \"scaffold\"\n\n    # Chromosome? Check location\n    elif seq_role == \"assembled-molecule\":\n        seq_region[\"coord_system_level\"] = \"chromosome\"\n        location = row[\"Assigned-Molecule-Location/Type\"].lower()\n\n        # Get location metadata\n        if location in molecule_location:\n            seq_location = molecule_location[location]\n            seq_region[\"location\"] = seq_location\n        else:\n            raise Exception(f\"Unrecognized sequence location: {location}\")\n    else:\n        raise Exception(f\"Unrecognized sequence role: {seq_role}\")\n    return seq_region\n</code></pre>"},{"location":"reference/ensembl/brc4/runnable/seqregion_parser/#src.ensembl.brc4.runnable.seqregion_parser.SeqregionParser.report_to_csv","title":"<code>report_to_csv(report_path)</code>","text":"<p>Load an assembly report as a csv string.</p> <p>Parameters:</p> Name Type Description Default <code>report_path</code> <code>Path</code> <p>Path to the INSDC seq_region report.</p> required <p>Returns:     The csv as a string, and the head metadata as a dict.</p> Source code in <code>src/ensembl/brc4/runnable/seqregion_parser.py</code> <pre><code>def report_to_csv(self, report_path: Path) -&gt; Tuple[str, dict]:\n    \"\"\"Load an assembly report as a csv string.\n\n    Args:\n        report_path: Path to the INSDC seq_region report.\n    Returns:\n        The csv as a string, and the head metadata as a dict.\n    \"\"\"\n\n    with open_gz_file(report_path) as report:\n        data = \"\"\n        metadata = {}\n        last_head = \"\"\n        for line in report:\n            # Ignore header\n            if line.startswith(\"#\"):\n                # Get metadata values if possible\n                match = re.search(\"# (.+?): (.+?)$\", line)\n                if match:\n                    metadata[match.group(1)] = match.group(2)\n                last_head = line\n                continue\n            else:\n                if last_head:\n                    data += last_head[2:].strip() + \"\\n\"\n                    last_head = \"\"\n                data += line\n        return data, metadata\n</code></pre>"},{"location":"reference/ensembl/io/genomio/","title":"genomio","text":"<p>Genome Input/Output (GenomIO) handling library.</p>"},{"location":"reference/ensembl/io/genomio/assembly/","title":"assembly","text":"<p>Assembly preparation module.</p>"},{"location":"reference/ensembl/io/genomio/assembly/download/","title":"download","text":"<p>Download an assembly data files from INSDC or RefSeq.</p>"},{"location":"reference/ensembl/io/genomio/assembly/download/#src.ensembl.io.genomio.assembly.download.FileDownloadError","title":"<code>FileDownloadError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>When a file download fails or there is a problem with that file.</p> Source code in <code>src/ensembl/io/genomio/assembly/download.py</code> <pre><code>class FileDownloadError(Exception):\n    \"\"\"When a file download fails or there is a problem with that file.\"\"\"\n</code></pre>"},{"location":"reference/ensembl/io/genomio/assembly/download/#src.ensembl.io.genomio.assembly.download.UnsupportedFormatError","title":"<code>UnsupportedFormatError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>When a string does not have the expected format.</p> Source code in <code>src/ensembl/io/genomio/assembly/download.py</code> <pre><code>class UnsupportedFormatError(Exception):\n    \"\"\"When a string does not have the expected format.\"\"\"\n</code></pre>"},{"location":"reference/ensembl/io/genomio/assembly/download/#src.ensembl.io.genomio.assembly.download.download_files","title":"<code>download_files(accession, dl_dir, max_redo)</code>","text":"<p>Given an INSDC accession, download all available files from the ftp to the download dir</p> Source code in <code>src/ensembl/io/genomio/assembly/download.py</code> <pre><code>def download_files(accession: str, dl_dir: Path, max_redo: int) -&gt; None:\n    \"\"\"\n    Given an INSDC accession, download all available files from the ftp to the download dir\n    \"\"\"\n    match = re.match(r\"(GC[AF])_([0-9]{3})([0-9]{3})([0-9]{3})\\.?([0-9]+)\", accession)\n    if not match:\n        raise UnsupportedFormatError(f\"Could not recognize GCA accession format: {accession}\")\n    gca = match.group(1)\n    part1 = match.group(2)\n    part2 = match.group(3)\n    part3 = match.group(4)\n\n    # Get the list of assemblies for this accession\n    ftp_url = \"ftp.ncbi.nlm.nih.gov\"\n    sub_dir = Path(\"genomes\", \"all\", gca, part1, part2, part3)\n    ftp_conn = FTP()\n    ftp_conn.connect(ftp_url)\n    ftp_conn.login()\n    ftp_conn.cwd(str(sub_dir))\n\n    for ftp_dir, _ in ftp_conn.mlsd():\n        if re.match(accession, ftp_dir):\n            ftp_conn.cwd(ftp_dir)\n            # First, get the md5sum file\n            md5_file = \"md5checksums.txt\"\n            md5_path = dl_dir / md5_file\n            with md5_path.open(\"wb\") as fp:\n                ftp_conn.retrbinary(f\"RETR {md5_file}\", fp.write)\n            md5_sums = get_checksums(md5_path)\n\n            # Get all the files\n            for ftp_file, _ in ftp_conn.mlsd():\n                for end in _FILE_ENDS:\n                    if ftp_file.endswith(end) and not ftp_file.endswith(f\"_from_{end}\"):\n                        _download_file(ftp_conn, ftp_file, md5_sums, dl_dir, max_redo)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/assembly/download/#src.ensembl.io.genomio.assembly.download.get_checksums","title":"<code>get_checksums(checksum_path)</code>","text":"<p>Get a dict of checksums from a file, with file names as keys and sums as values</p> Source code in <code>src/ensembl/io/genomio/assembly/download.py</code> <pre><code>def get_checksums(checksum_path: Path) -&gt; Dict[str, str]:\n    \"\"\"\n    Get a dict of checksums from a file, with file names as keys and sums as values\n    \"\"\"\n    sums: Dict[str, str] = {}\n    if not checksum_path.is_file():\n        return sums\n    with checksum_path.open(mode=\"r\") as fh:\n        for line in fh:\n            checksum, file_path = line.strip().split(\"  \")\n            file_path = file_path[2:]\n            if not file_path.find(\"/\") &gt;= 0:\n                sums[file_path] = checksum\n    return sums\n</code></pre>"},{"location":"reference/ensembl/io/genomio/assembly/download/#src.ensembl.io.genomio.assembly.download.get_files_selection","title":"<code>get_files_selection(dl_dir)</code>","text":"<p>Among all the files downloaded, only keep a subset for which we use a controlled name. Return a dict[name] = file_path The file_path is relative to the download dir Current names are defined in _FILE_ENDS</p> Source code in <code>src/ensembl/io/genomio/assembly/download.py</code> <pre><code>def get_files_selection(dl_dir: Path) -&gt; Dict[str, str]:\n    \"\"\"\n    Among all the files downloaded, only keep a subset for which we use a controlled name.\n    Return a dict[name] = file_path\n    The file_path is relative to the download dir\n    Current names are defined in _FILE_ENDS\n    \"\"\"\n    files = {}\n    root_name = get_root_name(dl_dir)\n    if root_name == \"\":\n        raise FileDownloadError(f\"Could not determine the files root name in {dl_dir}\")\n    for dl_file in dl_dir.iterdir():\n        for end, name in _FILE_ENDS.items():\n            file_with_end = dl_file.name.endswith(end) and not dl_file.name.endswith(f\"_from_{end}\")\n            if (root_name and dl_file.name == root_name + end) or file_with_end:\n                files[name] = str(dl_file)\n    return files\n</code></pre>"},{"location":"reference/ensembl/io/genomio/assembly/download/#src.ensembl.io.genomio.assembly.download.get_root_name","title":"<code>get_root_name(dl_dir)</code>","text":"<p>Get root name for assembly files, using the report file as base</p> Source code in <code>src/ensembl/io/genomio/assembly/download.py</code> <pre><code>def get_root_name(dl_dir: Path) -&gt; str:\n    \"\"\"Get root name for assembly files, using the report file as base\"\"\"\n    root_name = \"\"\n    for dl_file in dl_dir.iterdir():\n        matches = re.search(\"^(.+_)assembly_report.txt\", dl_file.name)\n        if matches:\n            root_name = matches.group(1)\n            break\n    return root_name\n</code></pre>"},{"location":"reference/ensembl/io/genomio/assembly/download/#src.ensembl.io.genomio.assembly.download.main","title":"<code>main()</code>","text":"<p>Module's entry-point.</p> Source code in <code>src/ensembl/io/genomio/assembly/download.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Module's entry-point.\"\"\"\n    parser = ArgumentParser(description=\"Download an assembly data files from INSDC or RefSeq.\")\n    parser.add_argument(\"--accession\", required=True, help=\"Genome assembly accession\")\n    parser.add_argument_dst_path(\n        \"--download_dir\", default=Path.cwd(), help=\"Folder where the data will be downloaded\"\n    )\n    args = parser.parse_args()\n\n    retrieve_assembly_data(**vars(args))\n</code></pre>"},{"location":"reference/ensembl/io/genomio/assembly/download/#src.ensembl.io.genomio.assembly.download.md5_files","title":"<code>md5_files(dl_dir)</code>","text":"<p>Check all files checksums with the sums listed in a checksum file, if available. Return False if there is no checksum file, or a file is missing, or has a wrong checksum.</p> Source code in <code>src/ensembl/io/genomio/assembly/download.py</code> <pre><code>def md5_files(dl_dir: Path) -&gt; bool:\n    \"\"\"\n    Check all files checksums with the sums listed in a checksum file, if available.\n    Return False if there is no checksum file, or a file is missing, or has a wrong checksum.\n    \"\"\"\n    md5_file = \"md5checksums.txt\"\n    # Get checksums and compare\n    md5_path = dl_dir / md5_file\n    sums = get_checksums(md5_path)\n    if not sums:\n        return False\n    logging.info(f\" File sums from {md5_path}: {len(sums)}\")\n    for dl_file, checksum in sums.items():\n        for end in _FILE_ENDS:\n            if dl_file.endswith(end) and not dl_file.endswith(f\"_from_{end}\"):\n                file_path = dl_dir / dl_file\n                if not file_path.is_file():\n                    logging.warning(f\" No file {file_path} found\")\n                    return False\n                # Check the file checksum\n                with file_path.open(mode=\"rb\") as f:\n                    content = f.read()\n                    file_sum = hashlib.md5(content).hexdigest()\n                if file_sum != checksum:\n                    logging.warning(f\" File {file_path} checksum doesn't match\")\n                    return False\n                logging.info(f\" File checksum ok {file_path}\")\n    logging.info(\" All checksums OK\")\n    return True\n</code></pre>"},{"location":"reference/ensembl/io/genomio/assembly/download/#src.ensembl.io.genomio.assembly.download.retrieve_assembly_data","title":"<code>retrieve_assembly_data(accession, download_dir, max_increment=0, max_redo=3)</code>","text":"<p>TODO</p> <p>Parameters:</p> Name Type Description Default <code>accession</code> <code>str</code> <p>Genome Assembly accession</p> required <code>download_dir</code> <code>PathLike</code> <p>Path to directory used to store retrieved</p> required <code>max_increment</code> <code>int</code> <p>If you want to allow assembly versions</p> <code>0</code> <code>max_redo</code> <code>int</code> <p>Set max number of times to retry downloading a file</p> <code>3</code> Source code in <code>src/ensembl/io/genomio/assembly/download.py</code> <pre><code>def retrieve_assembly_data(\n    accession: str,\n    download_dir: PathLike,\n    max_increment: int = 0,\n    max_redo: int = 3,\n) -&gt; None:\n    \"\"\"TODO\n\n    Args:\n        accession: Genome Assembly accession\n        download_dir: Path to directory used to store retrieved\n        max_increment: If you want to allow assembly versions\n        max_redo: Set max number of times to retry downloading a file\n\n    \"\"\"\n    download_path = Path(download_dir)\n    download_dir = download_path / accession\n\n    # Configure logging\n    log_file = f\"{accession}_download.log\"\n    reload(logging)\n    logging.basicConfig(\n        filename=log_file, format=\"%(levelname)s:%(message)s\", filemode=\"w\", level=logging.DEBUG\n    )\n\n    # Set and create dedicated dir for download\n    if not download_dir.is_dir():\n        download_dir.mkdir(parents=True)\n\n    # Download if files don't exist or fail checksum\n    if not md5_files(download_dir):\n        logging.info(\" Download the files\")\n\n        for increment in range(0, max_increment + 1):\n            if increment &gt; 0:\n                logging.info(f\" Increment accession version once from {accession}\")\n                version = int(accession[-1])\n                version += 1\n                accession = accession[:-1] + str(version)\n                download_dir = download_path / accession\n                download_dir.mkdir(parents=True, exist_ok=True)\n            download_files(accession, download_dir, max_redo)\n\n        if not md5_files(download_dir):\n            raise FileDownloadError(\"Failed md5sum of downloaded files\")\n\n    # Select specific files and give them a name\n    files = get_files_selection(download_dir)\n\n    if len(files) == 0:\n        raise FileDownloadError(\"No file downloaded\")\n</code></pre>"},{"location":"reference/ensembl/io/genomio/database/","title":"database","text":"<p>Ensembl core database interface module.</p>"},{"location":"reference/ensembl/io/genomio/database/factory/","title":"factory","text":"<p>Generates one JSON file per metadata type inside <code>manifest</code>, including the manifest itself.</p> <p>Can be imported as a module and called as a script as well, with the same parameters and expected outcome.</p>"},{"location":"reference/ensembl/io/genomio/database/factory/#src.ensembl.io.genomio.database.factory.format_db_data","title":"<code>format_db_data(server, dbs, brc_mode=False)</code>","text":"<p>Returns metadata from a list of databases on a server.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>CoreServer</code> <p>Server where all the databases are hosted.</p> required <code>dbs</code> <code>List[str]</code> <p>List of database names.</p> required <code>brc_mode</code> <code>bool</code> <p>If true, assign <code>BRC4.organism_abbrev</code> as the species, and <code>BRC4.component</code> as the division. Otherwise, the species will be <code>species.production_name</code> and the division will be <code>species.division</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List of dictionaries with 3 keys: \"database\", \"species\" and \"division\".</p> Source code in <code>src/ensembl/io/genomio/database/factory.py</code> <pre><code>def format_db_data(server: CoreServer, dbs: List[str], brc_mode: bool = False) -&gt; List[Dict]:\n    \"\"\"Returns metadata from a list of databases on a server.\n\n    Args:\n        server: Server where all the databases are hosted.\n        dbs: List of database names.\n        brc_mode: If true, assign ``BRC4.organism_abbrev`` as the species, and ``BRC4.component`` as the\n            division. Otherwise, the species will be ``species.production_name`` and the division will be\n            ``species.division``.\n\n    Returns:\n        List of dictionaries with 3 keys: \"database\", \"species\" and \"division\".\n\n    \"\"\"\n    databases_data = []\n    for db in dbs:\n        server.set_database(db)\n        metadata = server.get_db_metadata()\n\n        species = get_metadata_value(metadata, \"species.production_name\")\n        division = get_metadata_value(metadata, \"species.division\")\n\n        if brc_mode:\n            brc_organism = get_metadata_value(metadata, \"BRC4.organism_abbrev\")\n            brc_component = get_metadata_value(metadata, \"BRC4.component\")\n            if brc_organism is not None:\n                species = brc_organism\n            if brc_component is not None:\n                division = brc_component\n\n        if not division:\n            division = \"all\"\n\n        db_data = {\n            \"database\": db,\n            \"species\": species,\n            \"division\": division,\n        }\n        databases_data.append(db_data)\n    return databases_data\n</code></pre>"},{"location":"reference/ensembl/io/genomio/database/factory/#src.ensembl.io.genomio.database.factory.get_metadata_value","title":"<code>get_metadata_value(metadata, key)</code>","text":"<p>Returns the first element in the list assigned to <code>key</code> in <code>metadata</code>.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>Dict[str, List]</code> <p>Map of metadata information to lists of values.</p> required <code>key</code> <code>str</code> <p>Metadata key to search for.</p> required Source code in <code>src/ensembl/io/genomio/database/factory.py</code> <pre><code>def get_metadata_value(metadata: Dict[str, List], key: str) -&gt; Optional[str]:\n    \"\"\"Returns the first element in the list assigned to `key` in `metadata`.\n\n    Args:\n        metadata: Map of metadata information to lists of values.\n        key: Metadata key to search for.\n\n    \"\"\"\n    if (key in metadata) and metadata[key]:\n        return metadata[key][0]\n    return None\n</code></pre>"},{"location":"reference/ensembl/io/genomio/database/factory/#src.ensembl.io.genomio.database.factory.main","title":"<code>main()</code>","text":"<p>Main script entry-point.</p> Source code in <code>src/ensembl/io/genomio/database/factory.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main script entry-point.\"\"\"\n    parser = ArgumentParser(\n        description=\"Get the metadata from a list of databases on a server (in JSON format).\"\n    )\n    parser.add_server_arguments()\n    # Add filter arguments\n    parser.add_argument(\"--prefix\", default=\"\", help=\"Prefix to filter the databases\")\n    parser.add_argument(\"--build\", default=\"\", help=\"Build to filter the databases\")\n    parser.add_argument(\"--version\", default=\"\", help=\"EnsEMBL version to filter the databases\")\n    parser.add_argument(\"--db_regex\", default=\"\", help=\"Regular expression to match database names against\")\n    # Add flags\n    parser.add_argument(\n        \"--brc_mode\",\n        action=\"store_true\",\n        help=\"Enable BRC mode, i.e. use organism_abbrev for species, component for division\",\n    )\n    args = parser.parse_args()\n\n    server = CoreServer(host=args.host, port=args.port, user=args.user, password=args.password)\n    databases = server.get_cores(\n        prefix=args.prefix, build=args.build, version=args.version, dbname_re=args.db_regex\n    )\n    databases_data = format_db_data(server, databases, args.brc_mode)\n    print(json.dumps(databases_data, sort_keys=True, indent=4))\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/","title":"events","text":"<p>Gene events handling module.</p>"},{"location":"reference/ensembl/io/genomio/events/dump/","title":"dump","text":"<p>Module to dump stable id events from an Ensembl Core database</p>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.DumpStableIDs","title":"<code>DumpStableIDs</code>","text":"<p>An processor that create events from pairs of ids and can print those events out.</p> <p>Attributes:</p> Name Type Description <code>server</code> <p>a core server set to a database, to retrieve the data from.</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>class DumpStableIDs:\n    \"\"\"An processor that create events from pairs of ids and can print those events out.\n\n    Attributes:\n        server: a core server set to a database, to retrieve the data from.\n\n    \"\"\"\n\n    def __init__(self, server: CoreServer) -&gt; None:\n        \"\"\"Create a processor for events\"\"\"\n        self.server = server\n\n    def get_history(self) -&gt; List:\n        \"\"\"Retrieve all events from a database.\n\n        Returns:\n            A list of all events.\n\n        \"\"\"\n\n        sessions = self.get_mapping_sessions()\n\n        events = []\n        for session in sessions:\n            print(f\"Mapping session {session['release']}\")\n            pairs = self.get_pairs(session[\"id\"])\n            session_events = self.make_events(pairs)\n            for event in session_events:\n                event.set_release(session[\"release\"])\n                event.set_date(session[\"date\"])\n            events += session_events\n\n        # Then analyse the pairs to make events\n        return events\n\n    def print_events(self, events: List[StableIdEvent], output_file: Path) -&gt; None:\n        \"\"\"Print events in a format for BRC.\n\n        Args:\n            events: list of events for a given genome.\n            output_file: where the events will be printed.\n\n        \"\"\"\n        if not events:\n            print(\"No events to print\")\n            return\n        with output_file.open(\"w\") as out_fh:\n            for event in events:\n                event_lines = event.brc_format_2()\n                for line in event_lines:\n                    out_fh.write(line + \"\\n\")\n\n    def get_mapping_sessions(self) -&gt; List[Dict]:\n        \"\"\"Retrieve the mapping sessions from the connected database.\n\n        Returns:\n            A list of sessions, as dicts: {'id: str, 'release': str, 'date': str}.\n\n        \"\"\"\n        query = \"\"\"SELECT mapping_session_id, new_release, created\n        FROM mapping_session\n        \"\"\"\n        cursor = self.server.get_cursor()\n        cursor.execute(query)\n\n        sessions: List[Dict[str, str]] = []\n        for db in cursor:\n            session = {\"id\": db[0], \"release\": db[1], \"date\": db[2]}\n            sessions.append(session)\n        return sessions\n\n    def get_pairs(self, session_id: int) -&gt; List[Pair]:\n        \"\"\"Retrieve all pair of ids for a given session.\n\n        Args:\n            session_id: id of a session from the connected database.\n\n        Returns:\n            A list of all pairs of ids, as dicts: {'old_id': str, 'new_id': str}.\n\n        \"\"\"\n        query = \"\"\"SELECT old_stable_id, new_stable_id\n        FROM stable_id_event\n        WHERE (old_stable_id != new_stable_id OR old_stable_id IS NULL OR new_stable_id IS NULL)\n            AND type=\"gene\"\n            AND mapping_session_id=%s\n        GROUP BY old_stable_id, new_stable_id, mapping_session_id\n        \"\"\"\n        values = (session_id,)\n        cursor = self.server.get_cursor()\n        cursor.execute(query, values)\n\n        pairs: List[Pair] = []\n        for db in cursor:\n            pair = Pair(old_id=db[0], new_id=db[1])\n            pairs.append(pair)\n        print(f\"{len(pairs)} stable id events\")\n        return pairs\n\n    def make_events(self, pairs: List[Pair]) -&gt; List:\n        \"\"\"Given a list of pairs, create events.\n\n        Args:\n            pairs: list of Pair.\n\n        Return:\n            A list of events.\n\n        \"\"\"\n\n        from_list, to_list = self.get_pairs_from_to(pairs)\n\n        # Create events with those 2 dicts\n        events: List[StableIdEvent] = []\n        for old_id, from_old_list in from_list.items():\n            if not old_id or old_id not in from_list:\n                continue\n            event = StableIdEvent(set([old_id]), set(from_old_list))\n            (event, from_list, to_list) = self.extend_event(event, from_list, to_list)\n            event.add_pairs(pairs)\n            events.append(event)\n\n        # Remaining events should only be new genes\n        for new_id, to_new_list in to_list.items():\n            if not new_id:\n                continue\n            event = StableIdEvent(set(to_new_list), set([new_id]))\n            event.add_pairs(pairs)\n            events.append(event)\n\n        stats = {}\n        for event in events:\n            name = event.get_name()\n            event.clean_pairs()\n            if name not in stats:\n                stats[name] = 1\n            else:\n                stats[name] += 1\n\n        for stat, value in stats.items():\n            print(f\"\\t{stat} = {value}\")\n\n        return events\n\n    @staticmethod\n    def get_pairs_from_to(pairs: List[Pair]) -&gt; Tuple[DictToIdsSet, DictToIdsSet]:\n        \"\"\"\n        From a list of Pairs, extract a mapping of all ids from a given old id (from_list),\n        and a mapping of all ids to a given new id (to_list).\n\n        Args:\n            pairs: list of Pairs.\n\n        Return:\n             Tuple of 2 values:\n                from_list\n                to_list\n\n        \"\"\"\n        from_list: DictToIdsSet = {}\n        to_list: DictToIdsSet = {}\n        for pair in pairs:\n            old_id = pair.old_id\n            new_id = pair.new_id\n            if old_id is None:\n                old_id = \"\"\n            if new_id is None:\n                new_id = \"\"\n\n            if old_id in from_list:\n                from_list[old_id].add(new_id)\n            else:\n                from_list[old_id] = set([new_id])\n\n            if new_id in to_list:\n                to_list[new_id].add(old_id)\n            else:\n                to_list[new_id] = set([old_id])\n\n        # Remove empty elements\n        for from_id in from_list:\n            from_list[from_id] = StableIdEvent.clean_set(from_list[from_id])\n        for to_id in to_list:\n            to_list[to_id] = StableIdEvent.clean_set(to_list[to_id])\n\n        return from_list, to_list\n\n    def extend_event(\n        self, event: StableIdEvent, from_list: DictToIdsSet, to_list: DictToIdsSet\n    ) -&gt; Tuple[StableIdEvent, DictToIdsSet, DictToIdsSet]:\n        \"\"\"Given an event, aggregate ids in the 'from' and 'to' sets, to connect the whole group.\n\n        Args:\n            event: the event to extend.\n            from_list: A dict a the from ids, and their corresponding to ids.\n            to_list: A dict of the to ids, and their corresponding from ids.\n\n        Returns:\n            A tuple of the extended event, and the from_list and to_list from which the ids that\n            have been added to the event have been removed.\n\n        \"\"\"\n\n        extended = True\n\n        while extended:\n            extended = False\n\n            # Extend the group in the to ids\n            for to_id in event.to_set:\n                if to_id in to_list:\n                    to_from_ids: IdsSet = to_list[to_id]\n                    # Add to the from list?\n                    for to_from_id in to_from_ids:\n                        if to_from_id not in event.from_set:\n                            event.add_from(to_from_id)\n                            extended = True\n\n            # Extend the group in the from ids\n            for from_id in event.from_set:\n                if from_id in from_list:\n                    from_to_ids = from_list[from_id]\n                    # Add to the to list?\n                    for from_to_id in from_to_ids:\n                        if from_to_id not in event.to_set:\n                            event.add_to(from_to_id)\n                            extended = True\n\n        # Clean up\n        from_list = {from_id: from_list[from_id] for from_id in from_list if from_id not in event.from_set}\n        to_list = {to_id: to_list[to_id] for to_id in to_list if to_id not in event.to_set}\n\n        return (event, from_list, to_list)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.DumpStableIDs.__init__","title":"<code>__init__(server)</code>","text":"<p>Create a processor for events</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>def __init__(self, server: CoreServer) -&gt; None:\n    \"\"\"Create a processor for events\"\"\"\n    self.server = server\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.DumpStableIDs.extend_event","title":"<code>extend_event(event, from_list, to_list)</code>","text":"<p>Given an event, aggregate ids in the 'from' and 'to' sets, to connect the whole group.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>StableIdEvent</code> <p>the event to extend.</p> required <code>from_list</code> <code>DictToIdsSet</code> <p>A dict a the from ids, and their corresponding to ids.</p> required <code>to_list</code> <code>DictToIdsSet</code> <p>A dict of the to ids, and their corresponding from ids.</p> required <p>Returns:</p> Type Description <code>StableIdEvent</code> <p>A tuple of the extended event, and the from_list and to_list from which the ids that</p> <code>DictToIdsSet</code> <p>have been added to the event have been removed.</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>def extend_event(\n    self, event: StableIdEvent, from_list: DictToIdsSet, to_list: DictToIdsSet\n) -&gt; Tuple[StableIdEvent, DictToIdsSet, DictToIdsSet]:\n    \"\"\"Given an event, aggregate ids in the 'from' and 'to' sets, to connect the whole group.\n\n    Args:\n        event: the event to extend.\n        from_list: A dict a the from ids, and their corresponding to ids.\n        to_list: A dict of the to ids, and their corresponding from ids.\n\n    Returns:\n        A tuple of the extended event, and the from_list and to_list from which the ids that\n        have been added to the event have been removed.\n\n    \"\"\"\n\n    extended = True\n\n    while extended:\n        extended = False\n\n        # Extend the group in the to ids\n        for to_id in event.to_set:\n            if to_id in to_list:\n                to_from_ids: IdsSet = to_list[to_id]\n                # Add to the from list?\n                for to_from_id in to_from_ids:\n                    if to_from_id not in event.from_set:\n                        event.add_from(to_from_id)\n                        extended = True\n\n        # Extend the group in the from ids\n        for from_id in event.from_set:\n            if from_id in from_list:\n                from_to_ids = from_list[from_id]\n                # Add to the to list?\n                for from_to_id in from_to_ids:\n                    if from_to_id not in event.to_set:\n                        event.add_to(from_to_id)\n                        extended = True\n\n    # Clean up\n    from_list = {from_id: from_list[from_id] for from_id in from_list if from_id not in event.from_set}\n    to_list = {to_id: to_list[to_id] for to_id in to_list if to_id not in event.to_set}\n\n    return (event, from_list, to_list)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.DumpStableIDs.get_history","title":"<code>get_history()</code>","text":"<p>Retrieve all events from a database.</p> <p>Returns:</p> Type Description <code>List</code> <p>A list of all events.</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>def get_history(self) -&gt; List:\n    \"\"\"Retrieve all events from a database.\n\n    Returns:\n        A list of all events.\n\n    \"\"\"\n\n    sessions = self.get_mapping_sessions()\n\n    events = []\n    for session in sessions:\n        print(f\"Mapping session {session['release']}\")\n        pairs = self.get_pairs(session[\"id\"])\n        session_events = self.make_events(pairs)\n        for event in session_events:\n            event.set_release(session[\"release\"])\n            event.set_date(session[\"date\"])\n        events += session_events\n\n    # Then analyse the pairs to make events\n    return events\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.DumpStableIDs.get_mapping_sessions","title":"<code>get_mapping_sessions()</code>","text":"<p>Retrieve the mapping sessions from the connected database.</p> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>A list of sessions, as dicts: {'id: str, 'release': str, 'date': str}.</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>def get_mapping_sessions(self) -&gt; List[Dict]:\n    \"\"\"Retrieve the mapping sessions from the connected database.\n\n    Returns:\n        A list of sessions, as dicts: {'id: str, 'release': str, 'date': str}.\n\n    \"\"\"\n    query = \"\"\"SELECT mapping_session_id, new_release, created\n    FROM mapping_session\n    \"\"\"\n    cursor = self.server.get_cursor()\n    cursor.execute(query)\n\n    sessions: List[Dict[str, str]] = []\n    for db in cursor:\n        session = {\"id\": db[0], \"release\": db[1], \"date\": db[2]}\n        sessions.append(session)\n    return sessions\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.DumpStableIDs.get_pairs","title":"<code>get_pairs(session_id)</code>","text":"<p>Retrieve all pair of ids for a given session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>int</code> <p>id of a session from the connected database.</p> required <p>Returns:</p> Type Description <code>List[Pair]</code> <p>A list of all pairs of ids, as dicts: {'old_id': str, 'new_id': str}.</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>def get_pairs(self, session_id: int) -&gt; List[Pair]:\n    \"\"\"Retrieve all pair of ids for a given session.\n\n    Args:\n        session_id: id of a session from the connected database.\n\n    Returns:\n        A list of all pairs of ids, as dicts: {'old_id': str, 'new_id': str}.\n\n    \"\"\"\n    query = \"\"\"SELECT old_stable_id, new_stable_id\n    FROM stable_id_event\n    WHERE (old_stable_id != new_stable_id OR old_stable_id IS NULL OR new_stable_id IS NULL)\n        AND type=\"gene\"\n        AND mapping_session_id=%s\n    GROUP BY old_stable_id, new_stable_id, mapping_session_id\n    \"\"\"\n    values = (session_id,)\n    cursor = self.server.get_cursor()\n    cursor.execute(query, values)\n\n    pairs: List[Pair] = []\n    for db in cursor:\n        pair = Pair(old_id=db[0], new_id=db[1])\n        pairs.append(pair)\n    print(f\"{len(pairs)} stable id events\")\n    return pairs\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.DumpStableIDs.get_pairs_from_to","title":"<code>get_pairs_from_to(pairs)</code>  <code>staticmethod</code>","text":"<p>From a list of Pairs, extract a mapping of all ids from a given old id (from_list), and a mapping of all ids to a given new id (to_list).</p> <p>Parameters:</p> Name Type Description Default <code>pairs</code> <code>List[Pair]</code> <p>list of Pairs.</p> required Return <p>Tuple of 2 values:    from_list    to_list</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>@staticmethod\ndef get_pairs_from_to(pairs: List[Pair]) -&gt; Tuple[DictToIdsSet, DictToIdsSet]:\n    \"\"\"\n    From a list of Pairs, extract a mapping of all ids from a given old id (from_list),\n    and a mapping of all ids to a given new id (to_list).\n\n    Args:\n        pairs: list of Pairs.\n\n    Return:\n         Tuple of 2 values:\n            from_list\n            to_list\n\n    \"\"\"\n    from_list: DictToIdsSet = {}\n    to_list: DictToIdsSet = {}\n    for pair in pairs:\n        old_id = pair.old_id\n        new_id = pair.new_id\n        if old_id is None:\n            old_id = \"\"\n        if new_id is None:\n            new_id = \"\"\n\n        if old_id in from_list:\n            from_list[old_id].add(new_id)\n        else:\n            from_list[old_id] = set([new_id])\n\n        if new_id in to_list:\n            to_list[new_id].add(old_id)\n        else:\n            to_list[new_id] = set([old_id])\n\n    # Remove empty elements\n    for from_id in from_list:\n        from_list[from_id] = StableIdEvent.clean_set(from_list[from_id])\n    for to_id in to_list:\n        to_list[to_id] = StableIdEvent.clean_set(to_list[to_id])\n\n    return from_list, to_list\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.DumpStableIDs.make_events","title":"<code>make_events(pairs)</code>","text":"<p>Given a list of pairs, create events.</p> <p>Parameters:</p> Name Type Description Default <code>pairs</code> <code>List[Pair]</code> <p>list of Pair.</p> required Return <p>A list of events.</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>def make_events(self, pairs: List[Pair]) -&gt; List:\n    \"\"\"Given a list of pairs, create events.\n\n    Args:\n        pairs: list of Pair.\n\n    Return:\n        A list of events.\n\n    \"\"\"\n\n    from_list, to_list = self.get_pairs_from_to(pairs)\n\n    # Create events with those 2 dicts\n    events: List[StableIdEvent] = []\n    for old_id, from_old_list in from_list.items():\n        if not old_id or old_id not in from_list:\n            continue\n        event = StableIdEvent(set([old_id]), set(from_old_list))\n        (event, from_list, to_list) = self.extend_event(event, from_list, to_list)\n        event.add_pairs(pairs)\n        events.append(event)\n\n    # Remaining events should only be new genes\n    for new_id, to_new_list in to_list.items():\n        if not new_id:\n            continue\n        event = StableIdEvent(set(to_new_list), set([new_id]))\n        event.add_pairs(pairs)\n        events.append(event)\n\n    stats = {}\n    for event in events:\n        name = event.get_name()\n        event.clean_pairs()\n        if name not in stats:\n            stats[name] = 1\n        else:\n            stats[name] += 1\n\n    for stat, value in stats.items():\n        print(f\"\\t{stat} = {value}\")\n\n    return events\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.DumpStableIDs.print_events","title":"<code>print_events(events, output_file)</code>","text":"<p>Print events in a format for BRC.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>List[StableIdEvent]</code> <p>list of events for a given genome.</p> required <code>output_file</code> <code>Path</code> <p>where the events will be printed.</p> required Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>def print_events(self, events: List[StableIdEvent], output_file: Path) -&gt; None:\n    \"\"\"Print events in a format for BRC.\n\n    Args:\n        events: list of events for a given genome.\n        output_file: where the events will be printed.\n\n    \"\"\"\n    if not events:\n        print(\"No events to print\")\n        return\n    with output_file.open(\"w\") as out_fh:\n        for event in events:\n            event_lines = event.brc_format_2()\n            for line in event_lines:\n                out_fh.write(line + \"\\n\")\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.Pair","title":"<code>Pair</code>","text":"<p>Simple old_id - new_id pair representation</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>class Pair:\n    \"\"\"Simple old_id - new_id pair representation\"\"\"\n\n    def __init__(self, old_id: Optional[str], new_id: Optional[str]) -&gt; None:\n        \"\"\"Create a pair with an old_id and a new_id if provided\"\"\"\n\n        self.old_id = old_id if old_id is not None else \"\"\n        if new_id is not None:\n            self.new_id = new_id\n        else:\n            self.new_id = \"\"\n\n    def has_old_id(self) -&gt; bool:\n        \"\"\"Check if the pair has an old_id\"\"\"\n        return self.old_id != \"\"\n\n    def has_new_id(self) -&gt; bool:\n        \"\"\"Check if the pair has a new_id\"\"\"\n        return self.new_id != \"\"\n\n    def is_empty(self) -&gt; bool:\n        \"\"\"Test if the current pair has no id.\"\"\"\n\n        return not (self.has_old_id() or self.has_new_id())\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.Pair.__init__","title":"<code>__init__(old_id, new_id)</code>","text":"<p>Create a pair with an old_id and a new_id if provided</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>def __init__(self, old_id: Optional[str], new_id: Optional[str]) -&gt; None:\n    \"\"\"Create a pair with an old_id and a new_id if provided\"\"\"\n\n    self.old_id = old_id if old_id is not None else \"\"\n    if new_id is not None:\n        self.new_id = new_id\n    else:\n        self.new_id = \"\"\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.Pair.has_new_id","title":"<code>has_new_id()</code>","text":"<p>Check if the pair has a new_id</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>def has_new_id(self) -&gt; bool:\n    \"\"\"Check if the pair has a new_id\"\"\"\n    return self.new_id != \"\"\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.Pair.has_old_id","title":"<code>has_old_id()</code>","text":"<p>Check if the pair has an old_id</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>def has_old_id(self) -&gt; bool:\n    \"\"\"Check if the pair has an old_id\"\"\"\n    return self.old_id != \"\"\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.Pair.is_empty","title":"<code>is_empty()</code>","text":"<p>Test if the current pair has no id.</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>def is_empty(self) -&gt; bool:\n    \"\"\"Test if the current pair has no id.\"\"\"\n\n    return not (self.has_old_id() or self.has_new_id())\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.StableIdEvent","title":"<code>StableIdEvent</code>","text":"<p>Represents a stable id event from one gene set version to another one. Various events: - new genes - deleted genes - merged genes (several genes to one) - split genes (one gene to several) - mixed (several genes to several)</p> <p>Attributes:</p> Name Type Description <code>from_list</code> <p>List of genes the previous gene set.</p> <code>to_list</code> <p>List of genes in the new gene set.</p> <code>release</code> <p>New gene set release name.</p> <code>date</code> <p>Date of the new gene set.</p> <code>name</code> <p>Name of the event (will be updated automatically).</p> <code>pairs</code> <code>List[Pair]</code> <p>All pair of ids for this event.</p> <p>Any gene set before 2019-09 is dubbed pre-BRC4.</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>class StableIdEvent:\n    \"\"\"Represents a stable id event from one gene set version to another one. Various events:\n    - new genes\n    - deleted genes\n    - merged genes (several genes to one)\n    - split genes (one gene to several)\n    - mixed (several genes to several)\n\n    Attributes:\n        from_list: List of genes the previous gene set.\n        to_list: List of genes in the new gene set.\n        release: New gene set release name.\n        date: Date of the new gene set.\n        name: Name of the event (will be updated automatically).\n        pairs: All pair of ids for this event.\n\n    Any gene set before 2019-09 is dubbed pre-BRC4.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        from_list: Optional[Set[str]] = None,\n        to_list: Optional[Set[str]] = None,\n        release: Optional[str] = None,\n        date: Optional[datetime] = None,\n    ) -&gt; None:\n        \"\"\"Create a stable id event from a set of old_ids to a set of new_ids\"\"\"\n\n        if from_list is None:\n            from_list = set()\n        if to_list is None:\n            to_list = set()\n        self.from_set = self.clean_set(from_list)\n        self.to_set = self.clean_set(to_list)\n        self.release = release\n        self.date = date\n        self.name = \"\"\n        self.pairs: List[Pair] = []\n\n    def __str__(self) -&gt; str:\n        \"\"\"String representation of the stable id event\"\"\"\n\n        from_str = \",\".join(self.from_set)\n        to_str = \",\".join(self.to_set)\n        return f\"From {from_str} to {to_str} = {self.get_name()} in release {self.release}\"\n\n    def brc_format_1(self) -&gt; List[str]:\n        \"\"\"Returns a list events, one line per initial ID, in the following TSV format:\n        - old gene id\n        - event name\n        - release\n        - release date\n        - list of old gene ids in the event (comma-separated)\n        - list of new gene ids in the event (comma-separated)\n\n        \"\"\"\n        from_str = \",\".join(self.from_set)\n        to_str = \",\".join(self.to_set)\n        release = self.get_full_release()\n        if self.date:\n            date = self.date.strftime(\"%Y-%m\")\n        else:\n            date = \"no_date\"\n        name = self.get_name()\n        line_list = []\n        for identifier in self.from_set:\n            line = [\n                identifier,\n                name,\n                release,\n                date,\n            ]\n            if name in (\"merge\", \"split\", \"mixed\", \"change\"):\n                line.append(from_str)\n                line.append(to_str)\n            else:\n                line += [\"\", \"\"]\n            line_list.append(\"\\t\".join(line))\n\n        if self.get_name() == \"new\":\n            new_id = [self.to_set][0]\n            line = [new_id, name, release, date, \"\", \"\"]\n            line_list.append(\"\\t\".join(line))\n        return line_list\n\n    def brc_format_2(self) -&gt; List[str]:\n        \"\"\"Returns a list of combination of genes, one line per combination of old_id - new_ids, in the\n        following TSV format:\n        - old gene id\n        - new gene id\n        - event name\n        - release\n        - release date\n\n        \"\"\"\n        release = self.get_full_release()\n        if self.date:\n            date = self.date.strftime(\"%Y-%m\")\n        else:\n            date = \"no_date\"\n        name = self.get_name()\n        line_list = []\n\n        for pair in self.pairs:\n            line = [\n                pair.old_id,\n                pair.new_id,\n                name,\n                release,\n                date,\n            ]\n            line_list.append(\"\\t\".join(line))\n        return line_list\n\n    @staticmethod\n    def clean_set(this_list: Set) -&gt; Set:\n        \"\"\"Removes any empty elements from a list.\n\n        Args:\n            this_list: list of items, so of which can be empty/None.\n\n        Returns:\n            The cleaned list.\n\n        \"\"\"\n        return {identifier for identifier in this_list if identifier}\n\n    def add_from(self, from_id: str) -&gt; None:\n        \"\"\"Store an id in the from_set.\"\"\"\n        if from_id:\n            self.from_set.add(from_id)\n\n    def add_to(self, to_id: str) -&gt; None:\n        \"\"\"Store an id in the from_set.\"\"\"\n        if to_id:\n            self.to_set.add(to_id)\n\n    def set_release(self, release: str) -&gt; None:\n        \"\"\"Set the release name of the event\"\"\"\n        self.release = release\n\n    def set_date(self, date: datetime) -&gt; None:\n        \"\"\"Set the date of the release for this event\"\"\"\n        self.date = date\n\n    def add_pair(self, pair: Pair) -&gt; None:\n        \"\"\"Keeps a record of this pair.\n\n        Args:\n            pair: a Pair to record.\n\n        Raises:\n            ValueError: can't add an empty pair.\n\n        \"\"\"\n        if pair.is_empty():\n            raise ValueError(f\"Expected at least one value in the given pair {pair}\")\n        self.pairs.append(pair)\n\n    def get_full_release(self) -&gt; str:\n        \"\"\"Returns the expanded release name, pre-BRC4 or `BRC4 = build`.\"\"\"\n        release = self.release\n        date = self.date\n\n        if date and date &gt; BRC4_START_DATE:\n            release = f\"build {release}\"\n        else:\n            release = f\"pre-BRC4 {release}\"\n\n        return release\n\n    def _name_event(self) -&gt; None:\n        \"\"\"Identify the event name based on the old vs new id lists.\"\"\"\n        if not self.from_set and len(self.to_set) == 1:\n            self.name = \"new\"\n        elif not self.to_set and len(self.from_set) == 1:\n            self.name = \"deletion\"\n        elif len(self.from_set) == 1 and len(self.to_set) == 1:\n            self.name = \"change\"\n        elif len(self.from_set) == 1 and len(self.to_set) &gt; 1:\n            self.name = \"split\"\n        elif len(self.from_set) &gt; 1 and len(self.to_set) == 1:\n            self.name = \"merge\"\n        elif len(self.from_set) &gt; 1 and len(self.to_set) &gt; 1:\n            self.name = \"mixed\"\n        else:\n            raise UnsupportedEvent(f\"Event {self.from_set} to {self.to_set} is not supported\")\n\n    def clean_pairs(self) -&gt; None:\n        \"\"\"Remove the empty old pairs when the event is not 'new'.\"\"\"\n        if not self.name:\n            self._name_event()\n\n        if self.name != \"new\":\n            new_pairs = []\n            for pair in self.pairs:\n                if not pair.has_old_id():\n                    continue\n                new_pairs.append(pair)\n            self.pairs = new_pairs\n\n    def get_name(self) -&gt; str:\n        \"\"\"Retrieve the name for this event, update it beforehand.\"\"\"\n        self._name_event()\n        return self.name\n\n    def add_pairs(self, pairs: List[Pair]) -&gt; None:\n        \"\"\"Provided all the pairs, keep those that are used by this event.\n\n        Args:\n            pairs: list of Pair.\n\n        \"\"\"\n        for pair in pairs:\n            if (pair.has_old_id() and pair.old_id in self.from_set) or (\n                pair.has_new_id() and pair.new_id in self.to_set\n            ):\n                # Core db contains an empty line to signify that an old id has been removed\n                # in merge/split/mixed\n                name = self.get_name()\n                if (name != \"deletion\") and not pair.has_new_id():\n                    continue\n                self.add_pair(pair)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.StableIdEvent.__init__","title":"<code>__init__(from_list=None, to_list=None, release=None, date=None)</code>","text":"<p>Create a stable id event from a set of old_ids to a set of new_ids</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>def __init__(\n    self,\n    from_list: Optional[Set[str]] = None,\n    to_list: Optional[Set[str]] = None,\n    release: Optional[str] = None,\n    date: Optional[datetime] = None,\n) -&gt; None:\n    \"\"\"Create a stable id event from a set of old_ids to a set of new_ids\"\"\"\n\n    if from_list is None:\n        from_list = set()\n    if to_list is None:\n        to_list = set()\n    self.from_set = self.clean_set(from_list)\n    self.to_set = self.clean_set(to_list)\n    self.release = release\n    self.date = date\n    self.name = \"\"\n    self.pairs: List[Pair] = []\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.StableIdEvent.__str__","title":"<code>__str__()</code>","text":"<p>String representation of the stable id event</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"String representation of the stable id event\"\"\"\n\n    from_str = \",\".join(self.from_set)\n    to_str = \",\".join(self.to_set)\n    return f\"From {from_str} to {to_str} = {self.get_name()} in release {self.release}\"\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.StableIdEvent.add_from","title":"<code>add_from(from_id)</code>","text":"<p>Store an id in the from_set.</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>def add_from(self, from_id: str) -&gt; None:\n    \"\"\"Store an id in the from_set.\"\"\"\n    if from_id:\n        self.from_set.add(from_id)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.StableIdEvent.add_pair","title":"<code>add_pair(pair)</code>","text":"<p>Keeps a record of this pair.</p> <p>Parameters:</p> Name Type Description Default <code>pair</code> <code>Pair</code> <p>a Pair to record.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>can't add an empty pair.</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>def add_pair(self, pair: Pair) -&gt; None:\n    \"\"\"Keeps a record of this pair.\n\n    Args:\n        pair: a Pair to record.\n\n    Raises:\n        ValueError: can't add an empty pair.\n\n    \"\"\"\n    if pair.is_empty():\n        raise ValueError(f\"Expected at least one value in the given pair {pair}\")\n    self.pairs.append(pair)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.StableIdEvent.add_pairs","title":"<code>add_pairs(pairs)</code>","text":"<p>Provided all the pairs, keep those that are used by this event.</p> <p>Parameters:</p> Name Type Description Default <code>pairs</code> <code>List[Pair]</code> <p>list of Pair.</p> required Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>def add_pairs(self, pairs: List[Pair]) -&gt; None:\n    \"\"\"Provided all the pairs, keep those that are used by this event.\n\n    Args:\n        pairs: list of Pair.\n\n    \"\"\"\n    for pair in pairs:\n        if (pair.has_old_id() and pair.old_id in self.from_set) or (\n            pair.has_new_id() and pair.new_id in self.to_set\n        ):\n            # Core db contains an empty line to signify that an old id has been removed\n            # in merge/split/mixed\n            name = self.get_name()\n            if (name != \"deletion\") and not pair.has_new_id():\n                continue\n            self.add_pair(pair)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.StableIdEvent.add_to","title":"<code>add_to(to_id)</code>","text":"<p>Store an id in the from_set.</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>def add_to(self, to_id: str) -&gt; None:\n    \"\"\"Store an id in the from_set.\"\"\"\n    if to_id:\n        self.to_set.add(to_id)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.StableIdEvent.brc_format_1","title":"<code>brc_format_1()</code>","text":"<p>Returns a list events, one line per initial ID, in the following TSV format: - old gene id - event name - release - release date - list of old gene ids in the event (comma-separated) - list of new gene ids in the event (comma-separated)</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>def brc_format_1(self) -&gt; List[str]:\n    \"\"\"Returns a list events, one line per initial ID, in the following TSV format:\n    - old gene id\n    - event name\n    - release\n    - release date\n    - list of old gene ids in the event (comma-separated)\n    - list of new gene ids in the event (comma-separated)\n\n    \"\"\"\n    from_str = \",\".join(self.from_set)\n    to_str = \",\".join(self.to_set)\n    release = self.get_full_release()\n    if self.date:\n        date = self.date.strftime(\"%Y-%m\")\n    else:\n        date = \"no_date\"\n    name = self.get_name()\n    line_list = []\n    for identifier in self.from_set:\n        line = [\n            identifier,\n            name,\n            release,\n            date,\n        ]\n        if name in (\"merge\", \"split\", \"mixed\", \"change\"):\n            line.append(from_str)\n            line.append(to_str)\n        else:\n            line += [\"\", \"\"]\n        line_list.append(\"\\t\".join(line))\n\n    if self.get_name() == \"new\":\n        new_id = [self.to_set][0]\n        line = [new_id, name, release, date, \"\", \"\"]\n        line_list.append(\"\\t\".join(line))\n    return line_list\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.StableIdEvent.brc_format_2","title":"<code>brc_format_2()</code>","text":"<p>Returns a list of combination of genes, one line per combination of old_id - new_ids, in the following TSV format: - old gene id - new gene id - event name - release - release date</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>def brc_format_2(self) -&gt; List[str]:\n    \"\"\"Returns a list of combination of genes, one line per combination of old_id - new_ids, in the\n    following TSV format:\n    - old gene id\n    - new gene id\n    - event name\n    - release\n    - release date\n\n    \"\"\"\n    release = self.get_full_release()\n    if self.date:\n        date = self.date.strftime(\"%Y-%m\")\n    else:\n        date = \"no_date\"\n    name = self.get_name()\n    line_list = []\n\n    for pair in self.pairs:\n        line = [\n            pair.old_id,\n            pair.new_id,\n            name,\n            release,\n            date,\n        ]\n        line_list.append(\"\\t\".join(line))\n    return line_list\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.StableIdEvent.clean_pairs","title":"<code>clean_pairs()</code>","text":"<p>Remove the empty old pairs when the event is not 'new'.</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>def clean_pairs(self) -&gt; None:\n    \"\"\"Remove the empty old pairs when the event is not 'new'.\"\"\"\n    if not self.name:\n        self._name_event()\n\n    if self.name != \"new\":\n        new_pairs = []\n        for pair in self.pairs:\n            if not pair.has_old_id():\n                continue\n            new_pairs.append(pair)\n        self.pairs = new_pairs\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.StableIdEvent.clean_set","title":"<code>clean_set(this_list)</code>  <code>staticmethod</code>","text":"<p>Removes any empty elements from a list.</p> <p>Parameters:</p> Name Type Description Default <code>this_list</code> <code>Set</code> <p>list of items, so of which can be empty/None.</p> required <p>Returns:</p> Type Description <code>Set</code> <p>The cleaned list.</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>@staticmethod\ndef clean_set(this_list: Set) -&gt; Set:\n    \"\"\"Removes any empty elements from a list.\n\n    Args:\n        this_list: list of items, so of which can be empty/None.\n\n    Returns:\n        The cleaned list.\n\n    \"\"\"\n    return {identifier for identifier in this_list if identifier}\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.StableIdEvent.get_full_release","title":"<code>get_full_release()</code>","text":"<p>Returns the expanded release name, pre-BRC4 or <code>BRC4 = build</code>.</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>def get_full_release(self) -&gt; str:\n    \"\"\"Returns the expanded release name, pre-BRC4 or `BRC4 = build`.\"\"\"\n    release = self.release\n    date = self.date\n\n    if date and date &gt; BRC4_START_DATE:\n        release = f\"build {release}\"\n    else:\n        release = f\"pre-BRC4 {release}\"\n\n    return release\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.StableIdEvent.get_name","title":"<code>get_name()</code>","text":"<p>Retrieve the name for this event, update it beforehand.</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>def get_name(self) -&gt; str:\n    \"\"\"Retrieve the name for this event, update it beforehand.\"\"\"\n    self._name_event()\n    return self.name\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.StableIdEvent.set_date","title":"<code>set_date(date)</code>","text":"<p>Set the date of the release for this event</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>def set_date(self, date: datetime) -&gt; None:\n    \"\"\"Set the date of the release for this event\"\"\"\n    self.date = date\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.StableIdEvent.set_release","title":"<code>set_release(release)</code>","text":"<p>Set the release name of the event</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>def set_release(self, release: str) -&gt; None:\n    \"\"\"Set the release name of the event\"\"\"\n    self.release = release\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.UnsupportedEvent","title":"<code>UnsupportedEvent</code>","text":"<p>             Bases: <code>ValueError</code></p> <p>If an event is not supported</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>class UnsupportedEvent(ValueError):\n    \"\"\"If an event is not supported\"\"\"\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/dump/#src.ensembl.io.genomio.events.dump.main","title":"<code>main()</code>","text":"<p>Main entrypoint</p> Source code in <code>src/ensembl/io/genomio/events/dump.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entrypoint\"\"\"\n    parser = ArgumentParser(\n        description=\"Dump the stable ID events from the information available in a core database.\"\n    )\n    parser.add_server_arguments(include_database=True)\n    parser.add_argument_dst_path(\"--output_file\", required=True, help=\"Output file\")\n    args = parser.parse_args()\n\n    # Start\n    factory = CoreServer(host=args.host, port=args.port, user=args.user, password=args.password)\n    factory.set_database(args.database)\n    dumper = DumpStableIDs(factory)\n    events = dumper.get_history()\n    dumper.print_events(events, args.output_file)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/format/","title":"format","text":"<p>Module to map stable ids in a file, given a mapping.</p>"},{"location":"reference/ensembl/io/genomio/events/format/#src.ensembl.io.genomio.events.format.IdsMapper","title":"<code>IdsMapper</code>","text":"<p>Simple mapper object, to cleanly get a mapping dict.</p> Source code in <code>src/ensembl/io/genomio/events/format.py</code> <pre><code>class IdsMapper:\n    \"\"\"Simple mapper object, to cleanly get a mapping dict.\"\"\"\n\n    def __init__(self, map_file: PathLike) -&gt; None:\n        self.map = self._load_mapping(Path(map_file))\n\n    def _load_mapping(self, map_file: Path) -&gt; Dict[str, str]:\n        \"\"\"Return a mapping in a simple dict from a tab file with 2 columns: from_id, to_id.\n\n        Args:\n            map_file: Tab file path.\n        \"\"\"\n        mapping = {}\n        with map_file.open(\"r\") as map_fh:\n            for line in map_fh:\n                if line == \"\":\n                    continue\n                items = line.split(\"\\t\")\n                if len(items) &lt; 2:\n                    raise ValueError(f\"Not 2 elements in {line}\")\n                (from_id, to_id) = items[0:2]\n                mapping[from_id] = to_id\n\n        return mapping\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/format/#src.ensembl.io.genomio.events.format.load_list","title":"<code>load_list(list_file)</code>","text":"<p>Return a simple list from a file.</p> Source code in <code>src/ensembl/io/genomio/events/format.py</code> <pre><code>def load_list(list_file: Path) -&gt; List[str]:\n    \"\"\"Return a simple list from a file.\"\"\"\n    items = set()\n    empty_spaces = re.compile(r\"\\s+\")\n    with Path(list_file).open(\"r\") as map_fh:\n        for line in map_fh:\n            line = re.sub(empty_spaces, \"\", line)\n            if line == \"\":\n                continue\n            items.add(line)\n\n    return list(items)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/format/#src.ensembl.io.genomio.events.format.main","title":"<code>main()</code>","text":"<p>Main entrypoint</p> Source code in <code>src/ensembl/io/genomio/events/format.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entrypoint\"\"\"\n    parser = ArgumentParser(description=\"Map stable IDs in a file and produce an events file.\")\n    parser.add_argument_src_path(\"--input_file\", required=True, help=\"Input file from gene_diff\")\n    parser.add_argument_src_path(\n        \"--deletes_file\", required=True, help=\"Deleted genes file (apart from the deletes from the gene diff)\"\n    )\n    parser.add_argument_src_path(\n        \"--map_file\", required=True, help=\"Mapping tab file with 2 columns: old_id, new_id\"\n    )\n    parser.add_argument(\"--release_name\", required=True, metavar=\"NAME\", help=\"Release name for all events\")\n    parser.add_argument(\"--release_date\", required=True, metavar=\"DATE\", help=\"Release date for all events\")\n    parser.add_argument_dst_path(\"--output_file\", required=True, help=\"Output formatted event file\")\n    args = parser.parse_args()\n\n    events = EventCollection()\n    deleted_genes = load_list(args.deletes_file)\n    events.add_deletes(deleted_genes, args.release_name, args.release_date)\n    events.load_events_from_gene_diff(args.input_file, args.release_name, args.release_date)\n    mapper = IdsMapper(args.map_file)\n    events.remap_to_ids(mapper.map)\n    events.write_events_to_file(args.output_file)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/load/","title":"load","text":"<p>Provided a file with events, load them in a core database.</p> <p>cf the load_events functions for the events tab file format.</p>"},{"location":"reference/ensembl/io/genomio/events/load/#src.ensembl.io.genomio.events.load.EventCollection","title":"<code>EventCollection</code>","text":"<p>\"Collection of events with loader/writer in various formats.</p> Source code in <code>src/ensembl/io/genomio/events/load.py</code> <pre><code>class EventCollection:\n    \"\"\" \"Collection of events with loader/writer in various formats.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self.events: List[IdEvent] = []\n\n    def load_events(self, input_file: PathLike):\n        \"\"\"Load events from input file.\n        Expected tab file columns: old_id, new_id, event_name, release, release_date\n\n        \"\"\"\n        events: List[IdEvent] = []\n\n        with Path(input_file).open(\"r\") as events_fh:\n            for line in events_fh:\n                line.strip()\n                if line == \"\":\n                    continue\n                (from_id, to_id, event_name, release, release_date) = line.split(\"\\t\")\n                event = IdEvent(\n                    from_id=from_id, to_id=to_id, event=event_name, release=release, release_date=release_date\n                )\n                events.append(event)\n        self.events = events\n\n    def add_deletes(\n        self, genes: List[str], release_name: str = \"release_name\", release_date: str = \"release_date\"\n    ) -&gt; None:\n        \"\"\"Add deletion events from a list of deleted genes.\"\"\"\n        for gene_id in genes:\n            event = IdEvent(\n                from_id=gene_id, to_id=\"\", event=\"deletion\", release=release_name, release_date=release_date\n            )\n            self.events.append(event)\n\n    def load_events_from_gene_diff(\n        self, input_file: PathLike, release_name: str = \"release_name\", release_date: str = \"release_date\"\n    ):\n        \"\"\"Load events from input file from gene_diff.\"\"\"\n        loaded_event = set()\n\n        with Path(input_file).open(\"r\") as events_fh:\n            for line in events_fh:\n                if line.startswith(\"//\") or line == \"\":\n                    continue\n                (_, event_string, _) = line.split(\"\\t\")\n                for pair in self._parse_gene_diff_event(event_string):\n                    (from_id, to_id, event_name) = pair\n                    if event_name == \"identical\":\n                        continue\n                    fingerprint = f\"{from_id} {to_id}\"\n                    if fingerprint in loaded_event:\n                        print(f\"Duplicated event, skipped: {fingerprint}\")\n                        continue\n                    loaded_event.add(fingerprint)\n                    event = IdEvent(\n                        from_id=from_id,\n                        to_id=to_id,\n                        event=event_name,\n                        release=release_name,\n                        release_date=release_date,\n                    )\n                    self.events.append(event)\n\n    def _parse_gene_diff_event(self, event_string: str) -&gt; Generator[Tuple[str, str, str], None, None]:\n        \"\"\"Gets all the pairs of IDs from an event string from gene diff.\"\"\"\n        event_symbol = {\n            \"~\": \"identical\",\n            \"=+\": \"iso_gain\",\n            \"=-\": \"iso_loss\",\n            \"=!\": \"broken\",\n            \"=\": \"changed\",\n            \"&gt;\": \"merge\",\n            \"&lt;\": \"split\",\n            \"+\": \"new\",\n        }\n        event_sep = r\"|\".join([symbol.replace(r\"+\", r\"\\+\") for symbol in event_symbol])\n        splitter = f\"({event_sep})\"\n        parts = re.split(splitter, event_string)\n        if len(parts) != 3:\n            print(f\"Wrong partition: from '{event_string}' to '{parts}'\")\n            return\n        [from_ids, sep, to_ids] = parts\n        event_name = event_symbol[sep]\n\n        # Identical gene: no need to keep in the history\n        for from_id in from_ids.split(\":\"):\n            for to_id in to_ids.split(\":\"):\n                yield (from_id, to_id, event_name)\n\n    def remap_to_ids(self, map_dict: Dict[str, str]):\n        \"\"\"Using a mapping dict, remap the to_id of all events.\"\"\"\n\n        no_map = 0\n        for event in self.events:\n            if not event.to_id:\n                continue\n            if event.is_change():\n                event.to_id = event.from_id\n            elif event.to_id in map_dict:\n                event.to_id = map_dict[event.to_id]\n            else:\n                print(f\"No map for to_id {event.to_id}\")\n                no_map += 1\n\n        if no_map:\n            raise ValueError(f\"No map for {no_map} event to_ids\")\n\n    def write_events_to_file(self, output_file: PathLike) -&gt; None:\n        \"\"\"Write the events to a file.\"\"\"\n        with Path(output_file).open(\"w\") as out_fh:\n            print(f\"Write {len(self.events)} events to {output_file}\")\n            for event in self.events:\n                out_fh.write(f\"{event}\\n\")\n\n    def write_events_to_db(self, session: Session, update: bool = False) -&gt; None:\n        \"\"\"Insert the events in the core database.\n        A mapping session is created for each different 'release'.\n\n        \"\"\"\n        # First, create mapping_sessions based on the release\n        mappings: Dict[str, MapSession] = {}\n        for event in self.events:\n            release = event.release\n            if release not in mappings:\n                mappings[release] = MapSession(release, event.release_date)\n            mappings[release].add_event(event)\n\n        # Then, add the mapping, and the events for this mapping\n        for release, mapping in mappings.items():\n            if update:\n                print(f\"Adding mapping for release {release} ({len(mapping.events)} events)\")\n                map_session = MappingSession(new_release=mapping.release, created=mapping.release_date)\n                session.add(map_session)\n                session.flush()\n                session.refresh(map_session)\n                for event in mapping.events:\n                    from_id: Optional[str] = event.from_id\n                    if from_id == \"\":\n                        from_id = None\n                    to_id: Optional[str] = event.to_id\n                    if to_id == \"\":\n                        to_id = None\n                    id_event = StableIdEvent(\n                        mapping_session_id=map_session.mapping_session_id,\n                        old_stable_id=from_id,\n                        new_stable_id=to_id,\n                        id_type=\"gene\",\n                        old_version=1,\n                        new_version=1,\n                    )\n                    session.add(id_event)\n                session.commit()\n            else:\n                print(f\"Found mapping for release {release} ({len(mapping.events)} events)\")\n        if not update:\n            print(\"Run your command again with '--update' to add them\")\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/load/#src.ensembl.io.genomio.events.load.EventCollection.add_deletes","title":"<code>add_deletes(genes, release_name='release_name', release_date='release_date')</code>","text":"<p>Add deletion events from a list of deleted genes.</p> Source code in <code>src/ensembl/io/genomio/events/load.py</code> <pre><code>def add_deletes(\n    self, genes: List[str], release_name: str = \"release_name\", release_date: str = \"release_date\"\n) -&gt; None:\n    \"\"\"Add deletion events from a list of deleted genes.\"\"\"\n    for gene_id in genes:\n        event = IdEvent(\n            from_id=gene_id, to_id=\"\", event=\"deletion\", release=release_name, release_date=release_date\n        )\n        self.events.append(event)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/load/#src.ensembl.io.genomio.events.load.EventCollection.load_events","title":"<code>load_events(input_file)</code>","text":"<p>Load events from input file. Expected tab file columns: old_id, new_id, event_name, release, release_date</p> Source code in <code>src/ensembl/io/genomio/events/load.py</code> <pre><code>def load_events(self, input_file: PathLike):\n    \"\"\"Load events from input file.\n    Expected tab file columns: old_id, new_id, event_name, release, release_date\n\n    \"\"\"\n    events: List[IdEvent] = []\n\n    with Path(input_file).open(\"r\") as events_fh:\n        for line in events_fh:\n            line.strip()\n            if line == \"\":\n                continue\n            (from_id, to_id, event_name, release, release_date) = line.split(\"\\t\")\n            event = IdEvent(\n                from_id=from_id, to_id=to_id, event=event_name, release=release, release_date=release_date\n            )\n            events.append(event)\n    self.events = events\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/load/#src.ensembl.io.genomio.events.load.EventCollection.load_events_from_gene_diff","title":"<code>load_events_from_gene_diff(input_file, release_name='release_name', release_date='release_date')</code>","text":"<p>Load events from input file from gene_diff.</p> Source code in <code>src/ensembl/io/genomio/events/load.py</code> <pre><code>def load_events_from_gene_diff(\n    self, input_file: PathLike, release_name: str = \"release_name\", release_date: str = \"release_date\"\n):\n    \"\"\"Load events from input file from gene_diff.\"\"\"\n    loaded_event = set()\n\n    with Path(input_file).open(\"r\") as events_fh:\n        for line in events_fh:\n            if line.startswith(\"//\") or line == \"\":\n                continue\n            (_, event_string, _) = line.split(\"\\t\")\n            for pair in self._parse_gene_diff_event(event_string):\n                (from_id, to_id, event_name) = pair\n                if event_name == \"identical\":\n                    continue\n                fingerprint = f\"{from_id} {to_id}\"\n                if fingerprint in loaded_event:\n                    print(f\"Duplicated event, skipped: {fingerprint}\")\n                    continue\n                loaded_event.add(fingerprint)\n                event = IdEvent(\n                    from_id=from_id,\n                    to_id=to_id,\n                    event=event_name,\n                    release=release_name,\n                    release_date=release_date,\n                )\n                self.events.append(event)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/load/#src.ensembl.io.genomio.events.load.EventCollection.remap_to_ids","title":"<code>remap_to_ids(map_dict)</code>","text":"<p>Using a mapping dict, remap the to_id of all events.</p> Source code in <code>src/ensembl/io/genomio/events/load.py</code> <pre><code>def remap_to_ids(self, map_dict: Dict[str, str]):\n    \"\"\"Using a mapping dict, remap the to_id of all events.\"\"\"\n\n    no_map = 0\n    for event in self.events:\n        if not event.to_id:\n            continue\n        if event.is_change():\n            event.to_id = event.from_id\n        elif event.to_id in map_dict:\n            event.to_id = map_dict[event.to_id]\n        else:\n            print(f\"No map for to_id {event.to_id}\")\n            no_map += 1\n\n    if no_map:\n        raise ValueError(f\"No map for {no_map} event to_ids\")\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/load/#src.ensembl.io.genomio.events.load.EventCollection.write_events_to_db","title":"<code>write_events_to_db(session, update=False)</code>","text":"<p>Insert the events in the core database. A mapping session is created for each different 'release'.</p> Source code in <code>src/ensembl/io/genomio/events/load.py</code> <pre><code>def write_events_to_db(self, session: Session, update: bool = False) -&gt; None:\n    \"\"\"Insert the events in the core database.\n    A mapping session is created for each different 'release'.\n\n    \"\"\"\n    # First, create mapping_sessions based on the release\n    mappings: Dict[str, MapSession] = {}\n    for event in self.events:\n        release = event.release\n        if release not in mappings:\n            mappings[release] = MapSession(release, event.release_date)\n        mappings[release].add_event(event)\n\n    # Then, add the mapping, and the events for this mapping\n    for release, mapping in mappings.items():\n        if update:\n            print(f\"Adding mapping for release {release} ({len(mapping.events)} events)\")\n            map_session = MappingSession(new_release=mapping.release, created=mapping.release_date)\n            session.add(map_session)\n            session.flush()\n            session.refresh(map_session)\n            for event in mapping.events:\n                from_id: Optional[str] = event.from_id\n                if from_id == \"\":\n                    from_id = None\n                to_id: Optional[str] = event.to_id\n                if to_id == \"\":\n                    to_id = None\n                id_event = StableIdEvent(\n                    mapping_session_id=map_session.mapping_session_id,\n                    old_stable_id=from_id,\n                    new_stable_id=to_id,\n                    id_type=\"gene\",\n                    old_version=1,\n                    new_version=1,\n                )\n                session.add(id_event)\n            session.commit()\n        else:\n            print(f\"Found mapping for release {release} ({len(mapping.events)} events)\")\n    if not update:\n        print(\"Run your command again with '--update' to add them\")\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/load/#src.ensembl.io.genomio.events.load.EventCollection.write_events_to_file","title":"<code>write_events_to_file(output_file)</code>","text":"<p>Write the events to a file.</p> Source code in <code>src/ensembl/io/genomio/events/load.py</code> <pre><code>def write_events_to_file(self, output_file: PathLike) -&gt; None:\n    \"\"\"Write the events to a file.\"\"\"\n    with Path(output_file).open(\"w\") as out_fh:\n        print(f\"Write {len(self.events)} events to {output_file}\")\n        for event in self.events:\n            out_fh.write(f\"{event}\\n\")\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/load/#src.ensembl.io.genomio.events.load.IdEvent","title":"<code>IdEvent</code>  <code>dataclass</code>","text":"<p>Simple representation for the events from the input file</p> Source code in <code>src/ensembl/io/genomio/events/load.py</code> <pre><code>@dataclass\nclass IdEvent:\n    \"\"\"Simple representation for the events from the input file\"\"\"\n\n    from_id: str\n    to_id: str\n    event: str\n    release: str\n    release_date: str\n\n    def __str__(self) -&gt; str:\n        fields = [self.from_id, self.to_id, self.event, self.release, self.release_date]\n        return \"\\t\".join(fields)\n\n    def is_change(self) -&gt; bool:\n        \"\"\"If the event is an update of an existing gene.\"\"\"\n        changed_events = (\"iso_gain\", \"iso_loss\", \"broken\", \"changed\")\n        return self.event in changed_events\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/load/#src.ensembl.io.genomio.events.load.IdEvent.is_change","title":"<code>is_change()</code>","text":"<p>If the event is an update of an existing gene.</p> Source code in <code>src/ensembl/io/genomio/events/load.py</code> <pre><code>def is_change(self) -&gt; bool:\n    \"\"\"If the event is an update of an existing gene.\"\"\"\n    changed_events = (\"iso_gain\", \"iso_loss\", \"broken\", \"changed\")\n    return self.event in changed_events\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/load/#src.ensembl.io.genomio.events.load.MapSession","title":"<code>MapSession</code>","text":"<p>Simple mapping_sessions representation from the input file</p> Source code in <code>src/ensembl/io/genomio/events/load.py</code> <pre><code>class MapSession:\n    \"\"\"Simple mapping_sessions representation from the input file\"\"\"\n\n    def __init__(self, release: str, release_date: str) -&gt; None:\n        self.release = release\n        self.release_date = release_date\n        self.events: List[IdEvent] = []\n\n    def add_event(self, event: IdEvent) -&gt; None:\n        \"\"\"Add an event to this mapping_session\"\"\"\n        self.events.append(event)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/load/#src.ensembl.io.genomio.events.load.MapSession.add_event","title":"<code>add_event(event)</code>","text":"<p>Add an event to this mapping_session</p> Source code in <code>src/ensembl/io/genomio/events/load.py</code> <pre><code>def add_event(self, event: IdEvent) -&gt; None:\n    \"\"\"Add an event to this mapping_session\"\"\"\n    self.events.append(event)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/events/load/#src.ensembl.io.genomio.events.load.main","title":"<code>main()</code>","text":"<p>Main entrypoint</p> Source code in <code>src/ensembl/io/genomio/events/load.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entrypoint\"\"\"\n    parser = ArgumentParser(description=\"Load the events in the input file into a core database.\")\n    parser.add_server_arguments(include_database=True)\n    parser.add_argument_src_path(\n        \"--input_file\",\n        required=True,\n        help=(\n            \"Input TSV file with events in the format exported by the dumper: old_id, new_id, event_name, \"\n            \"release, date\"\n        ),\n    )\n    parser.add_argument(\"--update\", action=\"store_true\", help=\"Make changes to the database\")\n    args = parser.parse_args()\n\n    # Start\n    dbc = DBConnection(args.url)\n\n    collection = EventCollection()\n    collection.load_events(args.input_file)\n\n    with dbc.session_scope() as session:\n        collection.write_events_to_db(session, args.update)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/fasta/","title":"fasta","text":"<p>Fasta files processing module.</p>"},{"location":"reference/ensembl/io/genomio/fasta/process/","title":"process","text":"<p>Takes a FASTA file (DNA or peptide), cleans it up and optionally excludes some IDs.</p>"},{"location":"reference/ensembl/io/genomio/fasta/process/#src.ensembl.io.genomio.fasta.process.GFFParserError","title":"<code>GFFParserError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Error while parsing a GFF file.</p> Source code in <code>src/ensembl/io/genomio/fasta/process.py</code> <pre><code>class GFFParserError(Exception):\n    \"\"\"Error while parsing a GFF file.\"\"\"\n</code></pre>"},{"location":"reference/ensembl/io/genomio/fasta/process/#src.ensembl.io.genomio.fasta.process.get_peptides_to_exclude","title":"<code>get_peptides_to_exclude(genbank_path, seqr_to_exclude)</code>","text":"<p>Extract peptide IDs from a genbank file that are in a given list of seq regions</p> Source code in <code>src/ensembl/io/genomio/fasta/process.py</code> <pre><code>def get_peptides_to_exclude(genbank_path: PathLike, seqr_to_exclude: Set[str]) -&gt; Set[str]:\n    \"\"\"\n    Extract peptide IDs from a genbank file that are in a given list of seq regions\n    \"\"\"\n    peptides_to_exclude: Set[str] = set()\n    with open_gz_file(genbank_path) as in_genbank:\n        for record in SeqIO.parse(in_genbank, \"genbank\"):\n            if record.id in seqr_to_exclude:\n                print(f\"Skip sequence {record.id}\")\n                for feat in record.features:\n                    if feat.type == \"CDS\":\n                        if \"protein_id\" in feat.qualifiers:\n                            feat_id = feat.qualifiers[\"protein_id\"]\n                            peptides_to_exclude.add(feat_id[0])\n                        else:\n                            raise GFFParserError(f\"Peptide without peptide ID ${feat}\")\n    return peptides_to_exclude\n</code></pre>"},{"location":"reference/ensembl/io/genomio/fasta/process/#src.ensembl.io.genomio.fasta.process.main","title":"<code>main()</code>","text":"<p>Module's entry-point.</p> Source code in <code>src/ensembl/io/genomio/fasta/process.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Module's entry-point.\"\"\"\n    parser = ArgumentParser(description=\"Clean-up a given FASTA file to remove unwanted elements.\")\n    parser.add_argument_src_path(\"--fasta_infile\", required=True, help=\"Input FASTA file - DNA / Protein\")\n    parser.add_argument_src_path(\"--genbank_infile\", help=\"Input GenBank GBFF file\")\n    parser.add_argument_dst_path(\"--fasta_outfile\", required=True, help=\"Output FASTA file\")\n    parser.add_argument(\"--peptide_mode\", action=\"store_true\", help=\"Process proteins instead of DNA\")\n    args = parser.parse_args()\n\n    prep_fasta_data(**vars(args))\n</code></pre>"},{"location":"reference/ensembl/io/genomio/fasta/process/#src.ensembl.io.genomio.fasta.process.prep_fasta_data","title":"<code>prep_fasta_data(fasta_infile, genbank_infile, fasta_outfile, peptide_mode=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>fasta_file</code> <p>Input FASTA file - DNA / Protein</p> required <code>genbank_infile</code> <code>Optional[PathLike]</code> <p>Input GenBank GBFF file (Optional)</p> required <code>fasta_outfile</code> <code>PathLike</code> <p>Output FASTA sequence file.</p> required <code>peptide_mode</code> <code>bool</code> <p>Process proteins instead of DNA</p> <code>False</code> Source code in <code>src/ensembl/io/genomio/fasta/process.py</code> <pre><code>def prep_fasta_data(\n    fasta_infile: PathLike,\n    genbank_infile: Optional[PathLike],\n    fasta_outfile: PathLike,\n    peptide_mode: bool = False,\n) -&gt; None:\n    \"\"\"\n    Args:\n        fasta_file: Input FASTA file - DNA / Protein\n        genbank_infile: Input GenBank GBFF file (Optional)\n        fasta_outfile: Output FASTA sequence file.\n        peptide_mode: Process proteins instead of DNA\n    \"\"\"\n    file_path = Path(fasta_infile)\n\n    seqr_to_exclude = set(exclude_seq_regions)\n    if peptide_mode:\n        if genbank_infile is not None:\n            genbank_path = Path(genbank_infile)\n            to_exclude = get_peptides_to_exclude(genbank_path, seqr_to_exclude)\n    else:\n        to_exclude = seqr_to_exclude\n\n    # Copy and filter\n    records = []\n\n    # Final path\n    with open_gz_file(file_path) as in_fasta:\n        for record in SeqIO.parse(in_fasta, \"fasta\"):\n            if record.id in to_exclude:\n                print(f\"Skip record ${record.id}\")\n            else:\n                records.append(record)\n    with Path(fasta_outfile).open(\"w\") as out_fasta:\n        SeqIO.write(records, out_fasta, \"fasta\")\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genbank/","title":"genbank","text":"<p>GenBank fetching and data manipulation module.</p>"},{"location":"reference/ensembl/io/genomio/genbank/download/","title":"download","text":"<p>Download a Genbank file from NCBI from an accession.</p>"},{"location":"reference/ensembl/io/genomio/genbank/download/#src.ensembl.io.genomio.genbank.download.DownloadError","title":"<code>DownloadError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>In case a download failed.</p> Source code in <code>src/ensembl/io/genomio/genbank/download.py</code> <pre><code>class DownloadError(Exception):\n    \"\"\"In case a download failed.\"\"\"\n\n    def __init__(self, msg):\n        self.msg = msg\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genbank/download/#src.ensembl.io.genomio.genbank.download.download_genbank","title":"<code>download_genbank(accession, output_file)</code>","text":"<p>Given a GenBank accession, download the corresponding file in GenBank format.</p> <p>Uses NCBI Entrez service to fetch the data.</p> <p>Parameters:</p> Name Type Description Default <code>accession</code> <code>str</code> <p>INSDC Genbank record accession.</p> required <code>output_file</code> <code>PathLike</code> <p>Path to the downloaded record in Genbank format.</p> required <p>Raises:</p> Type Description <code>DownloadError</code> <p>If the download fails.</p> Source code in <code>src/ensembl/io/genomio/genbank/download.py</code> <pre><code>def download_genbank(accession: str, output_file: PathLike) -&gt; None:\n    \"\"\"Given a GenBank accession, download the corresponding file in GenBank format.\n\n    Uses NCBI Entrez service to fetch the data.\n\n    Args:\n        accession: INSDC Genbank record accession.\n        output_file: Path to the downloaded record in Genbank format.\n\n    Raises:\n        DownloadError: If the download fails.\n\n    \"\"\"\n\n    # Get the list of assemblies for this accession\n    entrez_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n    entrez_params = {\n        \"db\": \"nuccore\",\n        \"rettype\": \"gbwithparts\",\n        \"retmode\": \"text\",\n    }\n    entrez_params[\"id\"] = accession\n    result = requests.get(entrez_url, params=entrez_params, timeout=60)\n    if result and result.status_code == 200:\n        with Path(output_file).open(\"wb\") as gbff:\n            gbff.write(result.content)\n        print(f\"GBF file write to {output_file}\")\n        return\n    raise DownloadError(f\"Could not download the genbank ({accession}) file: {result}\")\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genbank/download/#src.ensembl.io.genomio.genbank.download.main","title":"<code>main()</code>","text":"<p>Main script entry-point.</p> Source code in <code>src/ensembl/io/genomio/genbank/download.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main script entry-point.\"\"\"\n    parser = ArgumentParser(description=\"Download a sequence from GenBank.\")\n    parser.add_argument(\"--accession\", required=True, help=\"Sequence accession\")\n    parser.add_argument_dst_path(\"--output_file\", required=True, help=\"Output GenBank file\")\n    args = parser.parse_args()\n\n    download_genbank(**vars(args))\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genbank/extract_data/","title":"extract_data","text":"<p>Parse a Genbank file and creates cleaned up files from it: - DNA fasta - Peptide fasta - Gene models GFF3 - seq_regions json - genome metadata json</p> <p>Raises:</p> Type Description <code>GFFPArseError</code> <p>If the structure of the gb file can't be parsed.</p> <code>UnsupportedData</code> <p>If some data is not as expected.</p> <p>Returns:</p> Name Type Description <code>json_output</code> <p>json file with a dict that contains all genome files created.</p>"},{"location":"reference/ensembl/io/genomio/genbank/extract_data/#src.ensembl.io.genomio.genbank.extract_data.FormattedFilesGenerator","title":"<code>FormattedFilesGenerator</code>","text":"<p>Contains a parser to load data from a file, and output a set of files that follow our schema for input into a core database</p> Source code in <code>src/ensembl/io/genomio/genbank/extract_data.py</code> <pre><code>class FormattedFilesGenerator:\n    \"\"\"\n    Contains a parser to load data from a file, and output a set of files that follow our schema\n    for input into a core database\n    \"\"\"\n\n    locations = {\n        \"mitochondrion\": \"mitochondrial_chromosome\",\n        \"apicoplast\": \"apicoplast_chromosome\",\n        \"chloroplast\": \"chloroplast_chromosome\",\n        \"chromoplast\": \"chromoplast_chromosome\",\n        \"cyanelle\": \"cyanelle_chromosome\",\n        \"leucoplast\": \"leucoplast_chromosome\",\n    }\n\n    allowed_feat_types = [\n        \"gene\",\n        \"transcript\",\n        \"tRNA\",\n        \"rRNA\",\n        \"CDS\",\n    ]\n\n    def __init__(self, prod_name: str, gb_file: Path, prefix: str = \"\"):\n        self.prefix = prefix\n        self.seq_records: List[SeqRecord] = []\n        self.prod_name = prod_name\n        self.gb_file = gb_file\n        self.files = GenomeFiles()\n\n    def set_prefix(self, prefix):\n        \"\"\"\n        Define a prefix to add to the feature IDs\n        \"\"\"\n        if prefix:\n            self.prefix = prefix\n\n    def set_production_name(self, prod_name):\n        \"\"\"\n        Define a production_name for the genome\n        \"\"\"\n        if prod_name:\n            self.prod_name = prod_name\n\n    def extract_gb(self, out_dir: Optional[PathLike]) -&gt; Dict[str, Path]:\n        \"\"\"Extract data from a Genbank file and create files from it.\"\"\"\n        if out_dir is not None:\n            self.files = GenomeFiles(out_dir)\n        self.set_prefix(self.prefix)\n        self.set_production_name(self.prod_name)\n        self.parse_genbank(Path(self.gb_file))\n\n        # Output the gff3 file\n        return self.files\n\n    def parse_genbank(self, gb_file):\n        \"\"\"\n        Load a sequence from a Genbank file\n        \"\"\"\n\n        organella = self._get_organella(gb_file)\n\n        with open(gb_file, \"r\") as gbh:\n            for record in SeqIO.parse(gbh, \"genbank\"):\n                # We don't want the record description (especially for the fasta file)\n                record.description = \"\"\n                record.organelle = None\n                if record.id in organella:\n                    record.organelle = organella[record.id]\n                self.seq_records.append(record)\n\n            self._write_genome_json()\n            self._write_genes_gff()\n            self._write_seq_region_json()\n            self._write_fasta_dna()\n\n    def _get_organella(self, gb_file):\n        \"\"\"\n        Retrieve the organelle from the genbank file, using the specific GenBank object,\n        because SeqIO does not support this field\n        \"\"\"\n        organella = {}\n        with open(gb_file, \"r\") as gbh:\n            for record in GenBank.parse(gbh):\n                accession = record.version\n                for q in record.features[0].qualifiers:\n                    if q.key == \"/organelle=\":\n                        organelle = q.value.replace('\"', \"\")\n                        organella[accession] = organelle\n        return organella\n\n    def _write_fasta_dna(self):\n        with open(self.files[\"fasta_dna\"], \"w\") as fasta_fh:\n            SeqIO.write(self.seq_records, fasta_fh, \"fasta\")\n\n    def _write_genes_gff(self) -&gt; None:\n        \"\"\"Extract gene models from the record, and write a GFF and peptide fasta file.\n        Raise GBParseError If the IDs in all the records are not unique.\"\"\"\n        peptides = []\n        records = []\n        all_ids = []\n\n        for record in self.seq_records:\n            new_record, rec_ids, rec_peptides = self._parse_record(record)\n            records.append(new_record)\n            all_ids += rec_ids\n            peptides += rec_peptides\n\n        # Write those records to a clean GFF\n        with self.files[\"gene_models\"].open(\"w\") as gff_fh:\n            GFF.write(records, gff_fh)\n\n        # Write the peptide sequences to a fasta file\n        with self.files[\"fasta_pep\"].open(\"w\") as fasta_fh:\n            SeqIO.write(peptides, fasta_fh, \"fasta\")\n\n        # Warn if some IDs are not unique\n        count = dict(Counter(all_ids))\n        num_duplicates = 0\n        for key in count:\n            if count[key] &gt; 1:\n                num_duplicates += 1\n                print(f\"ID {key} is duplicated {count[key]} times\")\n        if num_duplicates &gt; 0:\n            raise GBParseError(f\"Some {num_duplicates} IDs are duplicated\")\n\n    def _parse_record(self, record: SeqRecord) -&gt; Tuple[SeqRecord, List[SeqRecord], List[str]]:\n        all_ids: List[str] = []\n        peptides: List[SeqFeature] = []\n        feats: Dict[str, SeqFeature] = {}\n\n        for feat in record.features:\n            # Silently skip any unsupported feature type\n            if feat.type not in self.allowed_feat_types:\n                continue\n\n            # Create a clean clone of the feature\n            gff_qualifiers = feat.qualifiers\n            gff_feat = SeqFeature(\n                location=feat.location,\n                type=feat.type,\n                strand=feat.location.strand,\n                qualifiers=gff_qualifiers,\n            )\n            # Only Genes should have a name: use either attribute gene or locus_tag\n            gene_name = gff_qualifiers.get(\"gene\", [None])[0]\n            if gene_name is None:\n                gene_name = gff_qualifiers.get(\"locus_tag\", [None])[0]\n\n            # Parse this gene\n            if gene_name is not None:\n                gene_feats, gene_ids, gene_peptides = self._parse_gene_feat(gff_feat, gene_name)\n                peptides += gene_peptides\n                feats = {**feats, **gene_feats}\n                all_ids += gene_ids\n\n            # No gene ID: parse if it is a tRNA or rRNA\n            elif gff_feat.type in (\"tRNA\", \"rRNA\"):\n                rna_feats, rna_ids = self._parse_rna_feat(gff_feat)\n                feats = {**feats, **rna_feats}\n                all_ids += rna_ids\n\n            # Any other case? Fail here and check if we shoud support it, or add it to unsupported list\n            else:\n                raise GBParseError(f\"No ID for allowed feature: {feat}\")\n\n        new_record = SeqRecord(record.seq, record.id)\n        new_record.features = feats.values()\n        return new_record, all_ids, peptides\n\n    def _parse_gene_feat(\n        self, gene_feat: SeqFeature, gene_name: str\n    ) -&gt; Tuple[Dict[str, SeqFeature], List[str], List[SeqFeature]]:\n        gene_id = self.prefix + gene_name\n        gene_qualifiers = gene_feat.qualifiers\n        new_feats: Dict[str, Any] = {}\n        peptides: List[SeqFeature] = []\n        all_ids: List[str] = []\n\n        if gene_feat.type == \"gene\":\n            if \"pseudo\" in gene_qualifiers:\n                gene_feat.type = \"pseudogene\"\n            gene_feat.qualifiers[\"ID\"] = gene_id\n            gene_feat.qualifiers[\"Name\"] = gene_name\n            if \"gene\" in gene_feat.qualifiers:\n                del gene_feat.qualifiers[\"gene\"]\n            if \"locus_tag\" in gene_feat.qualifiers:\n                del gene_feat.qualifiers[\"locus_tag\"]\n            new_feats[str(gene_id)] = gene_feat\n            all_ids.append(str(gene_id))\n\n        if gene_feat.type in (\"tRNA\", \"rRNA\"):\n            tr_id = gene_id + \"_t1\"\n            gene_feat.qualifiers[\"ID\"] = tr_id\n            gene_feat.qualifiers[\"Parent\"] = gene_id\n            if \"gene\" in gene_feat.qualifiers:\n                del gene_feat.qualifiers[\"gene\"]\n            if \"locus_tag\" in gene_feat.qualifiers:\n                del gene_feat.qualifiers[\"locus_tag\"]\n            new_feats[str(tr_id)] = gene_feat\n            all_ids.append(str(tr_id))\n\n        if gene_feat.type == \"CDS\":\n            if \"pseudo\" in gene_qualifiers:\n                gene_feat.type = \"exon\"\n            cds_id = gene_id + \"_p1\"\n            tr_id = gene_id + \"_t1\"\n            gene_feat.qualifiers[\"ID\"] = cds_id\n            gene_feat.qualifiers[\"Parent\"] = tr_id\n            if \"gene\" in gene_feat.qualifiers:\n                del gene_feat.qualifiers[\"gene\"]\n            if \"locus_tag\" in gene_feat.qualifiers:\n                del gene_feat.qualifiers[\"locus_tag\"]\n\n            # Add fasta to pep fasta file\n            if \"translation\" in gene_qualifiers:\n                new_pep_record = SeqRecord(Seq(gene_qualifiers[\"translation\"][0]), id=cds_id)\n                peptides.append(new_pep_record)\n\n            # Also create a parent transcript for this translation\n            tr_qualifiers = {\"ID\": tr_id, \"Name\": gene_name, \"Parent\": gene_id}\n            gff_tr = SeqFeature(\n                location=gene_feat.location,\n                type=\"mRNA\",\n                strand=gene_feat.location.strand,\n                qualifiers=tr_qualifiers,\n            )\n            new_feats[str(tr_id)] = gff_tr\n            new_feats[str(cds_id)] = gene_feat\n            all_ids.append(str(tr_id))\n            all_ids.append(str(cds_id))\n\n        return new_feats, all_ids, peptides\n\n    def _parse_rna_feat(self, rna_feat: SeqFeature) -&gt; Tuple[Dict[str, SeqFeature], List[SeqFeature]]:\n        new_feats: Dict[str, Any] = {}\n        all_ids: List[str] = []\n\n        gff_qualifiers = rna_feat.qualifiers\n        feat_name = gff_qualifiers[\"product\"][0]\n        gene_id = self.prefix + feat_name\n\n        parts = gene_id.split(\" \")\n        if len(parts) &gt; 2:\n            print(f\"Shortening gene_id to {parts[0]}\")\n            gene_id = parts[0]\n        gene_id = self._uniquify_id(gene_id, all_ids)\n\n        feat_id = gene_id + \"_t1\"\n        rna_feat.qualifiers[\"ID\"] = feat_id\n        rna_feat.qualifiers[\"Name\"] = feat_name\n        rna_feat.qualifiers[\"Parent\"] = gene_id\n\n        # Also create a parent gene for this transcript\n        gene_qualifiers = {\n            \"ID\": gene_id,\n            \"Name\": feat_name,\n        }\n        gff_gene = SeqFeature(\n            location=rna_feat.location,\n            type=\"gene\",\n            strand=rna_feat.location.strand,\n            qualifiers=gene_qualifiers,\n        )\n        new_feats[str(gene_id)] = gff_gene\n        new_feats[str(feat_id)] = rna_feat\n        all_ids.append(str(gene_id))\n        all_ids.append(str(feat_id))\n\n        return new_feats, all_ids\n\n    def _uniquify_id(self, gene_id, all_ids):\n        \"\"\"Ensure the gene id used is unique,\n        and append a number otherwise, starting at 2\n        \"\"\"\n\n        new_id = gene_id\n        num = 1\n        while new_id in all_ids:\n            print(f\"{new_id} exists, update\")\n            num += 1\n            new_id = f\"{gene_id}_{num}\"\n        print(f\"Using {new_id}\")\n\n        return new_id\n\n    def _write_seq_region_json(self):\n        json_array = []\n\n        for seq in self.seq_records:\n            codon_table = self._get_codon_table(seq)\n            if codon_table is None:\n                print(\n                    (\n                        \"Warning: No codon table found. \"\n                        f\"Make sure to change the codon table number in {self.files['seq_region']} manually \"\n                        \"if it is not the standard codon table\"\n                    )\n                )\n\n                codon_table = 1\n            else:\n                codon_table = int(codon_table)\n            seq_obj = {\n                \"name\": seq.id,\n                \"coord_system_level\": \"chromosome\",\n                \"circular\": (seq.annotations[\"topology\"] == \"circular\"),\n                \"codon_table\": codon_table,\n                \"length\": len(seq.seq),\n            }\n            if seq.organelle:\n                seq_obj[\"location\"] = self._prepare_location(seq.organelle)\n                if not codon_table:\n                    print(\n                        (\n                            f\"Warning: '{seq.organelle}' is an organelle: \"\n                            f\"make sure to change the codon table number in {self.files['seq_region']} \"\n                            \"manually if it is not the standard codon table\"\n                        )\n                    )\n\n            # Additional attributes for Ensembl\n            seq_obj[\"added_sequence\"] = {\n                \"accession\": seq.id,\n                \"assembly_provider\": {\n                    \"name\": \"GenBank\",\n                    \"url\": \"https://www.ncbi.nlm.nih.gov/genbank\",\n                },\n            }\n            if not seq_obj[\"added_sequence\"][\"assembly_provider\"][\"name\"]:\n                print(\n                    (\n                        \"Warning: please add the relevant provider name\"\n                        f\"for the assembly in {self.files['seq_region']}\"\n                    )\n                )\n            if not seq_obj[\"added_sequence\"][\"assembly_provider\"][\"url\"]:\n                print(\n                    (\n                        \"Warning: please add the relevant provider url\"\n                        f\" for the assembly in {self.files['seq_region']}\"\n                    )\n                )\n\n            json_array.append(seq_obj)\n        with open(self.files[\"seq_region\"], \"w\") as seq_fh:\n            seq_fh.write(json.dumps(json_array, indent=4))\n\n    def _get_codon_table(self, seq) -&gt; Optional[int]:\n        \"\"\"\n        Look at the CDS features to see if they have a codon table\n        \"\"\"\n        for feat in seq.features:\n            if feat.type == \"CDS\":\n                quals = feat.qualifiers\n                if \"transl_table\" in quals:\n                    return quals[\"transl_table\"][0]\n                return None\n        return None\n\n    def _prepare_location(self, organelle):\n        \"\"\"\n        Given an organelle name, returns the SO term corresponding to its location\n        \"\"\"\n        if organelle in self.locations:\n            return self.locations[organelle]\n        raise UnsupportedData(f\"Unkown organelle: {organelle}\")\n\n    def _write_genome_json(self):\n        \"\"\"\n        Write a draft for the genome json file\n        Only the production_name is needed, but the rest of the fields need to be given\n        for the validation of the json file\n        \"\"\"\n\n        prod_name = self.prod_name if self.prod_name else \"\"\n\n        genome_data = {\n            \"species\": {\n                \"production_name\": prod_name,\n                \"taxonomy_id\": 0,\n            },\n            \"assembly\": {\"accession\": \"GCA_000000000\", \"version\": 1},\n            \"added_seq\": {},\n        }\n\n        if not genome_data[\"species\"][\"production_name\"]:\n            print(\n                f\"Warning: please add the relevant production_name for this genome in {self.files['genome']}\"\n            )\n\n        ids = [seq.id for seq in self.seq_records]\n        genome_data[\"added_seq\"][\"region_name\"] = ids\n\n        with open(self.files[\"genome\"], \"w\") as genome_fh:\n            genome_fh.write(json.dumps(genome_data, indent=4))\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genbank/extract_data/#src.ensembl.io.genomio.genbank.extract_data.FormattedFilesGenerator.extract_gb","title":"<code>extract_gb(out_dir)</code>","text":"<p>Extract data from a Genbank file and create files from it.</p> Source code in <code>src/ensembl/io/genomio/genbank/extract_data.py</code> <pre><code>def extract_gb(self, out_dir: Optional[PathLike]) -&gt; Dict[str, Path]:\n    \"\"\"Extract data from a Genbank file and create files from it.\"\"\"\n    if out_dir is not None:\n        self.files = GenomeFiles(out_dir)\n    self.set_prefix(self.prefix)\n    self.set_production_name(self.prod_name)\n    self.parse_genbank(Path(self.gb_file))\n\n    # Output the gff3 file\n    return self.files\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genbank/extract_data/#src.ensembl.io.genomio.genbank.extract_data.FormattedFilesGenerator.parse_genbank","title":"<code>parse_genbank(gb_file)</code>","text":"<p>Load a sequence from a Genbank file</p> Source code in <code>src/ensembl/io/genomio/genbank/extract_data.py</code> <pre><code>def parse_genbank(self, gb_file):\n    \"\"\"\n    Load a sequence from a Genbank file\n    \"\"\"\n\n    organella = self._get_organella(gb_file)\n\n    with open(gb_file, \"r\") as gbh:\n        for record in SeqIO.parse(gbh, \"genbank\"):\n            # We don't want the record description (especially for the fasta file)\n            record.description = \"\"\n            record.organelle = None\n            if record.id in organella:\n                record.organelle = organella[record.id]\n            self.seq_records.append(record)\n\n        self._write_genome_json()\n        self._write_genes_gff()\n        self._write_seq_region_json()\n        self._write_fasta_dna()\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genbank/extract_data/#src.ensembl.io.genomio.genbank.extract_data.FormattedFilesGenerator.set_prefix","title":"<code>set_prefix(prefix)</code>","text":"<p>Define a prefix to add to the feature IDs</p> Source code in <code>src/ensembl/io/genomio/genbank/extract_data.py</code> <pre><code>def set_prefix(self, prefix):\n    \"\"\"\n    Define a prefix to add to the feature IDs\n    \"\"\"\n    if prefix:\n        self.prefix = prefix\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genbank/extract_data/#src.ensembl.io.genomio.genbank.extract_data.FormattedFilesGenerator.set_production_name","title":"<code>set_production_name(prod_name)</code>","text":"<p>Define a production_name for the genome</p> Source code in <code>src/ensembl/io/genomio/genbank/extract_data.py</code> <pre><code>def set_production_name(self, prod_name):\n    \"\"\"\n    Define a production_name for the genome\n    \"\"\"\n    if prod_name:\n        self.prod_name = prod_name\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genbank/extract_data/#src.ensembl.io.genomio.genbank.extract_data.GBParseError","title":"<code>GBParseError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Error when parsing the Genbank file.</p> Source code in <code>src/ensembl/io/genomio/genbank/extract_data.py</code> <pre><code>class GBParseError(Exception):\n    \"\"\"Error when parsing the Genbank file.\"\"\"\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genbank/extract_data/#src.ensembl.io.genomio.genbank.extract_data.GenomeFiles","title":"<code>GenomeFiles</code>","text":"<p>             Bases: <code>dict</code></p> <p>Store the representation of the genome files created.</p> Source code in <code>src/ensembl/io/genomio/genbank/extract_data.py</code> <pre><code>class GenomeFiles(dict):\n    \"\"\"Store the representation of the genome files created.\"\"\"\n\n    def __init__(self, out_dir: PathLike = Path.cwd()) -&gt; None:\n        super().__init__()\n        out_dir = Path(out_dir)\n        self[\"genome\"] = out_dir / \"genome.json\"\n        self[\"seq_region\"] = out_dir / \"seq_region.json\"\n        self[\"fasta_dna\"] = out_dir / \"dna.fasta\"\n        self[\"fasta_pep\"] = out_dir / \"pep.fasta\"\n        self[\"gene_models\"] = out_dir / \"genes.gff\"\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genbank/extract_data/#src.ensembl.io.genomio.genbank.extract_data.UnsupportedData","title":"<code>UnsupportedData</code>","text":"<p>             Bases: <code>Exception</code></p> <p>When an expected data is not supported by the current parser.</p> Source code in <code>src/ensembl/io/genomio/genbank/extract_data.py</code> <pre><code>class UnsupportedData(Exception):\n    \"\"\"When an expected data is not supported by the current parser.\"\"\"\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genbank/extract_data/#src.ensembl.io.genomio.genbank.extract_data.main","title":"<code>main()</code>","text":"<p>Main script entry-point.</p> Source code in <code>src/ensembl/io/genomio/genbank/extract_data.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main script entry-point.\"\"\"\n    parser = ArgumentParser(description=\"Parse a GenBank file and create cleaned up files from it.\")\n    parser.add_argument_src_path(\"--gb_file\", required=True, help=\"Sequence accession file\")\n    parser.add_argument(\"--prefix\", required=True, help=\"Prefix to add to every feature ID\")\n    parser.add_argument(\"--prod_name\", required=True, help=\"Production name for the species\")\n    parser.add_argument_dst_path(\n        \"--out_dir\", default=Path.cwd(), help=\"Output folder where the generated files will be stored\"\n    )\n    args = parser.parse_args()\n\n    gb_extractor = FormattedFilesGenerator(prefix=args.prefix, prod_name=args.prod_name, gb_file=args.gb_file)\n    gb_extractor.extract_gb(args.out_dir)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_metadata/","title":"genome_metadata","text":"<p>Genome metadata handling module.</p>"},{"location":"reference/ensembl/io/genomio/genome_metadata/dump/","title":"dump","text":"<p>Generates a JSON file representing the genome metadata from a core database.</p>"},{"location":"reference/ensembl/io/genomio/genome_metadata/dump/#src.ensembl.io.genomio.genome_metadata.dump.check_assembly_version","title":"<code>check_assembly_version(gmeta_out)</code>","text":"<p>Update the assembly version of the genome metadata provided to use an integer. Get the version from the assembly accession as alternative.</p> <p>Parameters:</p> Name Type Description Default <code>gmeta</code> <code>Dict[str, Any]</code> <p>Nested metadata key values from the core metadata table.</p> required Source code in <code>src/ensembl/io/genomio/genome_metadata/dump.py</code> <pre><code>def check_assembly_version(gmeta_out: Dict[str, Any]) -&gt; None:\n    \"\"\"Update the assembly version of the genome metadata provided to use an integer.\n    Get the version from the assembly accession as alternative.\n\n    Args:\n        gmeta (Dict[str, Any]): Nested metadata key values from the core metadata table.\n    \"\"\"\n    assembly = gmeta_out[\"assembly\"]\n    version = assembly.get(\"version\")\n\n    # Check the version is an integer\n    if version is not None and version.isdigit():\n        assembly[\"version\"] = int(version)\n    else:\n        # Get the version from the assembly accession\n        accession = assembly[\"accession\"]\n        parts = accession.split(\".\")\n        if len(parts) == 2 and parts[1].isdigit():\n            version = parts[1]\n            assembly[\"version\"] = int(version)\n        else:\n            raise ValueError(f\"Assembly version is not an integer in {assembly}\")\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_metadata/dump/#src.ensembl.io.genomio.genome_metadata.dump.filter_genome_meta","title":"<code>filter_genome_meta(gmeta)</code>","text":"<p>Returns a filtered metadata dict with only predefined keys. Also converts expected numbers to integers (to follow the genome json schema).</p> <p>Parameters:</p> Name Type Description Default <code>gmeta</code> <code>Dict[str, Any]</code> <p>Nested metadata key values from the core metadata table.</p> required Source code in <code>src/ensembl/io/genomio/genome_metadata/dump.py</code> <pre><code>def filter_genome_meta(gmeta: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Returns a filtered metadata dict with only predefined keys.\n    Also converts expected numbers to integers (to follow the genome json schema).\n\n    Args:\n        gmeta (Dict[str, Any]): Nested metadata key values from the core metadata table.\n    \"\"\"\n    meta_list = {\n        \"species\": {\n            \"taxonomy_id\",\n            \"production_name\",\n            \"scientific_name\",\n            \"strain\",\n            \"display_name\",\n            \"division\",\n            \"alias\",\n            \"annotation_source\",\n        },\n        \"assembly\": {\"accession\", \"date\", \"name\", \"version\", \"provider_name\", \"provider_url\"},\n        \"genebuild\": {\"version\", \"method\", \"start_date\", \"method_display\"},\n        \"annotation\": {\"provider_name\", \"provider_url\"},\n        \"BRC4\": {\"organism_abbrev\", \"component\"},\n        \"added_seq\": {\"region_name\"},\n    }\n    is_integer = {\"species\": {\"taxonomy_id\"}, \"assembly\": {\"version\"}}\n\n    gmeta_out: Dict[str, Any] = {}\n    for key1, subkeys in meta_list.items():\n        if key1 not in gmeta:\n            continue\n        if subkeys:\n            gmeta_out[key1] = {}\n            for key2 in subkeys:\n                if key2 not in gmeta[key1]:\n                    continue\n                value = gmeta[key1][key2]\n                if len(value) == 1:\n                    value = value[0]\n                    if key2 in is_integer.get(key1, {}):\n                        value = int(value)\n                gmeta_out[key1][key2] = value\n        else:\n            value = gmeta[key1]\n            if len(value) == 1:\n                value = value[0]\n                if is_integer.get(key1):\n                    value = int(value)\n            gmeta_out[key1] = value\n\n    check_assembly_version(gmeta_out)\n\n    return gmeta_out\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_metadata/dump/#src.ensembl.io.genomio.genome_metadata.dump.get_genome_metadata","title":"<code>get_genome_metadata(session)</code>","text":"<p>Retrieve a select list of metadata from the core database.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session for the current core.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A nested dict.</p> Source code in <code>src/ensembl/io/genomio/genome_metadata/dump.py</code> <pre><code>def get_genome_metadata(session: Session) -&gt; Dict[str, Any]:\n    \"\"\"Retrieve a select list of metadata from the core database.\n\n    Args:\n        session: Session for the current core.\n\n    Returns:\n        A nested dict.\n    \"\"\"\n    gmeta: Dict[str, Any] = {}\n\n    gmeta_st = select(Meta)\n    for row in session.execute(gmeta_st).unique().all():\n        dat = row[0]\n        meta_key = dat.meta_key\n        meta_value = dat.meta_value\n\n        if \".\" in meta_key:\n            (high_key, low_key) = meta_key.split(\".\")\n            if high_key in gmeta:\n                if low_key in gmeta[high_key]:\n                    gmeta[high_key][low_key].append(meta_value)\n                else:\n                    gmeta[high_key][low_key] = [meta_value]\n            else:\n                gmeta[high_key] = {}\n                gmeta[high_key][low_key] = [meta_value]\n        else:\n            if meta_key in gmeta:\n                gmeta[meta_key].append(meta_value)\n            else:\n                gmeta[meta_key] = [meta_value]\n\n    return gmeta\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_metadata/dump/#src.ensembl.io.genomio.genome_metadata.dump.main","title":"<code>main()</code>","text":"<p>Main script entry-point.</p> Source code in <code>src/ensembl/io/genomio/genome_metadata/dump.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main script entry-point.\"\"\"\n    parser = ArgumentParser(\n        description=\"Fetch the genome metadata from a core database and print it in JSON format.\"\n    )\n    parser.add_server_arguments(include_database=True)\n    args = parser.parse_args()\n\n    dbc = DBConnection(args.url)\n\n    with dbc.session_scope() as session:\n        genome_meta = get_genome_metadata(session)\n        genome_meta = filter_genome_meta(genome_meta)\n\n    print(json.dumps(genome_meta, indent=2, sort_keys=True))\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_metadata/extend/","title":"extend","text":"<p>Add more metadata to the genome metadata file, including added seq_regions (e.g. MT chromosome).</p>"},{"location":"reference/ensembl/io/genomio/genome_metadata/extend/#src.ensembl.io.genomio.genome_metadata.extend.MissingDataError","title":"<code>MissingDataError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Used if some data is missing from the report file.</p> Source code in <code>src/ensembl/io/genomio/genome_metadata/extend.py</code> <pre><code>class MissingDataError(Exception):\n    \"\"\"Used if some data is missing from the report file.\"\"\"\n\n    def __init__(self, report_path: PathLike, accession: str, msg: str):\n        report_msg = f\"Can't get data for {accession} in report {report_path}\"\n        if msg:\n            report_msg = f\"{report_msg}: {msg}\"\n        self.msg = report_msg\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_metadata/extend/#src.ensembl.io.genomio.genome_metadata.extend.amend_genomic_metadata","title":"<code>amend_genomic_metadata(genome_infile, genome_outfile, report_file=None, genbank_infile=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>genome_infile</code> <code>PathLike</code> <p>Genome data following the schemas/genome_schema.json.</p> required <code>genome_outfile</code> <code>PathLike</code> <p>Amended genome data file.</p> required <code>report_file</code> <code>Optional[PathLike]</code> <p>INSDC/RefSeq sequences report file.</p> <code>None</code> <code>genbank_infile</code> <code>Optional[PathLike]</code> <p>INSDC/RefSeq GBFF file.</p> <code>None</code> Source code in <code>src/ensembl/io/genomio/genome_metadata/extend.py</code> <pre><code>def amend_genomic_metadata(\n    genome_infile: PathLike,\n    genome_outfile: PathLike,\n    report_file: Optional[PathLike] = None,\n    genbank_infile: Optional[PathLike] = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        genome_infile: Genome data following the schemas/genome_schema.json.\n        genome_outfile: Amended genome data file.\n        report_file: INSDC/RefSeq sequences report file.\n        genbank_infile: INSDC/RefSeq GBFF file.\n    \"\"\"\n    genome_metadata = get_json(genome_infile)\n\n    # Get additional sequences in the assembly but not in the data\n    if report_file:\n        gbff_path = Path(genbank_infile) if genbank_infile else None\n        additions = get_additions(Path(report_file), gbff_path)\n        if additions:\n            genome_metadata[\"added_seq\"] = {\"region_name\": additions}\n\n    # Print out the file\n    genome_outfile = Path(genome_outfile)\n    print_json(genome_outfile, genome_metadata)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_metadata/extend/#src.ensembl.io.genomio.genome_metadata.extend.get_additions","title":"<code>get_additions(report_path, gbff_path)</code>","text":"<p>Returns all <code>seq_regions</code> that are mentioned in the report but that are not in the data.</p> <p>Parameters:</p> Name Type Description Default <code>report_path</code> <code>Path</code> <p>Path to the report file.</p> required <code>gbff_path</code> <code>Optional[Path]</code> <p>Path to the GBFF file.</p> required Source code in <code>src/ensembl/io/genomio/genome_metadata/extend.py</code> <pre><code>def get_additions(report_path: Path, gbff_path: Optional[Path]) -&gt; List[str]:\n    \"\"\"Returns all `seq_regions` that are mentioned in the report but that are not in the data.\n\n    Args:\n        report_path: Path to the report file.\n        gbff_path: Path to the GBFF file.\n    \"\"\"\n    gbff_regions = set(get_gbff_regions(gbff_path))\n    report_regions = get_report_regions_names(report_path)\n\n    additions = []\n    for rep_seq in report_regions:\n        (rs_seq, gb_seq) = rep_seq\n        if rs_seq not in gbff_regions and gb_seq not in gbff_regions:\n            if rs_seq:\n                additions.append(rs_seq)\n            else:\n                additions.append(gb_seq)\n    additions = sorted(additions)\n    return additions\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_metadata/extend/#src.ensembl.io.genomio.genome_metadata.extend.get_gbff_regions","title":"<code>get_gbff_regions(gbff_path)</code>","text":"<p>Returns the <code>seq_region</code> data from the GBFF file.</p> <p>Parameters:</p> Name Type Description Default <code>gbff_path</code> <code>Optional[Path]</code> <p>Gbff file path to use.</p> required Source code in <code>src/ensembl/io/genomio/genome_metadata/extend.py</code> <pre><code>def get_gbff_regions(gbff_path: Optional[Path]) -&gt; List[str]:\n    \"\"\"Returns the `seq_region` data from the GBFF file.\n\n    Args:\n        gbff_path: Gbff file path to use.\n    \"\"\"\n    if not gbff_path:\n        return []\n\n    seq_regions = []\n    with open_gz_file(gbff_path) as gbff_file:\n        for record in SeqIO.parse(gbff_file, \"genbank\"):\n            record_id = re.sub(_VERSION_END, \"\", record.id)\n            seq_regions.append(record_id)\n    return seq_regions\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_metadata/extend/#src.ensembl.io.genomio.genome_metadata.extend.get_report_regions_names","title":"<code>get_report_regions_names(report_path)</code>","text":"<p>Returns a list of <code>seq_region</code> names from the report file.</p> <p>Parameters:</p> Name Type Description Default <code>report_path</code> <code>Path</code> <p>Path to the seq_regions report from INSDC/RefSeq.</p> required Source code in <code>src/ensembl/io/genomio/genome_metadata/extend.py</code> <pre><code>def get_report_regions_names(report_path: Path) -&gt; List[Tuple[str, str]]:\n    \"\"\"Returns a list of `seq_region` names from the report file.\n\n    Args:\n        report_path: Path to the seq_regions report from INSDC/RefSeq.\n    \"\"\"\n    # Get the report in a CSV format, easier to manipulate\n    report_csv, _ = _report_to_csv(report_path)\n\n    # Feed the csv string to the CSV reader\n    reader = csv.DictReader(report_csv.splitlines(), delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n\n    # Create the seq_regions\n    seq_regions = []\n    for row in reader:\n        refseq_name = row[\"RefSeq-Accn\"]\n        genbank_name = row[\"GenBank-Accn\"]\n\n        if refseq_name == \"na\":\n            refseq_name = \"\"\n        if genbank_name == \"na\":\n            genbank_name = \"\"\n        refseq_name = re.sub(_VERSION_END, \"\", refseq_name)\n        genbank_name = re.sub(_VERSION_END, \"\", genbank_name)\n        seq_regions.append((genbank_name, refseq_name))\n    return seq_regions\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_metadata/extend/#src.ensembl.io.genomio.genome_metadata.extend.main","title":"<code>main()</code>","text":"<p>Module's entry-point.</p> Source code in <code>src/ensembl/io/genomio/genome_metadata/extend.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Module's entry-point.\"\"\"\n    parser = ArgumentParser(\n        description=\"Update genome metadata file to include additional sequence regions (e.g. MT chromosome).\"\n    )\n    parser.add_argument_src_path(\n        \"--genome_infile\", required=True, help=\"Input genome file (following the schemas/genome_schema.json)\"\n    )\n    parser.add_argument_dst_path(\n        \"--genome_outfile\", required=True, help=\"Path to the new amended genome metadata file\"\n    )\n    parser.add_argument_src_path(\"--report_file\", help=\"INSDC/RefSeq sequences report file\")\n    parser.add_argument_src_path(\"--genbank_infile\", help=\"INSDC/RefSeq GBFF file\")\n    args = parser.parse_args()\n\n    amend_genomic_metadata(**vars(args))\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_metadata/prepare/","title":"prepare","text":"<p>Expand the genome_metadata with more details for: the provider, assembly and gene build version, and the taxonomy.</p>"},{"location":"reference/ensembl/io/genomio/genome_metadata/prepare/#src.ensembl.io.genomio.genome_metadata.prepare.MetadataError","title":"<code>MetadataError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>When a metadata value is not expected.</p> Source code in <code>src/ensembl/io/genomio/genome_metadata/prepare.py</code> <pre><code>class MetadataError(Exception):\n    \"\"\"When a metadata value is not expected.\"\"\"\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_metadata/prepare/#src.ensembl.io.genomio.genome_metadata.prepare.MissingNodeError","title":"<code>MissingNodeError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>When a taxon XML node cannot be found.</p> Source code in <code>src/ensembl/io/genomio/genome_metadata/prepare.py</code> <pre><code>class MissingNodeError(Exception):\n    \"\"\"When a taxon XML node cannot be found.\"\"\"\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_metadata/prepare/#src.ensembl.io.genomio.genome_metadata.prepare.add_assembly_version","title":"<code>add_assembly_version(genome_data)</code>","text":"<p>Adds version number to the genome's assembly if one is not present already.</p> <p>Parameters:</p> Name Type Description Default <code>genome_data</code> <code>Dict</code> <p>Genome information of assembly, accession and annotation.</p> required Source code in <code>src/ensembl/io/genomio/genome_metadata/prepare.py</code> <pre><code>def add_assembly_version(genome_data: Dict) -&gt; None:\n    \"\"\"Adds version number to the genome's assembly if one is not present already.\n\n    Args:\n        genome_data: Genome information of assembly, accession and annotation.\n\n    \"\"\"\n    assembly = genome_data[\"assembly\"]\n    if not \"version\" in assembly:\n        accession = assembly[\"accession\"]\n        values = accession.split(\".\")\n        if (len(values) == 2) and values[1]:\n            assembly[\"version\"] = int(values[1])\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_metadata/prepare/#src.ensembl.io.genomio.genome_metadata.prepare.add_genebuild_metadata","title":"<code>add_genebuild_metadata(genome_data)</code>","text":"<p>Adds missing genebuild metadata.</p> <p>The default convention is to use the current date as <code>version</code> and <code>start_date</code>.</p> <p>Parameters:</p> Name Type Description Default <code>genome_data</code> <code>Dict</code> <p>Genome information of assembly, accession and annotation.</p> required Source code in <code>src/ensembl/io/genomio/genome_metadata/prepare.py</code> <pre><code>def add_genebuild_metadata(genome_data: Dict) -&gt; None:\n    \"\"\"Adds missing genebuild metadata.\n\n    The default convention is to use the current date as ``version`` and ``start_date``.\n\n    Args:\n        genome_data: Genome information of assembly, accession and annotation.\n\n    \"\"\"\n    genebuild = genome_data[\"genebuild\"]\n    current_date = datetime.date.today().isoformat()\n    if not \"version\" in genebuild:\n        genebuild[\"version\"] = current_date\n    if not \"start_date\" in genebuild:\n        genebuild[\"start_date\"] = current_date\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_metadata/prepare/#src.ensembl.io.genomio.genome_metadata.prepare.add_provider","title":"<code>add_provider(genome_data, gff3_file=None)</code>","text":"<p>Adds provider metadata for assembly and gene models in <code>genome_data</code>.</p> <p>Assembly provider metadata will only be added if it is missing, i.e. neither <code>provider_name</code> or <code>provider_url</code> are present. The gene model metadata will only be added if <code>gff3_file</code> is provided.</p> <p>Parameters:</p> Name Type Description Default <code>genome_data</code> <code>Dict</code> <p>Genome information of assembly, accession and annotation.</p> required <code>gff3_file</code> <code>Optional[PathLike]</code> <p>Path to GFF3 file to use as annotation source for this genome.</p> <code>None</code> Source code in <code>src/ensembl/io/genomio/genome_metadata/prepare.py</code> <pre><code>def add_provider(genome_data: Dict, gff3_file: Optional[PathLike] = None) -&gt; None:\n    \"\"\"Adds provider metadata for assembly and gene models in `genome_data`.\n\n    Assembly provider metadata will only be added if it is missing, i.e. neither ``provider_name`` or\n    ``provider_url`` are present. The gene model metadata will only be added if `gff3_file` is provided.\n\n    Args:\n        genome_data: Genome information of assembly, accession and annotation.\n        gff3_file: Path to GFF3 file to use as annotation source for this genome.\n\n    \"\"\"\n    # Get accession provider\n    accession = genome_data[\"assembly\"][\"accession\"]\n    if accession.startswith(\"GCF\"):\n        provider = PROVIDER_DATA[\"RefSeq\"]\n    elif accession.startswith(\"GCA\"):\n        provider = PROVIDER_DATA[\"GenBank\"]\n    else:\n        raise MetadataError(f\"Accession doesn't look like an INSDC or RefSeq accession: {accession}\")\n\n    # Add assembly provider (if missing)\n    assembly = genome_data[\"assembly\"]\n    if (not \"provider_name\" in assembly) and (not \"provider_url\" in assembly):\n        assembly[\"provider_name\"] = provider[\"assembly\"][\"provider_name\"]\n        assembly[\"provider_url\"] = provider[\"assembly\"][\"provider_url\"]\n\n    # Add annotation provider if there are gene models\n    if gff3_file:\n        annotation = {}\n        if \"annotation\" in genome_data:\n            annotation = genome_data[\"annotation\"]\n        if (\"provider_name\" not in annotation) and (\"provider_url\" not in annotation):\n            annotation[\"provider_name\"] = provider[\"annotation\"][\"provider_name\"]\n            annotation[\"provider_url\"] = provider[\"annotation\"][\"provider_url\"]\n        genome_data[\"annotation\"] = annotation\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_metadata/prepare/#src.ensembl.io.genomio.genome_metadata.prepare.add_species_metadata","title":"<code>add_species_metadata(genome_data, base_api_url=DEFAULT_API_URL)</code>","text":"<p>Adds missing species metadata based on the genome's accession.</p> <p>The <code>taxonomy_id</code>, <code>strain</code> and <code>scientific_name</code> will be fetched from the taxonomy information linked to the given accession.</p> <p>Parameters:</p> Name Type Description Default <code>genome_data</code> <code>Dict</code> <p>Genome information of assembly, accession and annotation.</p> required <code>base_api_url</code> <code>str</code> <p>Base API URL to fetch the taxonomy data from.</p> <code>DEFAULT_API_URL</code> Source code in <code>src/ensembl/io/genomio/genome_metadata/prepare.py</code> <pre><code>def add_species_metadata(genome_data: Dict, base_api_url: str = DEFAULT_API_URL) -&gt; None:\n    \"\"\"Adds missing species metadata based on the genome's accession.\n\n    The ``taxonomy_id``, ``strain`` and ``scientific_name`` will be fetched from the taxonomy information\n    linked to the given accession.\n\n    Args:\n        genome_data: Genome information of assembly, accession and annotation.\n        base_api_url: Base API URL to fetch the taxonomy data from.\n\n    \"\"\"\n    species = genome_data[\"species\"]\n    if not \"taxonomy_id\" in species:\n        accession = genome_data[\"assembly\"][\"accession\"]\n        taxonomy = get_taxonomy_from_accession(accession, base_api_url)\n        species[\"taxonomy_id\"] = taxonomy[\"taxon_id\"]\n        if (not \"strain\" in species) and (\"strain\" in taxonomy):\n            species[\"strain\"] = taxonomy[\"strain\"]\n        if not \"scientific_name\" in species:\n            species[\"scientific_name\"] = taxonomy[\"scientific_name\"]\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_metadata/prepare/#src.ensembl.io.genomio.genome_metadata.prepare.get_taxonomy_from_accession","title":"<code>get_taxonomy_from_accession(accession, base_api_url=DEFAULT_API_URL)</code>","text":"<p>Returns the taxonomy metadata associated to the given accession.</p> <p>Parameters:</p> Name Type Description Default <code>accession</code> <code>str</code> <p>INSDC accession ID.</p> required <code>base_api_url</code> <code>str</code> <p>Base API URL to fetch the taxonomy data from.</p> <code>DEFAULT_API_URL</code> <p>Returns:</p> Type Description <code>Dict</code> <p>Dictionary with key-value pairs for <code>taxon_id</code> and <code>scientific_name</code>. <code>strain</code> will be added</p> <code>Dict</code> <p>only if present in the fetched taxonomy data.</p> <p>Raises:</p> Type Description <code>MissinDataException</code> <p>If <code>TAXON_ID</code> or <code>SCIENTIFIC_NAME</code> are missing in the taxonomy data fetched.</p> Source code in <code>src/ensembl/io/genomio/genome_metadata/prepare.py</code> <pre><code>def get_taxonomy_from_accession(accession: str, base_api_url: str = DEFAULT_API_URL) -&gt; Dict:\n    \"\"\"Returns the taxonomy metadata associated to the given accession.\n\n    Args:\n        accession: INSDC accession ID.\n        base_api_url: Base API URL to fetch the taxonomy data from.\n\n    Returns:\n        Dictionary with key-value pairs for ``taxon_id`` and ``scientific_name``. ``strain`` will be added\n        only if present in the fetched taxonomy data.\n\n    Raises:\n        MissinDataException: If ``TAXON_ID`` or ``SCIENTIFIC_NAME`` are missing in the taxonomy data fetched.\n\n    \"\"\"\n    # Use the GenBank accession without version\n    gb_accession = accession.replace(\"GCF\", \"GCA\").split(\".\")[0]\n    response = requests.get(f\"{base_api_url}/{gb_accession}\", timeout=60)\n    entry = ElementTree.fromstring(response.text)\n\n    taxon_node = entry.find(\".//TAXON\")\n    if taxon_node is None:\n        raise MissingNodeError(\"Can't find the TAXON node\")\n\n    # Fetch taxon ID, scientific_name and strain\n    taxon_id = _get_node_text(taxon_node, \"TAXON_ID\")\n    scientific_name = _get_node_text(taxon_node, \"SCIENTIFIC_NAME\")\n    strain = _get_node_text(taxon_node, \"STRAIN\", optional=True)\n\n    if taxon_id and scientific_name:\n        taxonomy = {\n            \"taxon_id\": int(taxon_id),\n            \"scientific_name\": scientific_name,\n        }\n    if strain:\n        taxonomy[\"strain\"] = strain\n    return taxonomy\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_metadata/prepare/#src.ensembl.io.genomio.genome_metadata.prepare.main","title":"<code>main()</code>","text":"<p>Module's entry-point.</p> Source code in <code>src/ensembl/io/genomio/genome_metadata/prepare.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Module's entry-point.\"\"\"\n    parser = ArgumentParser(\n        description=(\n            \"Add information about provider, taxonomy and assembly and gene build version to the genome \"\n            \"metadata file.\"\n        )\n    )\n    parser.add_argument_src_path(\"--input_file\", required=True, help=\"Genome metadata JSON file\")\n    parser.add_argument_dst_path(\n        \"--output_file\", required=True, help=\"Output path for the new genome metadata file\"\n    )\n    parser.add_argument_src_path(\"--gff3_file\", help=\"GFF3 file to use as annotation source\")\n    parser.add_argument(\n        \"--base_api_url\", default=DEFAULT_API_URL, help=\"API URL to fetch the taxonomy data from\"\n    )\n    args = parser.parse_args()\n\n    prepare_genome_metadata(**vars(args))\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_metadata/prepare/#src.ensembl.io.genomio.genome_metadata.prepare.prepare_genome_metadata","title":"<code>prepare_genome_metadata(input_file, output_file, gff3_file=None, base_api_url=DEFAULT_API_URL)</code>","text":"<p>Updates the genome metadata JSON file with additional information.</p> <p>In particular, more information is added about the provider, the assembly and its gene build version, and the taxonomy.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>PathLike</code> <p>Path to JSON file with genome metadata.</p> required <code>output_file</code> <code>PathLike</code> <p>Output directory where to generate the final <code>genome.json</code> file.</p> required <code>gff3_file</code> <code>Optional[PathLike]</code> <p>Path to GFF3 file to use as annotation source for this genome.</p> <code>None</code> <code>base_api_url</code> <code>str</code> <p>Base API URL to fetch the taxonomy data from.</p> <code>DEFAULT_API_URL</code> Source code in <code>src/ensembl/io/genomio/genome_metadata/prepare.py</code> <pre><code>def prepare_genome_metadata(\n    input_file: PathLike,\n    output_file: PathLike,\n    gff3_file: Optional[PathLike] = None,\n    base_api_url: str = DEFAULT_API_URL,\n) -&gt; None:\n    \"\"\"Updates the genome metadata JSON file with additional information.\n\n    In particular, more information is added about the provider, the assembly and its gene build version,\n    and the taxonomy.\n\n    Args:\n        input_file: Path to JSON file with genome metadata.\n        output_file: Output directory where to generate the final `genome.json` file.\n        gff3_file: Path to GFF3 file to use as annotation source for this genome.\n        base_api_url: Base API URL to fetch the taxonomy data from.\n\n    \"\"\"\n    genome_data = get_json(input_file)\n    # Amend any missing metadata\n    add_provider(genome_data, gff3_file)\n    add_assembly_version(genome_data)\n    add_genebuild_metadata(genome_data)\n    add_species_metadata(genome_data, base_api_url)\n    # Dump updated genome metadata\n    print_json(output_file, genome_data)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_stats/","title":"genome_stats","text":"<p>Genome statistics handling module.</p>"},{"location":"reference/ensembl/io/genomio/genome_stats/compare/","title":"compare","text":"<p>Compare stats in a JSON from NCBI dataset and a JSON from our core db.</p> <p>Returns the JSON from our core db including a section with comparisons.</p>"},{"location":"reference/ensembl/io/genomio/genome_stats/compare/#src.ensembl.io.genomio.genome_stats.compare.compare_annotation","title":"<code>compare_annotation(ncbi, core)</code>","text":"<p>Compare NCBI vs Core annotation stats (biotype counts).</p> <p>Parameters:</p> Name Type Description Default <code>ncbi</code> <code>Dict</code> <p>Dict of biotype counts from NCBI.</p> required <code>core</code> <code>Dict</code> <p>Dict of biotype counts from the core.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>Each count from NCBI, from Core, and their diff.</p> Source code in <code>src/ensembl/io/genomio/genome_stats/compare.py</code> <pre><code>def compare_annotation(ncbi: Dict, core: Dict) -&gt; Dict:\n    \"\"\"Compare NCBI vs Core annotation stats (biotype counts).\n\n    Args:\n        ncbi: Dict of biotype counts from NCBI.\n        core: Dict of biotype counts from the core.\n\n    Returns:\n        Dict: Each count from NCBI, from Core, and their diff.\n    \"\"\"\n    # Prepare counts to be comparable\n    core_biotypes = core.get(\"genes\", {}).get(\"biotypes\", {})\n\n    # We want a value for:\n    # protein_coding = same\n    # pseudogene = all pseudogene biotypes\n    # total = same\n    # other = number of misc_RNA\n\n    ncbi_counts = {\n        \"protein_coding\": ncbi.get(\"protein_coding\", 0),\n        \"pseudogene\": ncbi.get(\"pseudogene\", 0),\n        \"total_genes\": ncbi.get(\"total\", 0),\n        \"other\": ncbi.get(\"other\", 0),\n    }\n\n    # Add all pseudogenes\n    num_pseudogenes = 0\n    for name, num in core_biotypes.items():\n        if re.match(\".*pseudogen.*\", name):\n            num_pseudogenes += num\n\n    # Others? misc_mRNA (or anything similar?)\n    num_others = core_biotypes.get(\"misc_RNA\", 0)\n\n    core_counts = {\n        \"protein_coding\": core_biotypes.get(\"protein_coding\", 0),\n        \"pseudogene\": num_pseudogenes,\n        \"total_genes\": core.get(\"genes\", {}).get(\"total\", 0),\n        \"other\": num_others,\n    }\n\n    return _diff_dicts(ncbi_counts, core_counts)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_stats/compare/#src.ensembl.io.genomio.genome_stats.compare.compare_assembly","title":"<code>compare_assembly(ncbi, core)</code>","text":"<p>Returns a compilation of count comparisons. Each comparison is a dict with the value from NCBI, from Core, and their diff.</p> <p>Parameters:</p> Name Type Description Default <code>ncbi</code> <code>Dict[str, Any]</code> <p>Dict of stats from NCBI.</p> required <code>core</code> <code>Dict[str, Any]</code> <p>Dict of assembly stats from the core.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict[str, Any]</code> <p>Each count from NCBI, from Core, and their diff.</p> Source code in <code>src/ensembl/io/genomio/genome_stats/compare.py</code> <pre><code>def compare_assembly(ncbi: Dict[str, Any], core: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Returns a compilation of count comparisons.\n    Each comparison is a dict with the value from NCBI, from Core, and their diff.\n\n    Args:\n        ncbi: Dict of stats from NCBI.\n        core: Dict of assembly stats from the core.\n\n    Returns:\n        Dict: Each count from NCBI, from Core, and their diff.\n    \"\"\"\n\n    # Prepare counts to be comparable to the NCBI stats\n    ncbi_main = ncbi.get(\"assembly_stats\", {})\n    ncbi_info = ncbi.get(\"assembly_info\", {})\n    ncbi_organella = ncbi.get(\"organelle_info\", [])\n\n    # First count the organella\n    core_num_organella = 0\n    core_num_chrs = 0\n    for loc, loc_count in core[\"locations\"].items():\n        if loc == \"nuclear_chromosome\":\n            core_num_chrs += loc_count\n        else:\n            core_num_organella += loc_count\n\n    # Our core stats count Organella chromosomes, sanity check here\n    core_chr = core[\"coord_system\"].get(\"chromosome\", 0)\n    core_adjusted_chrs = 0\n    if core_chr:\n        core_adjusted_chrs = core_chr - core_num_organella\n\n    # Number of scaffolds from our core\n    core_num_scaffolds = core[\"coord_system\"].get(\"scaffold\", 0)\n\n    # NCBI includes the chromosomes in its stats\n    core_adjusted_scaffolds = core_num_scaffolds + core_num_chrs\n\n    # Compile the counts\n    ncbi_counts = {\n        \"num_organella\": len(ncbi_organella),\n        \"num_chromosomes\": ncbi_main.get(\"total_number_of_chromosomes\", 0),\n        \"num_scaffolds\": ncbi_main.get(\"number_of_scaffolds\", 0),\n        \"num_contigs\": ncbi_main.get(\"number_of_contigs\", 0),\n    }\n    core_counts = {\n        \"num_organella\": core_num_organella,\n        \"num_chromosomes\": core_adjusted_chrs,\n        \"num_scaffolds\": core_adjusted_scaffolds,\n        \"num_contigs\": core[\"coord_system\"].get(\"contig\", 0),\n    }\n\n    # Only compare contigs if there are any in the core db\n    if ncbi_info.get(\"assembly_level\") != \"Contig\":\n        del ncbi_counts[\"num_contigs\"]\n        del core_counts[\"num_contigs\"]\n\n    return _diff_dicts(ncbi_counts, core_counts)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_stats/compare/#src.ensembl.io.genomio.genome_stats.compare.compare_stats","title":"<code>compare_stats(ncbi, core)</code>","text":"<p>Compare stats from NCBI and our core.</p> <p>Parameters:</p> Name Type Description Default <code>ncbi</code> <code>Dict</code> <p>Dict of stats from NCBI.</p> required <code>core</code> <code>Dict</code> <p>Dict of stats from the core.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict[str, Any]</code> <p>Each count from NCBI, from Core, and their diff.</p> Source code in <code>src/ensembl/io/genomio/genome_stats/compare.py</code> <pre><code>def compare_stats(ncbi: Dict, core: Dict) -&gt; Dict[str, Any]:\n    \"\"\"Compare stats from NCBI and our core.\n\n    Args:\n        ncbi: Dict of stats from NCBI.\n        core: Dict of stats from the core.\n\n    Returns:\n        Dict: Each count from NCBI, from Core, and their diff.\n    \"\"\"\n\n    ncbi_annotation_stats = ncbi.get(\"annotation_info\", {}).get(\"stats\", {}).get(\"gene_counts\", {})\n    core_assembly_stats = core.get(\"assembly_stats\", {})\n    core_annotation_stats = core.get(\"annotation_stats\", {})\n\n    comp: Dict[str, Any] = {\n        \"assembly_diff\": compare_assembly(ncbi, core_assembly_stats),\n    }\n    if core_annotation_stats and ncbi_annotation_stats:\n        comp[\"annotation_diff\"] = compare_annotation(ncbi_annotation_stats, core_annotation_stats)\n\n    return comp\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_stats/compare/#src.ensembl.io.genomio.genome_stats.compare.main","title":"<code>main()</code>","text":"<p>Main script entry-point.</p> Source code in <code>src/ensembl/io/genomio/genome_stats/compare.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main script entry-point.\"\"\"\n    parser = ArgumentParser(\n        description=\"Compare genome statistics between an NCBI dataset and a core database.\"\n    )\n    parser.add_argument_src_path(\"--ncbi_stats\", required=True, help=\"NCBI dataset JSON file\")\n    parser.add_argument_src_path(\"--core_stats\", required=True, help=\"Core database JSON file\")\n    args = parser.parse_args()\n\n    try:\n        ncbi_stats = get_json(args.ncbi_stats)[\"reports\"][0]\n    except KeyError:\n        ncbi_stats = {}\n    core_stats = get_json(args.core_stats)\n    all_stats = compare_stats(ncbi_stats, core_stats)\n\n    print(json.dumps(all_stats, indent=2, sort_keys=True))\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_stats/dump/","title":"dump","text":"<p>Generates a JSON file representing various stats for the assembly and annotation from a core db.</p>"},{"location":"reference/ensembl/io/genomio/genome_stats/dump/#src.ensembl.io.genomio.genome_stats.dump.StatsGenerator","title":"<code>StatsGenerator</code>","text":"<p>Interface to extract stats from a core database.</p> Source code in <code>src/ensembl/io/genomio/genome_stats/dump.py</code> <pre><code>class StatsGenerator:\n    \"\"\"Interface to extract stats from a core database.\"\"\"\n\n    def __init__(self, session: Session) -&gt; None:\n        self.session = session\n\n    def get_assembly_stats(self) -&gt; Dict[str, Any]:\n        \"\"\"Returns a dict of stats about the assembly.\"\"\"\n        stats = {\n            \"coord_system\": self.get_attrib_counts(\"coord_system_tag\"),\n            \"locations\": self.get_attrib_counts(\"sequence_location\"),\n            \"codon_table\": self.get_attrib_counts(\"codon_table\"),\n        }\n\n        # Special: rename supercontigs to scaffolds for homogeneity\n        stats = self._fix_scaffolds(stats)\n        return stats\n\n    def _fix_scaffolds(self, stats: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Rename supercontigs to scaffolds in a stats dict and return it.\"\"\"\n        coords = stats.get(\"coord_system\", {})\n        if \"supercontig\" in coords:\n            if \"scaffold\" not in coords:\n                coords[\"scaffold\"] = coords[\"supercontig\"]\n                del coords[\"supercontig\"]\n        return stats\n\n    def get_attrib_counts(self, code: str) -&gt; Dict[str, Any]:\n        \"\"\"Returns a dict of count for each value counted with the attrib_type code provided.\n\n        Args:\n            code: Ensembl database attrib_type code.\n        \"\"\"\n        session = self.session\n\n        seqs_st = (\n            select(SeqRegionAttrib.value, func.count(SeqRegionAttrib.value))\n            .join(AttribType)\n            .filter(AttribType.code == code)\n            .group_by(SeqRegionAttrib.value)\n        )\n\n        attribs = {}\n        for row in session.execute(seqs_st):\n            (attrib_name, count) = row\n            attribs[attrib_name] = count\n\n        return attribs\n\n    def get_annotation_stats(self) -&gt; Dict[str, Any]:\n        \"\"\"Returns a dict of stats about the coordinate systems (number of biotypes, etc.).\"\"\"\n\n        stats = {\n            \"genes\": self.get_feature_stats(Gene),\n            \"transcripts\": self.get_feature_stats(Transcript),\n        }\n\n        return stats\n\n    def get_biotypes(self, table) -&gt; Dict[str, int]:\n        \"\"\"Returns a dict of stats about the feature biotypes.\"\"\"\n        session = self.session\n\n        seqs_st = select(table.biotype, func.count()).group_by(table.biotype)\n\n        biotypes = {}\n        for row in session.execute(seqs_st):\n            (biotype, count) = row\n            biotypes[biotype] = count\n\n        return biotypes\n\n    def get_feature_stats(self, table) -&gt; Dict[str, int]:\n        \"\"\"Returns a dict of stats about a given feature.\"\"\"\n        session = self.session\n\n        totals_st = select(func.count(table.stable_id))\n        (total,) = session.execute(totals_st).one()\n        no_desc_st = select(func.count(table.stable_id)).filter(table.description is None)\n        (no_desc,) = session.execute(no_desc_st).one()\n        xref_desc_st = select(func.count(table.stable_id)).where(table.description.like(\"%[Source:%\"))\n        (xref_desc,) = session.execute(xref_desc_st).one()\n\n        left_over = total - no_desc - xref_desc\n\n        feat_stats = {\n            \"total\": total,\n            \"biotypes\": self.get_biotypes(table),\n            \"description\": {\n                \"empty\": no_desc,\n                \"source_xref\": xref_desc,\n                \"normal\": left_over,\n            },\n        }\n        return feat_stats\n\n    def get_stats(self) -&gt; Dict[str, Any]:\n        \"\"\"Returns a dict of stats about the assembly and annotation.\"\"\"\n        all_stats = {\n            \"assembly_stats\": self.get_assembly_stats(),\n            \"annotation_stats\": self.get_annotation_stats(),\n        }\n        return all_stats\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_stats/dump/#src.ensembl.io.genomio.genome_stats.dump.StatsGenerator.get_annotation_stats","title":"<code>get_annotation_stats()</code>","text":"<p>Returns a dict of stats about the coordinate systems (number of biotypes, etc.).</p> Source code in <code>src/ensembl/io/genomio/genome_stats/dump.py</code> <pre><code>def get_annotation_stats(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns a dict of stats about the coordinate systems (number of biotypes, etc.).\"\"\"\n\n    stats = {\n        \"genes\": self.get_feature_stats(Gene),\n        \"transcripts\": self.get_feature_stats(Transcript),\n    }\n\n    return stats\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_stats/dump/#src.ensembl.io.genomio.genome_stats.dump.StatsGenerator.get_assembly_stats","title":"<code>get_assembly_stats()</code>","text":"<p>Returns a dict of stats about the assembly.</p> Source code in <code>src/ensembl/io/genomio/genome_stats/dump.py</code> <pre><code>def get_assembly_stats(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns a dict of stats about the assembly.\"\"\"\n    stats = {\n        \"coord_system\": self.get_attrib_counts(\"coord_system_tag\"),\n        \"locations\": self.get_attrib_counts(\"sequence_location\"),\n        \"codon_table\": self.get_attrib_counts(\"codon_table\"),\n    }\n\n    # Special: rename supercontigs to scaffolds for homogeneity\n    stats = self._fix_scaffolds(stats)\n    return stats\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_stats/dump/#src.ensembl.io.genomio.genome_stats.dump.StatsGenerator.get_attrib_counts","title":"<code>get_attrib_counts(code)</code>","text":"<p>Returns a dict of count for each value counted with the attrib_type code provided.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>Ensembl database attrib_type code.</p> required Source code in <code>src/ensembl/io/genomio/genome_stats/dump.py</code> <pre><code>def get_attrib_counts(self, code: str) -&gt; Dict[str, Any]:\n    \"\"\"Returns a dict of count for each value counted with the attrib_type code provided.\n\n    Args:\n        code: Ensembl database attrib_type code.\n    \"\"\"\n    session = self.session\n\n    seqs_st = (\n        select(SeqRegionAttrib.value, func.count(SeqRegionAttrib.value))\n        .join(AttribType)\n        .filter(AttribType.code == code)\n        .group_by(SeqRegionAttrib.value)\n    )\n\n    attribs = {}\n    for row in session.execute(seqs_st):\n        (attrib_name, count) = row\n        attribs[attrib_name] = count\n\n    return attribs\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_stats/dump/#src.ensembl.io.genomio.genome_stats.dump.StatsGenerator.get_biotypes","title":"<code>get_biotypes(table)</code>","text":"<p>Returns a dict of stats about the feature biotypes.</p> Source code in <code>src/ensembl/io/genomio/genome_stats/dump.py</code> <pre><code>def get_biotypes(self, table) -&gt; Dict[str, int]:\n    \"\"\"Returns a dict of stats about the feature biotypes.\"\"\"\n    session = self.session\n\n    seqs_st = select(table.biotype, func.count()).group_by(table.biotype)\n\n    biotypes = {}\n    for row in session.execute(seqs_st):\n        (biotype, count) = row\n        biotypes[biotype] = count\n\n    return biotypes\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_stats/dump/#src.ensembl.io.genomio.genome_stats.dump.StatsGenerator.get_feature_stats","title":"<code>get_feature_stats(table)</code>","text":"<p>Returns a dict of stats about a given feature.</p> Source code in <code>src/ensembl/io/genomio/genome_stats/dump.py</code> <pre><code>def get_feature_stats(self, table) -&gt; Dict[str, int]:\n    \"\"\"Returns a dict of stats about a given feature.\"\"\"\n    session = self.session\n\n    totals_st = select(func.count(table.stable_id))\n    (total,) = session.execute(totals_st).one()\n    no_desc_st = select(func.count(table.stable_id)).filter(table.description is None)\n    (no_desc,) = session.execute(no_desc_st).one()\n    xref_desc_st = select(func.count(table.stable_id)).where(table.description.like(\"%[Source:%\"))\n    (xref_desc,) = session.execute(xref_desc_st).one()\n\n    left_over = total - no_desc - xref_desc\n\n    feat_stats = {\n        \"total\": total,\n        \"biotypes\": self.get_biotypes(table),\n        \"description\": {\n            \"empty\": no_desc,\n            \"source_xref\": xref_desc,\n            \"normal\": left_over,\n        },\n    }\n    return feat_stats\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_stats/dump/#src.ensembl.io.genomio.genome_stats.dump.StatsGenerator.get_stats","title":"<code>get_stats()</code>","text":"<p>Returns a dict of stats about the assembly and annotation.</p> Source code in <code>src/ensembl/io/genomio/genome_stats/dump.py</code> <pre><code>def get_stats(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns a dict of stats about the assembly and annotation.\"\"\"\n    all_stats = {\n        \"assembly_stats\": self.get_assembly_stats(),\n        \"annotation_stats\": self.get_annotation_stats(),\n    }\n    return all_stats\n</code></pre>"},{"location":"reference/ensembl/io/genomio/genome_stats/dump/#src.ensembl.io.genomio.genome_stats.dump.main","title":"<code>main()</code>","text":"<p>Main script entry-point.</p> Source code in <code>src/ensembl/io/genomio/genome_stats/dump.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main script entry-point.\"\"\"\n    parser = ArgumentParser(\n        description=\"Fetch all the sequence regions from a core database and print them in JSON format.\"\n    )\n    parser.add_server_arguments(include_database=True)\n    args = parser.parse_args()\n\n    dbc = DBConnection(args.url)\n\n    with dbc.session_scope() as session:\n        generator = StatsGenerator(session)\n        all_stats = generator.get_stats()\n\n    print(json.dumps(all_stats, indent=2, sort_keys=True))\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/","title":"gff3","text":"<p>GFF3 files processing module.</p>"},{"location":"reference/ensembl/io/genomio/gff3/extract_annotation/","title":"extract_annotation","text":"<p>Simple representation of gene features functional annotation extracted from a GFF3 file.</p>"},{"location":"reference/ensembl/io/genomio/gff3/extract_annotation/#src.ensembl.io.genomio.gff3.extract_annotation.AnnotationError","title":"<code>AnnotationError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>If anything wrong happens when recording annotations.</p> Source code in <code>src/ensembl/io/genomio/gff3/extract_annotation.py</code> <pre><code>class AnnotationError(Exception):\n    \"\"\"If anything wrong happens when recording annotations.\"\"\"\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/extract_annotation/#src.ensembl.io.genomio.gff3.extract_annotation.DuplicateIdError","title":"<code>DuplicateIdError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Trying to add a feature with an ID already in use.</p> Source code in <code>src/ensembl/io/genomio/gff3/extract_annotation.py</code> <pre><code>class DuplicateIdError(Exception):\n    \"\"\"Trying to add a feature with an ID already in use.\"\"\"\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/extract_annotation/#src.ensembl.io.genomio.gff3.extract_annotation.FunctionalAnnotations","title":"<code>FunctionalAnnotations</code>","text":"<p>List of annotations extracted from a GFF3 file.</p> Source code in <code>src/ensembl/io/genomio/gff3/extract_annotation.py</code> <pre><code>class FunctionalAnnotations:\n    \"\"\"List of annotations extracted from a GFF3 file.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self.annotations: List[Annotation] = []\n\n        # Annotated features\n        # Under each feature, each dict's key is a feature ID\n        self.features: Dict[str, Dict[str, Annotation]] = {\n            \"gene\": {},\n            \"transcript\": {},\n            \"translation\": {},\n            \"transposable_element\": {},\n        }\n        # Keep parent info: key is the feature ID, value is the parent ID\n        self.parents: Dict[str, Dict[str, str]] = {\n            \"gene\": {},\n            \"transcript\": {},\n        }\n\n    def add_parent(self, parent_type: str, parent_id: str, child_id: str) -&gt; None:\n        \"\"\"Record a parent-child IDs relationship for a given parent biotype.\"\"\"\n        if parent_type in self.parents:\n            self.parents[parent_type][child_id] = parent_id\n        else:\n            raise MissingParentError(f\"Unsupported parent type {parent_type}\")\n\n    def get_parent(self, parent_type: str, child_id: str) -&gt; str:\n        \"\"\"Returns the parent ID of a given child for a given parent biotype.\"\"\"\n        if parent_type in self.parents:\n            parent_id = self.parents[parent_type].get(child_id)\n            if parent_id is None:\n                raise MissingParentError(f\"Can't find {parent_type} parent for {child_id}\")\n            return parent_id\n        raise MissingParentError(f\"Unsupported parent type {parent_type}\")\n\n    def add_feature(self, feature: SeqFeature, feat_type: str, parent_id: Optional[str] = None) -&gt; None:\n        \"\"\"Add annotation for a feature of a given type. If a parent_id is provided, record the relatioship.\n\n        Args:\n            feature: The feature to create an annotation.\n            feat_type: Type of the feature to annotate.\n        \"\"\"\n        if feat_type not in self.features:\n            raise AnnotationError(f\"Unsupported feature type {feat_type}\")\n\n        if feature.id in self.features[feat_type]:\n            raise AnnotationError(f\"Feature {feat_type} ID {feature.id} already added\")\n        feature_object = self._generic_feature(feature, feat_type)\n        self.features[feat_type][feature.id] = feature_object\n\n        if parent_id:\n            if feat_type in _PARENTS:\n                parent_type = _PARENTS[feat_type]\n                self.add_parent(parent_type, parent_id, feature.id)\n            else:\n                raise AnnotationError(f\"No parent possible for {feat_type} {feature.id}\")\n\n    def get_features(self, feat_type: str) -&gt; Dict[str, Annotation]:\n        \"\"\"Get all feature annotations for the requested type.\"\"\"\n        if feat_type in self.features:\n            return self.features[feat_type]\n        raise AnnotationError(f\"No such feature type {feat_type}\")\n\n    def _generic_feature(self, feature: SeqFeature, feat_type: str) -&gt; Dict[str, Any]:\n        \"\"\"Create a feature object following the specifications.\n\n        Args:\n            feature: The SeqFeature to add to the list.\n            feat_type: Feature type of the feature to store (e.g. gene, transcript, translation).\n\n        \"\"\"\n\n        feature_object: Annotation = {\"object_type\": feat_type, \"id\": feature.id}\n\n        # Description?\n        if \"product\" in feature.qualifiers:\n            description = feature.qualifiers[\"product\"][0]\n            if self.product_is_informative(description):\n                feature_object[\"description\"] = description\n\n        if \"Name\" in feature.qualifiers and \"description\" not in feature_object:\n            feature_object[\"description\"] = feature.qualifiers[\"Name\"][0]\n\n        # Don't keep useless description\n        if (\"description\" in feature_object) and not self.product_is_informative(\n            feature_object[\"description\"], feature.id\n        ):\n            del feature_object[\"description\"]\n\n        # Synonyms?\n        if \"Name\" in feature.qualifiers:\n            feat_name = feature.qualifiers[\"Name\"][0]\n            if feat_name != feature.id:\n                feature_object[\"synonyms\"] = {\"synonym\": feat_name, \"default\": True}\n\n        # is_pseudogene?\n        if feature.type.startswith(\"pseudogen\"):\n            feature_object[\"is_pseudogene\"] = True\n\n        return feature_object\n\n    def _transfer_descriptions(self) -&gt; None:\n        \"\"\"Transfers the feature descriptions in 2 steps:\n        - from translations to transcripts (if the transcript description is empty)\n        - from transcripts to genes (same case)\n\n        \"\"\"\n        self._transfer_description_up(\"translation\")\n        self._transfer_description_up(\"transcript\")\n\n    def _transfer_description_up(self, child_feature: str) -&gt; None:\n        \"\"\"Transfer descriptions from all feature of a given type, up to their parent.\n\n        Args:\n            child_feature: Either \"translation\" (transfer to transcript) or \"transcript\" (to gene).\n\n        \"\"\"\n        children_features = self.get_features(child_feature)\n        parent_type = _PARENTS[child_feature]\n        parent_features = self.get_features(parent_type)\n\n        # Transfer description from children to their parent\n        for child_id, child in children_features.items():\n            child_description = child.get(\"description\")\n            if child_description is not None:\n                # Check parent\n                parent_id = self.get_parent(parent_type, child_id)\n                parent = parent_features[parent_id]\n                parent_description = parent.get(\"description\")\n                if parent_description is None:\n                    parent[\"description\"] = child_description\n\n    @staticmethod\n    def product_is_informative(product: str, feat_id: Optional[str] = None) -&gt; bool:\n        \"\"\"Returns True if the product name contains informative words, False otherwise.\n\n        It is considered uninformative when the description contains words such as \"hypothetical\" or\n        or \"putative\". If a feature ID is provided, consider it uninformative as well (we do not want\n        descriptions to be just the ID).\n\n        Args:\n            product: A product name.\n            feat_id: Feature ID (optional).\n\n        \"\"\"\n        non_informative_words = [\n            \"hypothetical\",\n            \"putative\",\n            \"uncharacterized\",\n            \"unspecified\",\n            r\"(of )?unknown function\",\n            \"conserved\",\n            \"predicted\",\n            \"fragment\",\n            \"product\",\n            \"protein\",\n            \"RNA\",\n            r\"variant( \\d+)?\",\n        ]\n        non_informative_re = re.compile(r\"|\".join(non_informative_words), re.IGNORECASE)\n\n        # Remove the feature ID if it's in the description\n        if feat_id is not None:\n            feat_id_re = re.compile(feat_id, re.IGNORECASE)\n            product = re.sub(feat_id_re, \"\", product)\n\n        # Remove punctuations\n        punct_re = re.compile(r\"[,;: _()-]+\")\n        product = re.sub(punct_re, \" \", product)\n\n        # Then remove non informative words\n        product = re.sub(non_informative_re, \" \", product)\n\n        # Anything (informative) left?\n        empty_re = re.compile(r\"^[ ]*$\")\n        return not bool(empty_re.match(product))\n\n    def _to_list(self):\n        all_list = []\n        for feat_dict in self.features.values():\n            all_list += feat_dict.values()\n        return all_list\n\n    def to_json(self, out_path: PathLike) -&gt; None:\n        \"\"\"Print out the current annotation list in a json file.\n\n        Args:\n            out_path: JSON file path where to write the data.\n\n        \"\"\"\n        self._transfer_descriptions()\n        feats_list = self._to_list()\n        print_json(Path(out_path), feats_list)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/extract_annotation/#src.ensembl.io.genomio.gff3.extract_annotation.FunctionalAnnotations.add_feature","title":"<code>add_feature(feature, feat_type, parent_id=None)</code>","text":"<p>Add annotation for a feature of a given type. If a parent_id is provided, record the relatioship.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>SeqFeature</code> <p>The feature to create an annotation.</p> required <code>feat_type</code> <code>str</code> <p>Type of the feature to annotate.</p> required Source code in <code>src/ensembl/io/genomio/gff3/extract_annotation.py</code> <pre><code>def add_feature(self, feature: SeqFeature, feat_type: str, parent_id: Optional[str] = None) -&gt; None:\n    \"\"\"Add annotation for a feature of a given type. If a parent_id is provided, record the relatioship.\n\n    Args:\n        feature: The feature to create an annotation.\n        feat_type: Type of the feature to annotate.\n    \"\"\"\n    if feat_type not in self.features:\n        raise AnnotationError(f\"Unsupported feature type {feat_type}\")\n\n    if feature.id in self.features[feat_type]:\n        raise AnnotationError(f\"Feature {feat_type} ID {feature.id} already added\")\n    feature_object = self._generic_feature(feature, feat_type)\n    self.features[feat_type][feature.id] = feature_object\n\n    if parent_id:\n        if feat_type in _PARENTS:\n            parent_type = _PARENTS[feat_type]\n            self.add_parent(parent_type, parent_id, feature.id)\n        else:\n            raise AnnotationError(f\"No parent possible for {feat_type} {feature.id}\")\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/extract_annotation/#src.ensembl.io.genomio.gff3.extract_annotation.FunctionalAnnotations.add_parent","title":"<code>add_parent(parent_type, parent_id, child_id)</code>","text":"<p>Record a parent-child IDs relationship for a given parent biotype.</p> Source code in <code>src/ensembl/io/genomio/gff3/extract_annotation.py</code> <pre><code>def add_parent(self, parent_type: str, parent_id: str, child_id: str) -&gt; None:\n    \"\"\"Record a parent-child IDs relationship for a given parent biotype.\"\"\"\n    if parent_type in self.parents:\n        self.parents[parent_type][child_id] = parent_id\n    else:\n        raise MissingParentError(f\"Unsupported parent type {parent_type}\")\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/extract_annotation/#src.ensembl.io.genomio.gff3.extract_annotation.FunctionalAnnotations.get_features","title":"<code>get_features(feat_type)</code>","text":"<p>Get all feature annotations for the requested type.</p> Source code in <code>src/ensembl/io/genomio/gff3/extract_annotation.py</code> <pre><code>def get_features(self, feat_type: str) -&gt; Dict[str, Annotation]:\n    \"\"\"Get all feature annotations for the requested type.\"\"\"\n    if feat_type in self.features:\n        return self.features[feat_type]\n    raise AnnotationError(f\"No such feature type {feat_type}\")\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/extract_annotation/#src.ensembl.io.genomio.gff3.extract_annotation.FunctionalAnnotations.get_parent","title":"<code>get_parent(parent_type, child_id)</code>","text":"<p>Returns the parent ID of a given child for a given parent biotype.</p> Source code in <code>src/ensembl/io/genomio/gff3/extract_annotation.py</code> <pre><code>def get_parent(self, parent_type: str, child_id: str) -&gt; str:\n    \"\"\"Returns the parent ID of a given child for a given parent biotype.\"\"\"\n    if parent_type in self.parents:\n        parent_id = self.parents[parent_type].get(child_id)\n        if parent_id is None:\n            raise MissingParentError(f\"Can't find {parent_type} parent for {child_id}\")\n        return parent_id\n    raise MissingParentError(f\"Unsupported parent type {parent_type}\")\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/extract_annotation/#src.ensembl.io.genomio.gff3.extract_annotation.FunctionalAnnotations.product_is_informative","title":"<code>product_is_informative(product, feat_id=None)</code>  <code>staticmethod</code>","text":"<p>Returns True if the product name contains informative words, False otherwise.</p> <p>It is considered uninformative when the description contains words such as \"hypothetical\" or or \"putative\". If a feature ID is provided, consider it uninformative as well (we do not want descriptions to be just the ID).</p> <p>Parameters:</p> Name Type Description Default <code>product</code> <code>str</code> <p>A product name.</p> required <code>feat_id</code> <code>Optional[str]</code> <p>Feature ID (optional).</p> <code>None</code> Source code in <code>src/ensembl/io/genomio/gff3/extract_annotation.py</code> <pre><code>@staticmethod\ndef product_is_informative(product: str, feat_id: Optional[str] = None) -&gt; bool:\n    \"\"\"Returns True if the product name contains informative words, False otherwise.\n\n    It is considered uninformative when the description contains words such as \"hypothetical\" or\n    or \"putative\". If a feature ID is provided, consider it uninformative as well (we do not want\n    descriptions to be just the ID).\n\n    Args:\n        product: A product name.\n        feat_id: Feature ID (optional).\n\n    \"\"\"\n    non_informative_words = [\n        \"hypothetical\",\n        \"putative\",\n        \"uncharacterized\",\n        \"unspecified\",\n        r\"(of )?unknown function\",\n        \"conserved\",\n        \"predicted\",\n        \"fragment\",\n        \"product\",\n        \"protein\",\n        \"RNA\",\n        r\"variant( \\d+)?\",\n    ]\n    non_informative_re = re.compile(r\"|\".join(non_informative_words), re.IGNORECASE)\n\n    # Remove the feature ID if it's in the description\n    if feat_id is not None:\n        feat_id_re = re.compile(feat_id, re.IGNORECASE)\n        product = re.sub(feat_id_re, \"\", product)\n\n    # Remove punctuations\n    punct_re = re.compile(r\"[,;: _()-]+\")\n    product = re.sub(punct_re, \" \", product)\n\n    # Then remove non informative words\n    product = re.sub(non_informative_re, \" \", product)\n\n    # Anything (informative) left?\n    empty_re = re.compile(r\"^[ ]*$\")\n    return not bool(empty_re.match(product))\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/extract_annotation/#src.ensembl.io.genomio.gff3.extract_annotation.FunctionalAnnotations.to_json","title":"<code>to_json(out_path)</code>","text":"<p>Print out the current annotation list in a json file.</p> <p>Parameters:</p> Name Type Description Default <code>out_path</code> <code>PathLike</code> <p>JSON file path where to write the data.</p> required Source code in <code>src/ensembl/io/genomio/gff3/extract_annotation.py</code> <pre><code>def to_json(self, out_path: PathLike) -&gt; None:\n    \"\"\"Print out the current annotation list in a json file.\n\n    Args:\n        out_path: JSON file path where to write the data.\n\n    \"\"\"\n    self._transfer_descriptions()\n    feats_list = self._to_list()\n    print_json(Path(out_path), feats_list)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/extract_annotation/#src.ensembl.io.genomio.gff3.extract_annotation.MissingParentError","title":"<code>MissingParentError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Trying to add a feature without an expected parent.</p> Source code in <code>src/ensembl/io/genomio/gff3/extract_annotation.py</code> <pre><code>class MissingParentError(Exception):\n    \"\"\"Trying to add a feature without an expected parent.\"\"\"\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/process/","title":"process","text":"<p>Standardize the gene model representation of a GFF3 file, and extract the functional annotation in a separate file.</p>"},{"location":"reference/ensembl/io/genomio/gff3/process/#src.ensembl.io.genomio.gff3.process.GFFGeneMerger","title":"<code>GFFGeneMerger</code>","text":"<p>             Bases: <code>GFFParserCommon</code></p> <p>Specialized class to merge split genes in a GFF3 file, prior to further parsing.</p> Source code in <code>src/ensembl/io/genomio/gff3/process.py</code> <pre><code>class GFFGeneMerger(GFFParserCommon):\n    \"\"\"Specialized class to merge split genes in a GFF3 file, prior to further parsing.\"\"\"\n\n    def merge(self, in_gff_path: PathLike, out_gff_path: PathLike) -&gt; List[str]:\n        \"\"\"\n        Merge genes in a gff that are split in multiple lines\n        \"\"\"\n        to_merge = []\n        merged: List[str] = []\n\n        with Path(in_gff_path).open(\"r\") as in_gff_fh, Path(out_gff_path).open(\"w\") as out_gff_fh:\n            for line in in_gff_fh:\n                # Skip comments\n                if line.startswith(\"#\"):\n                    out_gff_fh.write(line)\n                else:\n                    # Parse one line\n                    line = line.rstrip()\n                    fields = line.split(\"\\t\")\n                    attr_fields = fields[8].split(\";\")\n                    attrs = {}\n                    for a in attr_fields:\n                        (key, value) = a.split(\"=\")\n                        attrs[key] = value\n\n                    # Check this is a gene to merge; cache it then\n                    if fields[2] in self.gene_types and (\"part\" in attrs or \"is_ordered\" in attrs):\n                        to_merge.append(fields)\n\n                    # If not, merge previous gene if needed, and print the line\n                    else:\n                        if to_merge:\n                            merged_str = []\n                            for line_to_merge in to_merge:\n                                merged_str.append(\"\\t\".join(line_to_merge))\n                            merged.append(\"\\n\".join(merged_str) + \"\\n\")\n\n                            new_line = self._merge_genes(to_merge)\n                            out_gff_fh.write(new_line)\n                            to_merge = []\n                        out_gff_fh.write(line + \"\\n\")\n\n            # Print last merged gene if there is one\n            if to_merge:\n                merged_str = []\n                for line_to_merge in to_merge:\n                    merged_str.append(\"\\t\".join(line_to_merge))\n                merged.append(\"\\n\".join(merged_str) + \"\\n\")\n\n                new_line = self._merge_genes(to_merge)\n                out_gff_fh.write(new_line)\n\n        return merged\n\n    def _merge_genes(self, to_merge: List) -&gt; str:\n        \"\"\"Returns a single gene gff3 line merged from separate parts.\n\n        Args:\n            to_merge: List of gff3 lines with gene parts.\n\n        \"\"\"\n        print(f\"Merge gene in {len(to_merge)} parts\")\n        min_start = -1\n        max_end = -1\n        for gene in to_merge:\n            print(f\"Merge part: {gene[8]}\")\n            start = int(gene[3])\n            end = int(gene[4])\n\n            if start &lt; min_start or min_start &lt; 0:\n                min_start = start\n            if end &gt; max_end or max_end &lt; 0:\n                max_end = end\n\n        # Take the first line as template and replace things\n        new_gene = to_merge[0]\n        new_gene[3] = str(min_start)\n        new_gene[4] = str(max_end)\n\n        attrs = new_gene[8]\n        attrs = attrs.replace(\";is_ordered=true\", \"\")\n        attrs = re.sub(r\";part=\\d+/\\d+\", \"\", attrs)\n        new_gene[8] = attrs\n\n        return \"\\t\".join(new_gene) + \"\\n\"\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/process/#src.ensembl.io.genomio.gff3.process.GFFGeneMerger.merge","title":"<code>merge(in_gff_path, out_gff_path)</code>","text":"<p>Merge genes in a gff that are split in multiple lines</p> Source code in <code>src/ensembl/io/genomio/gff3/process.py</code> <pre><code>def merge(self, in_gff_path: PathLike, out_gff_path: PathLike) -&gt; List[str]:\n    \"\"\"\n    Merge genes in a gff that are split in multiple lines\n    \"\"\"\n    to_merge = []\n    merged: List[str] = []\n\n    with Path(in_gff_path).open(\"r\") as in_gff_fh, Path(out_gff_path).open(\"w\") as out_gff_fh:\n        for line in in_gff_fh:\n            # Skip comments\n            if line.startswith(\"#\"):\n                out_gff_fh.write(line)\n            else:\n                # Parse one line\n                line = line.rstrip()\n                fields = line.split(\"\\t\")\n                attr_fields = fields[8].split(\";\")\n                attrs = {}\n                for a in attr_fields:\n                    (key, value) = a.split(\"=\")\n                    attrs[key] = value\n\n                # Check this is a gene to merge; cache it then\n                if fields[2] in self.gene_types and (\"part\" in attrs or \"is_ordered\" in attrs):\n                    to_merge.append(fields)\n\n                # If not, merge previous gene if needed, and print the line\n                else:\n                    if to_merge:\n                        merged_str = []\n                        for line_to_merge in to_merge:\n                            merged_str.append(\"\\t\".join(line_to_merge))\n                        merged.append(\"\\n\".join(merged_str) + \"\\n\")\n\n                        new_line = self._merge_genes(to_merge)\n                        out_gff_fh.write(new_line)\n                        to_merge = []\n                    out_gff_fh.write(line + \"\\n\")\n\n        # Print last merged gene if there is one\n        if to_merge:\n            merged_str = []\n            for line_to_merge in to_merge:\n                merged_str.append(\"\\t\".join(line_to_merge))\n            merged.append(\"\\n\".join(merged_str) + \"\\n\")\n\n            new_line = self._merge_genes(to_merge)\n            out_gff_fh.write(new_line)\n\n    return merged\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/process/#src.ensembl.io.genomio.gff3.process.GFFParserCommon","title":"<code>GFFParserCommon</code>","text":"<p>Heritable class to share the list of feature types supported or ignored by the parser</p> Source code in <code>src/ensembl/io/genomio/gff3/process.py</code> <pre><code>class GFFParserCommon:\n    \"\"\"Heritable class to share the list of feature types supported or ignored by the parser\"\"\"\n\n    # Supported gene level biotypes\n    gene_types = [\n        \"gene\",\n        \"pseudogene\",\n        \"ncRNA_gene\",\n    ]\n\n    # Exception: non_gene_types that are nonetheless kept in the GFF3 file\n    non_gene_types = [\n        \"transposable_element\",\n    ]\n\n    # Supported transcript level biotypes\n    transcript_types = [\n        \"C_gene_segment\",\n        \"guide_RNA\",\n        \"lnc_RNA\",\n        \"miRNA\",\n        \"misc_RNA\",\n        \"mRNA\",\n        \"ncRNA\",\n        \"piRNA\",\n        \"pseudogenic_rRNA\",\n        \"pseudogenic_transcript\",\n        \"pseudogenic_tRNA\",\n        \"ribozyme\",\n        \"RNase_MRP_RNA\",\n        \"RNase_P_RNA\",\n        \"rRNA\",\n        \"scRNA\",\n        \"snoRNA\",\n        \"snRNA\",\n        \"SRP_RNA\",\n        \"telomerase_RNA\",\n        \"transcript\",\n        \"tRNA\",\n        \"V_gene_segment\",\n    ]\n\n    # Biotypes that are ignored, and removed from the final GFF3 file\n    ignored_gene_types = [\n        \"cDNA_match\",\n        \"centromere\",\n        \"D_loop\",\n        \"direct_repeat\",\n        \"dispersed_repeat\",\n        \"gap\",\n        \"intron\",\n        \"inverted_repeat\",\n        \"long_terminal_repeat\",\n        \"microsatellite\",\n        \"origin_of_replication\",\n        \"region\",\n        \"repeat_region\",\n        \"satellite_DNA\",\n        \"sequence_feature\",\n        \"sequence_secondary_structure\",\n        \"sequence_uncertainty\",\n        \"STS\",\n        \"tandem_repeat\",\n        \"telomere\",\n        \"terminal%2Cinverted\",\n    ]\n\n    # Ignored biotypes that are under a gene parent feature\n    ignored_transcript_types = [\n        \"3'UTR\",\n        \"5'UTR\",\n        \"antisense_RNA\",\n        \"intron\",\n        \"non_canonical_five_prime_splice_site\",\n        \"non_canonical_three_prime_splice_site\",\n    ]\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/process/#src.ensembl.io.genomio.gff3.process.GFFParserError","title":"<code>GFFParserError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Error when parsing a GFF3 file.</p> Source code in <code>src/ensembl/io/genomio/gff3/process.py</code> <pre><code>class GFFParserError(Exception):\n    \"\"\"Error when parsing a GFF3 file.\"\"\"\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/process/#src.ensembl.io.genomio.gff3.process.GFFSimplifier","title":"<code>GFFSimplifier</code>","text":"<p>             Bases: <code>GFFParserCommon</code></p> <p>Parse a GGF3 file and output a cleaned up GFF3 + annotation json file.</p> <p>Raises:</p> Type Description <code>GFFParserError</code> <p>If an error cannot be automatically fixed.</p> Source code in <code>src/ensembl/io/genomio/gff3/process.py</code> <pre><code>class GFFSimplifier(GFFParserCommon):\n    \"\"\"Parse a GGF3 file and output a cleaned up GFF3 + annotation json file.\n\n    Raises:\n        GFFParserError: If an error cannot be automatically fixed.\n    \"\"\"\n\n    # Multiple parameters to automate various fixes\n    skip_unrecognized = False\n    gene_cds_skip_others = False\n    allow_pseudogene_with_CDS = False\n    exclude_seq_regions: List = []\n    validate_gene_id = True\n    min_id_length = 8\n    stable_id_prefix = None\n    current_stable_id_number: int = 0\n\n    def __init__(self, genome_path: Optional[PathLike] = None, make_missing_stable_ids: bool = False):\n        self.records = Records()\n        self.annotations = FunctionalAnnotations()\n        self.genome = {}\n        if genome_path:\n            with Path(genome_path).open(\"r\") as genome_fh:\n                self.genome = json.load(genome_fh)\n        self.make_missing_stable_ids: bool = make_missing_stable_ids\n\n    def simpler_gff3(self, in_gff_path: PathLike) -&gt; None:\n        \"\"\"\n        Load a GFF3 from INSDC and rewrite it in a simpler version,\n        and also write a functional_annotation file\n        \"\"\"\n\n        allowed_gene_types = self.gene_types\n        ignored_gene_types = self.ignored_gene_types\n        transcript_types = self.transcript_types\n        allowed_non_gene_types = self.non_gene_types\n        skip_unrecognized = self.skip_unrecognized\n        to_exclude = self.exclude_seq_regions\n\n        with Path(in_gff_path).open(\"r\") as in_gff_fh:\n            fail_types: Dict[str, int] = {}\n\n            for record in GFF.parse(in_gff_fh):\n                new_record = SeqRecord(record.seq, id=record.id)\n                if record.id in to_exclude:\n                    print(f\"Skip seq_region {record.id}\")\n                    continue\n\n                # Root features (usually genes)\n                for feat in record.features:\n                    # Skip or format depending on the feature type\n                    if feat.type in ignored_gene_types:\n                        continue\n                    if feat.type in transcript_types:\n                        feat = self.transcript_gene(feat)\n                    elif feat.type == \"CDS\":\n                        feat = self.cds_gene(feat)\n                    elif feat.type in (\"mobile_genetic_element\", \"transposable_element\"):\n                        feat = self.format_mobile_element(feat)\n\n                    # Normalize the gene structure\n                    if feat.type in allowed_gene_types:\n                        feat = self.normalize_gene(feat, fail_types)\n                    elif feat.type in allowed_non_gene_types:\n                        pass\n                    else:\n                        fail_types[\"gene=\" + feat.type] = 1\n                        message = f\"Unsupported feature type: {feat.type} (for {feat.id})\"\n                        print(message)\n                        if skip_unrecognized:\n                            del feat\n                            continue\n\n                    new_record.features.append(feat)\n                self.records.append(new_record)\n\n            if fail_types and not skip_unrecognized:\n                fail_errors = \"\\n   \".join(fail_types.keys())\n                raise GFFParserError(f\"Unrecognized types found:\\n   {fail_errors}\")\n\n    def format_mobile_element(self, feat: SeqFeature) -&gt; SeqFeature:\n        \"\"\"Given a mobile_genetic_element feature, transform it into a transposable_element\"\"\"\n\n        # Change mobile_genetic_element into a transposable_element feature\n        if feat.type == \"mobile_genetic_element\":\n            mobile_element_type = feat.qualifiers.get(\"mobile_element_type\", [])\n            if mobile_element_type:\n                # Get the type (and name) from the attrib\n                if \":\" in mobile_element_type[0]:\n                    element_type, element_name = mobile_element_type[0].split(\":\")\n                    description = f\"{element_type} ({element_name})\"\n                else:\n                    element_type = mobile_element_type[0]\n                    description = element_type\n\n                # Keep the metadata in the description if the type is known\n                if element_type in (\"transposon\", \"retrotransposon\"):\n                    feat.type = \"transposable_element\"\n                    if not feat.qualifiers.get(\"product\"):\n                        feat.qualifiers[\"product\"] = [description]\n                else:\n                    print(f\"Mobile genetic element 'mobile_element_type' is not transposon: {element_type}\")\n                    return feat\n            else:\n                print(\"Mobile genetic element does not have a 'mobile_element_type' tag\")\n                return feat\n        elif feat.type == \"transposable_element\":\n            pass\n        else:\n            print(f\"Feature {feat.id} is not a supported TE feature {feat.type}\")\n            return feat\n\n        # Generate ID if needed and add it to the functional annotation\n        feat.id = self.normalize_gene_id(feat)\n        self.annotations.add_feature(feat, \"transposable_element\")\n        feat.qualifiers = {\"ID\": feat.id}\n\n        return feat\n\n    def format_gene_segments(self, transcript: SeqFeature) -&gt; SeqFeature:\n        \"\"\"Returns the equivalent Ensembl biotype feature for gene segment transcript features.\n\n        Supported features: \"C_gene_segment\" and \"V_gene_segment\".\n\n        Args:\n            transcript: Gene segment transcript feature.\n\n        \"\"\"\n        # Change V/C_gene_segment into a its corresponding transcript names\n        if transcript.type in (\"C_gene_segment\", \"V_gene_segment\"):\n            standard_name = transcript.qualifiers[\"standard_name\"][0]\n            biotype = transcript.type.replace(\"_segment\", \"\")\n            if re.search(r\"\\b(immunoglobulin|ig)\\b\", standard_name, flags=re.IGNORECASE):\n                biotype = f\"IG_{biotype}\"\n            elif re.search(r\"\\bt[- _]cell\\b\", standard_name, flags=re.IGNORECASE):\n                biotype = f\"TR_{biotype}\"\n            else:\n                print(f\"Unexpected 'standard_name' content for feature {transcript.id}: {standard_name}\")\n                return transcript\n            transcript.type = biotype\n        return transcript\n\n    def normalize_gene(self, gene: SeqFeature, fail_types: Dict[str, int]) -&gt; SeqFeature:\n        \"\"\"Returns a normalized gene structure, separate from the functional elements.\n\n        Args:\n            gene: Gene object to normalize.\n            functional_annotation: List of feature annotations (appended by this method).\n            fail_types: List of feature types that are not supported (appended by this method).\n\n        \"\"\"\n\n        # New gene ID\n        gene.id = self.normalize_gene_id(gene)\n\n        # Gene with no subfeatures: need to create a transcript at least\n        if len(gene.sub_features) == 0:\n            print(f\"Insert transcript for lone gene {gene.id}\")\n            transcript = self.transcript_for_gene(gene)\n            gene.sub_features = [transcript]\n\n        # Count features\n        fcounter = Counter([feat.type for feat in gene.sub_features])\n\n        # Transform gene - CDS to gene-transcript-exon-CDS\n        if len(fcounter) == 1:\n            if fcounter.get(\"CDS\"):\n                num_subs = len(gene.sub_features)\n                print(f\"Insert transcript-exon feats for {gene.id} ({num_subs} CDSs)\")\n                transcripts = self.gene_to_cds(gene)\n                gene.sub_features = transcripts\n\n            # Transform gene - exon to gene-transcript-exon\n            elif fcounter.get(\"exon\"):\n                num_subs = len(gene.sub_features)\n                print(f\"Insert transcript for {gene.id} ({num_subs} exons)\")\n                transcript = self.gene_to_exon(gene)\n                gene.sub_features = [transcript]\n        else:\n            # Check that we don't mix\n            if fcounter.get(\"mRNA\") and fcounter.get(\"CDS\"):\n                # Move CDS(s) from parent gene to parent mRNA if needed\n                gene = self.move_cds_to_mrna(gene)\n            if fcounter.get(\"mRNA\") and fcounter.get(\"exon\"):\n                # Special case with extra exons\n                gene = self.clean_extra_exons(gene)\n\n        # Remove CDS from pseudogenes\n        if gene.type == \"pseudogene\" and not self.allow_pseudogene_with_CDS:\n            self.remove_cds_from_pseudogene(gene)\n\n        # TRANSCRIPTS\n        gene = self._normalize_transcripts(gene, fail_types)\n\n        # PSEUDOGENE CDS IDs\n        if gene.type == \"pseudogene\" and self.allow_pseudogene_with_CDS:\n            self.normalize_pseudogene_cds(gene)\n\n        # Finally, store gene functional annotation\n        self.annotations.add_feature(gene, \"gene\")\n\n        # replace qualifiers\n        old_gene_qualifiers = gene.qualifiers\n        gene.qualifiers = {\"ID\": gene.id, \"source\": old_gene_qualifiers[\"source\"]}\n\n        return gene\n\n    def _normalize_transcripts(self, gene: SeqFeature, fail_types) -&gt; SeqFeature:\n        \"\"\"Returns a normalized transcript.\"\"\"\n        allowed_transcript_types = self.transcript_types\n        skip_unrecognized = self.skip_unrecognized\n\n        transcripts_to_delete = []\n        for count, transcript in enumerate(gene.sub_features):\n            if (\n                transcript.type not in allowed_transcript_types\n                and transcript.type not in self.ignored_transcript_types\n            ):\n                fail_types[\"transcript=\" + transcript.type] = 1\n                message = (\n                    f\"Unrecognized transcript type: {transcript.type}\" f\" for {transcript.id} ({gene.id})\"\n                )\n                print(message)\n                if skip_unrecognized:\n                    transcripts_to_delete.append(count)\n                    continue\n\n            # New transcript ID\n            transcript_number = count + 1\n            transcript.id = self.normalize_transcript_id(gene.id, transcript_number)\n\n            transcript = self.format_gene_segments(transcript)\n\n            # Store transcript functional annotation\n            self.annotations.add_feature(transcript, \"transcript\", gene.id)\n\n            # Replace qualifiers\n            old_transcript_qualifiers = transcript.qualifiers\n            transcript.qualifiers = {\n                \"ID\": transcript.id,\n                \"Parent\": gene.id,\n            }\n            if \"source\" in old_transcript_qualifiers:\n                transcript.qualifiers[\"source\"] = old_transcript_qualifiers[\"source\"]\n\n            # EXONS AND CDS\n            transcript = self._normalize_transcript_subfeatures(gene, transcript, fail_types)\n\n        if transcripts_to_delete:\n            for elt in sorted(transcripts_to_delete, reverse=True):\n                gene.sub_features.pop(elt)\n\n        return gene\n\n    def _normalize_transcript_subfeatures(\n        self, gene: SeqFeature, transcript: SeqFeature, fail_types\n    ) -&gt; SeqFeature:\n        \"\"\"Returns a transcript with normalized sub-features.\"\"\"\n        ignored_transcript_types = self.ignored_transcript_types\n        cds_found = False\n        exons_to_delete = []\n        for tcount, feat in enumerate(transcript.sub_features):\n            if feat.type == \"exon\":\n                # Replace qualifiers\n                old_exon_qualifiers = feat.qualifiers\n                feat.qualifiers = {\"Parent\": transcript.id}\n                if \"source\" in old_exon_qualifiers:\n                    feat.qualifiers[\"source\"] = old_exon_qualifiers[\"source\"]\n            elif feat.type == \"CDS\":\n                # New CDS ID\n                feat.id = self.normalize_cds_id(feat.id)\n                if feat.id in (\"\", gene.id, transcript.id):\n                    feat.id = f\"{transcript.id}_cds\"\n\n                # Store CDS functional annotation (only once)\n                if not cds_found:\n                    cds_found = True\n                    self.annotations.add_feature(feat, \"translation\", transcript.id)\n\n                # Replace qualifiers\n                feat.qualifiers = {\n                    \"ID\": feat.id,\n                    \"Parent\": transcript.id,\n                    \"phase\": feat.qualifiers[\"phase\"],\n                    \"source\": feat.qualifiers[\"source\"],\n                }\n            else:\n                if feat.type in ignored_transcript_types:\n                    exons_to_delete.append(tcount)\n                    continue\n\n                fail_types[f\"sub_transcript={feat.type}\"] = 1\n                message = (\n                    f\"Unrecognized exon type for {feat.type}: {feat.id}\"\n                    f\" (for transcript {transcript.id} of type {transcript.type})\"\n                )\n                print(message)\n                if self.skip_unrecognized:\n                    exons_to_delete.append(tcount)\n                    continue\n\n        if exons_to_delete:\n            for elt in sorted(exons_to_delete, reverse=True):\n                transcript.sub_features.pop(elt)\n        return transcript\n\n    def transcript_gene(self, ncrna: SeqFeature) -&gt; SeqFeature:\n        \"\"\"Create a gene for lone transcripts: 'gene' for tRNA/rRNA, and 'ncRNA' for all others\n\n        Args:\n            ncrna: the transcript for which we want to create a gene.\n\n        Returns:\n            The gene that contains the transcript.\n\n        \"\"\"\n        new_type = \"ncRNA_gene\"\n        if ncrna.type in (\"tRNA\", \"rRNA\"):\n            new_type = \"gene\"\n        print(f\"Put the transcript {ncrna.type} in a {new_type} parent feature\")\n        gene = SeqFeature(ncrna.location, type=new_type)\n        gene.qualifiers[\"source\"] = ncrna.qualifiers[\"source\"]\n        gene.sub_features = [ncrna]\n        gene.id = ncrna.id\n\n        return gene\n\n    def cds_gene(self, cds: SeqFeature) -&gt; SeqFeature:\n        \"\"\"Returns a gene created for a lone CDS.\"\"\"\n\n        print(\"Put the lone CDS in gene-mRNA parent features\")\n\n        # Create a transcript, add the CDS\n        transcript = SeqFeature(cds.location, type=\"mRNA\")\n        transcript.qualifiers[\"source\"] = cds.qualifiers[\"source\"]\n        transcript.sub_features = [cds]\n\n        # Add an exon too\n        exon = SeqFeature(cds.location, type=\"exon\")\n        exon.qualifiers[\"source\"] = cds.qualifiers[\"source\"]\n        transcript.sub_features.append(exon)\n\n        # Create a gene, add the transcript\n        gene_type = \"gene\"\n        if (\"pseudo\" in cds.qualifiers) and (cds.qualifiers[\"pseudo\"][0] == \"true\"):\n            gene_type = \"pseudogene\"\n        gene = SeqFeature(cds.location, type=gene_type)\n        gene.qualifiers[\"source\"] = cds.qualifiers[\"source\"]\n        gene.sub_features = [transcript]\n        gene.id = self.generate_stable_id()\n\n        return gene\n\n    def transcript_for_gene(self, gene: SeqFeature) -&gt; SeqFeature:\n        \"\"\"Returns a transcript, from a gene without one.\"\"\"\n\n        transcript = SeqFeature(gene.location, type=\"transcript\")\n        transcript.qualifiers[\"source\"] = gene.qualifiers[\"source\"]\n        transcript.sub_features = []\n\n        return transcript\n\n    def gene_to_cds(self, gene: SeqFeature) -&gt; List[SeqFeature]:\n        \"\"\"Returns a list of transcripts (with exons), from a gene with only CDS children.\"\"\"\n\n        gene_cds_skip_others = self.gene_cds_skip_others\n        transcripts_dict = {}\n        del_transcript = []\n\n        for count, cds in enumerate(gene.sub_features):\n            if cds.type != \"CDS\":\n                if gene_cds_skip_others:\n                    del_transcript.append(count)\n                    continue\n                raise GFFParserError(\n                    \"Can not create a chain 'transcript - exon - CDS'\"\n                    f\" when the gene children are not all CDSs\"\n                    f\" ({cds.id} of type {cds.type} is child of gene {gene.id})\"\n                )\n\n            exon = SeqFeature(cds.location, type=\"exon\")\n\n            # Add to transcript or create a new one\n            if cds.id not in transcripts_dict:\n                print(f\"Create new mRNA for {cds.id}\")\n                transcript = self.build_transcript(gene)\n                transcripts_dict[cds.id] = transcript\n            exon.qualifiers[\"source\"] = gene.qualifiers[\"source\"]\n            transcripts_dict[cds.id].sub_features.append(exon)\n            transcripts_dict[cds.id].sub_features.append(cds)\n\n        for elt in sorted(del_transcript, reverse=True):\n            gene.sub_features.pop(elt)\n\n        transcripts = list(transcripts_dict.values())\n\n        return transcripts\n\n    def build_transcript(self, gene: SeqFeature) -&gt; SeqFeature:\n        \"\"\"Returns a transcript with same metadata as the gene provided.\"\"\"\n\n        transcript = SeqFeature(gene.location, type=\"mRNA\")\n        transcript.qualifiers[\"source\"] = gene.qualifiers[\"source\"]\n        transcript.sub_features = []\n        return transcript\n\n    def move_cds_to_mrna(self, gene: SeqFeature) -&gt; SeqFeature:\n        \"\"\"Move CDS child features of a gene, to the mRNA.\n\n        This is to fix the case where we have the following structure:\n        gene -&gt; [ mRNA, CDSs ]\n        and change it to\n        gene -&gt; [ mRNA -&gt; [ CDSs ] ]\n        The mRNA might have exons, in which case check that they match the CDS coordinates.\n\n        Raises an exception if the feature structure is not recognized.\n\n        Args:\n            A gene with only one transcript, to check and fix.\n\n        Returns:\n            The gene where the CDSs have been moved, if needed.\n\n        \"\"\"\n        # First, count the types\n        mrnas = []\n        cdss = []\n\n        gene_subf_clean = []\n        for subf in gene.sub_features:\n            if subf.type == \"mRNA\":\n                mrnas.append(subf)\n            elif subf.type == \"CDS\":\n                cdss.append(subf)\n            else:\n                gene_subf_clean.append(subf)\n\n        if len(cdss) == 0:\n            # Nothing to fix here, no CDSs to move\n            return gene\n        if len(mrnas) &gt; 1:\n            raise GFFParserError(\n                f\"Can't fix gene {gene.id}: contains several mRNAs and CDSs, all children of the gene\"\n            )\n\n        mrna = mrnas[0]\n\n        # Check if there are exons (or CDSs) under the mRNA\n        sub_exons = []\n        sub_cdss = []\n        for subf in mrna.sub_features:\n            if subf.type == \"CDS\":\n                sub_cdss.append(subf)\n            elif subf.type == \"exon\":\n                sub_exons.append(subf)\n\n        self._check_sub_cdss(gene, sub_cdss)\n        self._check_sub_exons(gene, cdss, sub_exons)\n\n        print(f\"Gene {gene.id}: move {len(cdss)} CDSs to the mRNA\")\n        # No more issues? move the CDSs\n        mrna.sub_features += cdss\n        # And remove them from the gene\n        gene.sub_features = gene_subf_clean\n        gene.sub_features.append(mrna)\n\n        return gene\n\n    @staticmethod\n    def _check_sub_cdss(gene, sub_cdss) -&gt; None:\n        if len(sub_cdss) &gt; 0:\n            raise GFFParserError(f\"Gene {gene.id} has CDSs as children of the gene and mRNA\")\n\n    @staticmethod\n    def _check_sub_exons(gene, cdss, sub_exons) -&gt; None:\n        \"\"\"Check that the exons of the mRNA and the CDSs match\"\"\"\n\n        if len(sub_exons) &gt; 0:\n            # Check that they match the CDS outside\n            if len(sub_exons) == len(cdss):\n                # Now that all coordinates are the same\n                coord_exons = [f\"{exon.location}\" for exon in sub_exons]\n                coord_cdss = [f\"{cds.location}\" for cds in cdss]\n\n                if coord_exons != coord_cdss:\n                    raise GFFParserError(f\"Gene {gene.id} CDSs and exons under the mRNA do not match\")\n            else:\n                raise GFFParserError(\n                    f\"Gene {gene.id} CDSs and exons under the mRNA do not match (different count)\"\n                )\n\n    def clean_extra_exons(self, gene: SeqFeature) -&gt; SeqFeature:\n        \"\"\"Remove extra exons, already existing in the mRNA.\n\n        This is a special case where a gene contains proper mRNAs, etc. but also\n        extra exons for the same features. Those exons usually have an ID starting with\n        \"id-\", so that's what we use to detect them.\n        \"\"\"\n        exons = []\n        mrnas = []\n        others = []\n        for subf in gene.sub_features:\n            if subf.type == \"exon\":\n                exons.append(subf)\n            elif subf.type == \"mRNA\":\n                mrnas.append(subf)\n            else:\n                others.append(subf)\n\n        if exons and mrnas:\n            exon_has_id = 0\n            # Check if the exon ids start with \"id-\", which is an indication that they do not belong here\n            for exon in exons:\n                if exon.id.startswith(\"id-\"):\n                    exon_has_id += 1\n            if exon_has_id:\n                if exon_has_id == len(exons):\n                    print(f\"Remove {exon_has_id} extra exons from {gene.id}\")\n                    gene.sub_features = mrnas\n                    gene.sub_features += others\n                else:\n                    raise GFFParserError(f\"Can't remove extra exons for {gene.id}, not all start with 'id-'\")\n\n        return gene\n\n    def gene_to_exon(self, gene: SeqFeature) -&gt; SeqFeature:\n        \"\"\"Returns an intermediary transcript for a gene with direct exon children.\"\"\"\n\n        transcript = SeqFeature(gene.location, type=\"mRNA\")\n        transcript.qualifiers[\"source\"] = gene.qualifiers[\"source\"]\n        transcript.sub_features = []\n\n        for exon in gene.sub_features:\n            transcript.sub_features.append(exon)\n\n        return transcript\n\n    def normalize_gene_id(self, gene: SeqFeature) -&gt; str:\n        \"\"\"Remove any unnecessary prefixes around the gene ID.\n\n        Generate a new stable id if it is not recognized as valid.\n\n        Args:\n            gene: Gene feature to normalize.\n\n        Returns:\n            A normalized gene id.\n\n        \"\"\"\n\n        prefixes = [\"gene-\", \"gene:\"]\n        new_gene_id = self.remove_prefixes(gene.id, prefixes)\n\n        # In case the gene id is not valid, use the GeneID\n        if not self.valid_id(new_gene_id):\n            print(f\"Gene id is not valid: {new_gene_id}\")\n            qual = gene.qualifiers\n            if \"Dbxref\" in qual:\n                for xref in qual[\"Dbxref\"]:\n                    (db, value) = xref.split(\":\")\n                    if db == \"GeneID\":\n                        new_gene_id = f\"{db}_{value}\"\n                        print(f\"Using GeneID {new_gene_id} for stable_id instead of {gene.id}\")\n                        return new_gene_id\n\n            # Make a new stable_id\n            if self.make_missing_stable_ids:\n                new_id = self.generate_stable_id()\n                print(f\"New id: {new_gene_id} -&gt; {new_id}\")\n                return new_id\n            raise GFFParserError(f\"Can't use invalid gene id for {gene}\")\n\n        return new_gene_id\n\n    def generate_stable_id(self) -&gt; str:\n        \"\"\"Returns a new unique gene stable_id with a prefix.\n\n        The id is made up of a prefix and a number, which is auto incremented.\n        Define the prefix with the param \"stable_id_prefix\",\n        or use the genome organism_abbrev and prepend \"TMP_\" to it.\n\n        \"\"\"\n        if self.stable_id_prefix:\n            prefix = self.stable_id_prefix\n        else:\n            if self.genome:\n                org = self.genome.get(\"BRC4\", {}).get(\"organism_abbrev\")\n            if org is None:\n                prefix = \"TMP_PREFIX_\"\n            else:\n                prefix = \"TMP_\" + org + \"_\"\n            self.stable_id_prefix = prefix\n\n        number = self.current_stable_id_number + 1\n        new_id = f\"{prefix}{number}\"\n        self.current_stable_id_number = number\n\n        return new_id\n\n    def valid_id(self, name: str) -&gt; bool:\n        \"\"\"Check that the format of a stable id is valid.\"\"\"\n\n        if not self.validate_gene_id:\n            return True\n\n        min_length = self.min_id_length\n\n        # Trna (from tRNAscan)\n        if re.search(r\"^Trna\", name):\n            print(f\"Stable id is a Trna from tRNA-scan: {name}\")\n            return False\n\n        # Coordinates\n        if re.search(r\"^.+:\\d+..\\d+\", name):\n            print(f\"Stable id is a coordinate: {name}\")\n            return False\n\n        # Special characters\n        if re.search(r\"[ |]\", name):\n            print(f\"Stable id contains special characters: {name}\")\n            return False\n\n        # Min length\n        if len(name) &lt; min_length:\n            print(f\"Stable id is too short (&lt;{min_length}) {name}\")\n            return False\n\n        return True\n\n    def normalize_transcript_id(self, gene_id: str, number: int) -&gt; str:\n        \"\"\"Use a gene ID and a number to make a formatted transcript ID.\"\"\"\n\n        transcript_id = f\"{gene_id}_t{number}\"\n        return transcript_id\n\n    def normalize_cds_id(self, cds_id: str) -&gt; str:\n        \"\"\"\n        Check the CDS ID is proper:\n        - Remove any unnecessary prefixes around the CDS ID\n        - Delete the ID if it is not proper\n        \"\"\"\n\n        prefixes = [\"cds-\", \"cds:\"]\n        cds_id = self.remove_prefixes(cds_id, prefixes)\n\n        # Special case: if the ID doesn't look like one, remove it\n        # It needs to be regenerated\n        if not self.valid_id(cds_id):\n            cds_id = \"\"\n\n        return cds_id\n\n    def normalize_pseudogene_cds(self, gene: SeqFeature) -&gt; None:\n        \"\"\"Ensure CDS from a pseudogene have a proper ID\n        - different from the gene\n        - derived from the gene if it is not proper\n        \"\"\"\n\n        for transcript in gene.sub_features:\n            for feat in transcript.sub_features:\n                if feat.type == \"CDS\":\n                    feat.id = self.normalize_cds_id(feat.id)\n                    if feat.id in (\"\", gene.id):\n                        feat.id = f\"{transcript.id}_cds\"\n                        feat.qualifiers[\"ID\"] = feat.id\n\n    def remove_cds_from_pseudogene(self, gene: SeqFeature) -&gt; None:\n        \"\"\"Remove CDS from a pseudogene\n        This assumes the CDSs are sub features of the transcript or the gene\n        \"\"\"\n\n        gene_subfeats = []\n        for transcript in gene.sub_features:\n            if transcript.type == \"CDS\":\n                print(f\"Remove pseudo CDS {transcript.id}\")\n                continue\n            new_subfeats = []\n            for feat in transcript.sub_features:\n                if feat.type == \"CDS\":\n                    print(f\"Remove pseudo CDS {feat.id}\")\n                    continue\n                new_subfeats.append(feat)\n            transcript.sub_features = new_subfeats\n            gene_subfeats.append(transcript)\n        gene.sub_features = gene_subfeats\n\n    def remove_prefixes(self, identifier: str, prefixes: List[str]) -&gt; str:\n        \"\"\"\n        Remove prefixes from an identifier if they are found\n        Return the unaltered identifier otherwise\n        \"\"\"\n        for prefix in prefixes:\n            if identifier.startswith(prefix):\n                identifier = identifier[len(prefix) :]\n        return identifier\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/process/#src.ensembl.io.genomio.gff3.process.GFFSimplifier.build_transcript","title":"<code>build_transcript(gene)</code>","text":"<p>Returns a transcript with same metadata as the gene provided.</p> Source code in <code>src/ensembl/io/genomio/gff3/process.py</code> <pre><code>def build_transcript(self, gene: SeqFeature) -&gt; SeqFeature:\n    \"\"\"Returns a transcript with same metadata as the gene provided.\"\"\"\n\n    transcript = SeqFeature(gene.location, type=\"mRNA\")\n    transcript.qualifiers[\"source\"] = gene.qualifiers[\"source\"]\n    transcript.sub_features = []\n    return transcript\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/process/#src.ensembl.io.genomio.gff3.process.GFFSimplifier.cds_gene","title":"<code>cds_gene(cds)</code>","text":"<p>Returns a gene created for a lone CDS.</p> Source code in <code>src/ensembl/io/genomio/gff3/process.py</code> <pre><code>def cds_gene(self, cds: SeqFeature) -&gt; SeqFeature:\n    \"\"\"Returns a gene created for a lone CDS.\"\"\"\n\n    print(\"Put the lone CDS in gene-mRNA parent features\")\n\n    # Create a transcript, add the CDS\n    transcript = SeqFeature(cds.location, type=\"mRNA\")\n    transcript.qualifiers[\"source\"] = cds.qualifiers[\"source\"]\n    transcript.sub_features = [cds]\n\n    # Add an exon too\n    exon = SeqFeature(cds.location, type=\"exon\")\n    exon.qualifiers[\"source\"] = cds.qualifiers[\"source\"]\n    transcript.sub_features.append(exon)\n\n    # Create a gene, add the transcript\n    gene_type = \"gene\"\n    if (\"pseudo\" in cds.qualifiers) and (cds.qualifiers[\"pseudo\"][0] == \"true\"):\n        gene_type = \"pseudogene\"\n    gene = SeqFeature(cds.location, type=gene_type)\n    gene.qualifiers[\"source\"] = cds.qualifiers[\"source\"]\n    gene.sub_features = [transcript]\n    gene.id = self.generate_stable_id()\n\n    return gene\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/process/#src.ensembl.io.genomio.gff3.process.GFFSimplifier.clean_extra_exons","title":"<code>clean_extra_exons(gene)</code>","text":"<p>Remove extra exons, already existing in the mRNA.</p> <p>This is a special case where a gene contains proper mRNAs, etc. but also extra exons for the same features. Those exons usually have an ID starting with \"id-\", so that's what we use to detect them.</p> Source code in <code>src/ensembl/io/genomio/gff3/process.py</code> <pre><code>def clean_extra_exons(self, gene: SeqFeature) -&gt; SeqFeature:\n    \"\"\"Remove extra exons, already existing in the mRNA.\n\n    This is a special case where a gene contains proper mRNAs, etc. but also\n    extra exons for the same features. Those exons usually have an ID starting with\n    \"id-\", so that's what we use to detect them.\n    \"\"\"\n    exons = []\n    mrnas = []\n    others = []\n    for subf in gene.sub_features:\n        if subf.type == \"exon\":\n            exons.append(subf)\n        elif subf.type == \"mRNA\":\n            mrnas.append(subf)\n        else:\n            others.append(subf)\n\n    if exons and mrnas:\n        exon_has_id = 0\n        # Check if the exon ids start with \"id-\", which is an indication that they do not belong here\n        for exon in exons:\n            if exon.id.startswith(\"id-\"):\n                exon_has_id += 1\n        if exon_has_id:\n            if exon_has_id == len(exons):\n                print(f\"Remove {exon_has_id} extra exons from {gene.id}\")\n                gene.sub_features = mrnas\n                gene.sub_features += others\n            else:\n                raise GFFParserError(f\"Can't remove extra exons for {gene.id}, not all start with 'id-'\")\n\n    return gene\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/process/#src.ensembl.io.genomio.gff3.process.GFFSimplifier.format_gene_segments","title":"<code>format_gene_segments(transcript)</code>","text":"<p>Returns the equivalent Ensembl biotype feature for gene segment transcript features.</p> <p>Supported features: \"C_gene_segment\" and \"V_gene_segment\".</p> <p>Parameters:</p> Name Type Description Default <code>transcript</code> <code>SeqFeature</code> <p>Gene segment transcript feature.</p> required Source code in <code>src/ensembl/io/genomio/gff3/process.py</code> <pre><code>def format_gene_segments(self, transcript: SeqFeature) -&gt; SeqFeature:\n    \"\"\"Returns the equivalent Ensembl biotype feature for gene segment transcript features.\n\n    Supported features: \"C_gene_segment\" and \"V_gene_segment\".\n\n    Args:\n        transcript: Gene segment transcript feature.\n\n    \"\"\"\n    # Change V/C_gene_segment into a its corresponding transcript names\n    if transcript.type in (\"C_gene_segment\", \"V_gene_segment\"):\n        standard_name = transcript.qualifiers[\"standard_name\"][0]\n        biotype = transcript.type.replace(\"_segment\", \"\")\n        if re.search(r\"\\b(immunoglobulin|ig)\\b\", standard_name, flags=re.IGNORECASE):\n            biotype = f\"IG_{biotype}\"\n        elif re.search(r\"\\bt[- _]cell\\b\", standard_name, flags=re.IGNORECASE):\n            biotype = f\"TR_{biotype}\"\n        else:\n            print(f\"Unexpected 'standard_name' content for feature {transcript.id}: {standard_name}\")\n            return transcript\n        transcript.type = biotype\n    return transcript\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/process/#src.ensembl.io.genomio.gff3.process.GFFSimplifier.format_mobile_element","title":"<code>format_mobile_element(feat)</code>","text":"<p>Given a mobile_genetic_element feature, transform it into a transposable_element</p> Source code in <code>src/ensembl/io/genomio/gff3/process.py</code> <pre><code>def format_mobile_element(self, feat: SeqFeature) -&gt; SeqFeature:\n    \"\"\"Given a mobile_genetic_element feature, transform it into a transposable_element\"\"\"\n\n    # Change mobile_genetic_element into a transposable_element feature\n    if feat.type == \"mobile_genetic_element\":\n        mobile_element_type = feat.qualifiers.get(\"mobile_element_type\", [])\n        if mobile_element_type:\n            # Get the type (and name) from the attrib\n            if \":\" in mobile_element_type[0]:\n                element_type, element_name = mobile_element_type[0].split(\":\")\n                description = f\"{element_type} ({element_name})\"\n            else:\n                element_type = mobile_element_type[0]\n                description = element_type\n\n            # Keep the metadata in the description if the type is known\n            if element_type in (\"transposon\", \"retrotransposon\"):\n                feat.type = \"transposable_element\"\n                if not feat.qualifiers.get(\"product\"):\n                    feat.qualifiers[\"product\"] = [description]\n            else:\n                print(f\"Mobile genetic element 'mobile_element_type' is not transposon: {element_type}\")\n                return feat\n        else:\n            print(\"Mobile genetic element does not have a 'mobile_element_type' tag\")\n            return feat\n    elif feat.type == \"transposable_element\":\n        pass\n    else:\n        print(f\"Feature {feat.id} is not a supported TE feature {feat.type}\")\n        return feat\n\n    # Generate ID if needed and add it to the functional annotation\n    feat.id = self.normalize_gene_id(feat)\n    self.annotations.add_feature(feat, \"transposable_element\")\n    feat.qualifiers = {\"ID\": feat.id}\n\n    return feat\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/process/#src.ensembl.io.genomio.gff3.process.GFFSimplifier.gene_to_cds","title":"<code>gene_to_cds(gene)</code>","text":"<p>Returns a list of transcripts (with exons), from a gene with only CDS children.</p> Source code in <code>src/ensembl/io/genomio/gff3/process.py</code> <pre><code>def gene_to_cds(self, gene: SeqFeature) -&gt; List[SeqFeature]:\n    \"\"\"Returns a list of transcripts (with exons), from a gene with only CDS children.\"\"\"\n\n    gene_cds_skip_others = self.gene_cds_skip_others\n    transcripts_dict = {}\n    del_transcript = []\n\n    for count, cds in enumerate(gene.sub_features):\n        if cds.type != \"CDS\":\n            if gene_cds_skip_others:\n                del_transcript.append(count)\n                continue\n            raise GFFParserError(\n                \"Can not create a chain 'transcript - exon - CDS'\"\n                f\" when the gene children are not all CDSs\"\n                f\" ({cds.id} of type {cds.type} is child of gene {gene.id})\"\n            )\n\n        exon = SeqFeature(cds.location, type=\"exon\")\n\n        # Add to transcript or create a new one\n        if cds.id not in transcripts_dict:\n            print(f\"Create new mRNA for {cds.id}\")\n            transcript = self.build_transcript(gene)\n            transcripts_dict[cds.id] = transcript\n        exon.qualifiers[\"source\"] = gene.qualifiers[\"source\"]\n        transcripts_dict[cds.id].sub_features.append(exon)\n        transcripts_dict[cds.id].sub_features.append(cds)\n\n    for elt in sorted(del_transcript, reverse=True):\n        gene.sub_features.pop(elt)\n\n    transcripts = list(transcripts_dict.values())\n\n    return transcripts\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/process/#src.ensembl.io.genomio.gff3.process.GFFSimplifier.gene_to_exon","title":"<code>gene_to_exon(gene)</code>","text":"<p>Returns an intermediary transcript for a gene with direct exon children.</p> Source code in <code>src/ensembl/io/genomio/gff3/process.py</code> <pre><code>def gene_to_exon(self, gene: SeqFeature) -&gt; SeqFeature:\n    \"\"\"Returns an intermediary transcript for a gene with direct exon children.\"\"\"\n\n    transcript = SeqFeature(gene.location, type=\"mRNA\")\n    transcript.qualifiers[\"source\"] = gene.qualifiers[\"source\"]\n    transcript.sub_features = []\n\n    for exon in gene.sub_features:\n        transcript.sub_features.append(exon)\n\n    return transcript\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/process/#src.ensembl.io.genomio.gff3.process.GFFSimplifier.generate_stable_id","title":"<code>generate_stable_id()</code>","text":"<p>Returns a new unique gene stable_id with a prefix.</p> <p>The id is made up of a prefix and a number, which is auto incremented. Define the prefix with the param \"stable_id_prefix\", or use the genome organism_abbrev and prepend \"TMP_\" to it.</p> Source code in <code>src/ensembl/io/genomio/gff3/process.py</code> <pre><code>def generate_stable_id(self) -&gt; str:\n    \"\"\"Returns a new unique gene stable_id with a prefix.\n\n    The id is made up of a prefix and a number, which is auto incremented.\n    Define the prefix with the param \"stable_id_prefix\",\n    or use the genome organism_abbrev and prepend \"TMP_\" to it.\n\n    \"\"\"\n    if self.stable_id_prefix:\n        prefix = self.stable_id_prefix\n    else:\n        if self.genome:\n            org = self.genome.get(\"BRC4\", {}).get(\"organism_abbrev\")\n        if org is None:\n            prefix = \"TMP_PREFIX_\"\n        else:\n            prefix = \"TMP_\" + org + \"_\"\n        self.stable_id_prefix = prefix\n\n    number = self.current_stable_id_number + 1\n    new_id = f\"{prefix}{number}\"\n    self.current_stable_id_number = number\n\n    return new_id\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/process/#src.ensembl.io.genomio.gff3.process.GFFSimplifier.move_cds_to_mrna","title":"<code>move_cds_to_mrna(gene)</code>","text":"<p>Move CDS child features of a gene, to the mRNA.</p> <p>This is to fix the case where we have the following structure: gene -&gt; [ mRNA, CDSs ] and change it to gene -&gt; [ mRNA -&gt; [ CDSs ] ] The mRNA might have exons, in which case check that they match the CDS coordinates.</p> <p>Raises an exception if the feature structure is not recognized.</p> <p>Returns:</p> Type Description <code>SeqFeature</code> <p>The gene where the CDSs have been moved, if needed.</p> Source code in <code>src/ensembl/io/genomio/gff3/process.py</code> <pre><code>def move_cds_to_mrna(self, gene: SeqFeature) -&gt; SeqFeature:\n    \"\"\"Move CDS child features of a gene, to the mRNA.\n\n    This is to fix the case where we have the following structure:\n    gene -&gt; [ mRNA, CDSs ]\n    and change it to\n    gene -&gt; [ mRNA -&gt; [ CDSs ] ]\n    The mRNA might have exons, in which case check that they match the CDS coordinates.\n\n    Raises an exception if the feature structure is not recognized.\n\n    Args:\n        A gene with only one transcript, to check and fix.\n\n    Returns:\n        The gene where the CDSs have been moved, if needed.\n\n    \"\"\"\n    # First, count the types\n    mrnas = []\n    cdss = []\n\n    gene_subf_clean = []\n    for subf in gene.sub_features:\n        if subf.type == \"mRNA\":\n            mrnas.append(subf)\n        elif subf.type == \"CDS\":\n            cdss.append(subf)\n        else:\n            gene_subf_clean.append(subf)\n\n    if len(cdss) == 0:\n        # Nothing to fix here, no CDSs to move\n        return gene\n    if len(mrnas) &gt; 1:\n        raise GFFParserError(\n            f\"Can't fix gene {gene.id}: contains several mRNAs and CDSs, all children of the gene\"\n        )\n\n    mrna = mrnas[0]\n\n    # Check if there are exons (or CDSs) under the mRNA\n    sub_exons = []\n    sub_cdss = []\n    for subf in mrna.sub_features:\n        if subf.type == \"CDS\":\n            sub_cdss.append(subf)\n        elif subf.type == \"exon\":\n            sub_exons.append(subf)\n\n    self._check_sub_cdss(gene, sub_cdss)\n    self._check_sub_exons(gene, cdss, sub_exons)\n\n    print(f\"Gene {gene.id}: move {len(cdss)} CDSs to the mRNA\")\n    # No more issues? move the CDSs\n    mrna.sub_features += cdss\n    # And remove them from the gene\n    gene.sub_features = gene_subf_clean\n    gene.sub_features.append(mrna)\n\n    return gene\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/process/#src.ensembl.io.genomio.gff3.process.GFFSimplifier.normalize_cds_id","title":"<code>normalize_cds_id(cds_id)</code>","text":"<p>Check the CDS ID is proper: - Remove any unnecessary prefixes around the CDS ID - Delete the ID if it is not proper</p> Source code in <code>src/ensembl/io/genomio/gff3/process.py</code> <pre><code>def normalize_cds_id(self, cds_id: str) -&gt; str:\n    \"\"\"\n    Check the CDS ID is proper:\n    - Remove any unnecessary prefixes around the CDS ID\n    - Delete the ID if it is not proper\n    \"\"\"\n\n    prefixes = [\"cds-\", \"cds:\"]\n    cds_id = self.remove_prefixes(cds_id, prefixes)\n\n    # Special case: if the ID doesn't look like one, remove it\n    # It needs to be regenerated\n    if not self.valid_id(cds_id):\n        cds_id = \"\"\n\n    return cds_id\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/process/#src.ensembl.io.genomio.gff3.process.GFFSimplifier.normalize_gene","title":"<code>normalize_gene(gene, fail_types)</code>","text":"<p>Returns a normalized gene structure, separate from the functional elements.</p> <p>Parameters:</p> Name Type Description Default <code>gene</code> <code>SeqFeature</code> <p>Gene object to normalize.</p> required <code>functional_annotation</code> <p>List of feature annotations (appended by this method).</p> required <code>fail_types</code> <code>Dict[str, int]</code> <p>List of feature types that are not supported (appended by this method).</p> required Source code in <code>src/ensembl/io/genomio/gff3/process.py</code> <pre><code>def normalize_gene(self, gene: SeqFeature, fail_types: Dict[str, int]) -&gt; SeqFeature:\n    \"\"\"Returns a normalized gene structure, separate from the functional elements.\n\n    Args:\n        gene: Gene object to normalize.\n        functional_annotation: List of feature annotations (appended by this method).\n        fail_types: List of feature types that are not supported (appended by this method).\n\n    \"\"\"\n\n    # New gene ID\n    gene.id = self.normalize_gene_id(gene)\n\n    # Gene with no subfeatures: need to create a transcript at least\n    if len(gene.sub_features) == 0:\n        print(f\"Insert transcript for lone gene {gene.id}\")\n        transcript = self.transcript_for_gene(gene)\n        gene.sub_features = [transcript]\n\n    # Count features\n    fcounter = Counter([feat.type for feat in gene.sub_features])\n\n    # Transform gene - CDS to gene-transcript-exon-CDS\n    if len(fcounter) == 1:\n        if fcounter.get(\"CDS\"):\n            num_subs = len(gene.sub_features)\n            print(f\"Insert transcript-exon feats for {gene.id} ({num_subs} CDSs)\")\n            transcripts = self.gene_to_cds(gene)\n            gene.sub_features = transcripts\n\n        # Transform gene - exon to gene-transcript-exon\n        elif fcounter.get(\"exon\"):\n            num_subs = len(gene.sub_features)\n            print(f\"Insert transcript for {gene.id} ({num_subs} exons)\")\n            transcript = self.gene_to_exon(gene)\n            gene.sub_features = [transcript]\n    else:\n        # Check that we don't mix\n        if fcounter.get(\"mRNA\") and fcounter.get(\"CDS\"):\n            # Move CDS(s) from parent gene to parent mRNA if needed\n            gene = self.move_cds_to_mrna(gene)\n        if fcounter.get(\"mRNA\") and fcounter.get(\"exon\"):\n            # Special case with extra exons\n            gene = self.clean_extra_exons(gene)\n\n    # Remove CDS from pseudogenes\n    if gene.type == \"pseudogene\" and not self.allow_pseudogene_with_CDS:\n        self.remove_cds_from_pseudogene(gene)\n\n    # TRANSCRIPTS\n    gene = self._normalize_transcripts(gene, fail_types)\n\n    # PSEUDOGENE CDS IDs\n    if gene.type == \"pseudogene\" and self.allow_pseudogene_with_CDS:\n        self.normalize_pseudogene_cds(gene)\n\n    # Finally, store gene functional annotation\n    self.annotations.add_feature(gene, \"gene\")\n\n    # replace qualifiers\n    old_gene_qualifiers = gene.qualifiers\n    gene.qualifiers = {\"ID\": gene.id, \"source\": old_gene_qualifiers[\"source\"]}\n\n    return gene\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/process/#src.ensembl.io.genomio.gff3.process.GFFSimplifier.normalize_gene_id","title":"<code>normalize_gene_id(gene)</code>","text":"<p>Remove any unnecessary prefixes around the gene ID.</p> <p>Generate a new stable id if it is not recognized as valid.</p> <p>Parameters:</p> Name Type Description Default <code>gene</code> <code>SeqFeature</code> <p>Gene feature to normalize.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A normalized gene id.</p> Source code in <code>src/ensembl/io/genomio/gff3/process.py</code> <pre><code>def normalize_gene_id(self, gene: SeqFeature) -&gt; str:\n    \"\"\"Remove any unnecessary prefixes around the gene ID.\n\n    Generate a new stable id if it is not recognized as valid.\n\n    Args:\n        gene: Gene feature to normalize.\n\n    Returns:\n        A normalized gene id.\n\n    \"\"\"\n\n    prefixes = [\"gene-\", \"gene:\"]\n    new_gene_id = self.remove_prefixes(gene.id, prefixes)\n\n    # In case the gene id is not valid, use the GeneID\n    if not self.valid_id(new_gene_id):\n        print(f\"Gene id is not valid: {new_gene_id}\")\n        qual = gene.qualifiers\n        if \"Dbxref\" in qual:\n            for xref in qual[\"Dbxref\"]:\n                (db, value) = xref.split(\":\")\n                if db == \"GeneID\":\n                    new_gene_id = f\"{db}_{value}\"\n                    print(f\"Using GeneID {new_gene_id} for stable_id instead of {gene.id}\")\n                    return new_gene_id\n\n        # Make a new stable_id\n        if self.make_missing_stable_ids:\n            new_id = self.generate_stable_id()\n            print(f\"New id: {new_gene_id} -&gt; {new_id}\")\n            return new_id\n        raise GFFParserError(f\"Can't use invalid gene id for {gene}\")\n\n    return new_gene_id\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/process/#src.ensembl.io.genomio.gff3.process.GFFSimplifier.normalize_pseudogene_cds","title":"<code>normalize_pseudogene_cds(gene)</code>","text":"<p>Ensure CDS from a pseudogene have a proper ID - different from the gene - derived from the gene if it is not proper</p> Source code in <code>src/ensembl/io/genomio/gff3/process.py</code> <pre><code>def normalize_pseudogene_cds(self, gene: SeqFeature) -&gt; None:\n    \"\"\"Ensure CDS from a pseudogene have a proper ID\n    - different from the gene\n    - derived from the gene if it is not proper\n    \"\"\"\n\n    for transcript in gene.sub_features:\n        for feat in transcript.sub_features:\n            if feat.type == \"CDS\":\n                feat.id = self.normalize_cds_id(feat.id)\n                if feat.id in (\"\", gene.id):\n                    feat.id = f\"{transcript.id}_cds\"\n                    feat.qualifiers[\"ID\"] = feat.id\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/process/#src.ensembl.io.genomio.gff3.process.GFFSimplifier.normalize_transcript_id","title":"<code>normalize_transcript_id(gene_id, number)</code>","text":"<p>Use a gene ID and a number to make a formatted transcript ID.</p> Source code in <code>src/ensembl/io/genomio/gff3/process.py</code> <pre><code>def normalize_transcript_id(self, gene_id: str, number: int) -&gt; str:\n    \"\"\"Use a gene ID and a number to make a formatted transcript ID.\"\"\"\n\n    transcript_id = f\"{gene_id}_t{number}\"\n    return transcript_id\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/process/#src.ensembl.io.genomio.gff3.process.GFFSimplifier.remove_cds_from_pseudogene","title":"<code>remove_cds_from_pseudogene(gene)</code>","text":"<p>Remove CDS from a pseudogene This assumes the CDSs are sub features of the transcript or the gene</p> Source code in <code>src/ensembl/io/genomio/gff3/process.py</code> <pre><code>def remove_cds_from_pseudogene(self, gene: SeqFeature) -&gt; None:\n    \"\"\"Remove CDS from a pseudogene\n    This assumes the CDSs are sub features of the transcript or the gene\n    \"\"\"\n\n    gene_subfeats = []\n    for transcript in gene.sub_features:\n        if transcript.type == \"CDS\":\n            print(f\"Remove pseudo CDS {transcript.id}\")\n            continue\n        new_subfeats = []\n        for feat in transcript.sub_features:\n            if feat.type == \"CDS\":\n                print(f\"Remove pseudo CDS {feat.id}\")\n                continue\n            new_subfeats.append(feat)\n        transcript.sub_features = new_subfeats\n        gene_subfeats.append(transcript)\n    gene.sub_features = gene_subfeats\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/process/#src.ensembl.io.genomio.gff3.process.GFFSimplifier.remove_prefixes","title":"<code>remove_prefixes(identifier, prefixes)</code>","text":"<p>Remove prefixes from an identifier if they are found Return the unaltered identifier otherwise</p> Source code in <code>src/ensembl/io/genomio/gff3/process.py</code> <pre><code>def remove_prefixes(self, identifier: str, prefixes: List[str]) -&gt; str:\n    \"\"\"\n    Remove prefixes from an identifier if they are found\n    Return the unaltered identifier otherwise\n    \"\"\"\n    for prefix in prefixes:\n        if identifier.startswith(prefix):\n            identifier = identifier[len(prefix) :]\n    return identifier\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/process/#src.ensembl.io.genomio.gff3.process.GFFSimplifier.simpler_gff3","title":"<code>simpler_gff3(in_gff_path)</code>","text":"<p>Load a GFF3 from INSDC and rewrite it in a simpler version, and also write a functional_annotation file</p> Source code in <code>src/ensembl/io/genomio/gff3/process.py</code> <pre><code>def simpler_gff3(self, in_gff_path: PathLike) -&gt; None:\n    \"\"\"\n    Load a GFF3 from INSDC and rewrite it in a simpler version,\n    and also write a functional_annotation file\n    \"\"\"\n\n    allowed_gene_types = self.gene_types\n    ignored_gene_types = self.ignored_gene_types\n    transcript_types = self.transcript_types\n    allowed_non_gene_types = self.non_gene_types\n    skip_unrecognized = self.skip_unrecognized\n    to_exclude = self.exclude_seq_regions\n\n    with Path(in_gff_path).open(\"r\") as in_gff_fh:\n        fail_types: Dict[str, int] = {}\n\n        for record in GFF.parse(in_gff_fh):\n            new_record = SeqRecord(record.seq, id=record.id)\n            if record.id in to_exclude:\n                print(f\"Skip seq_region {record.id}\")\n                continue\n\n            # Root features (usually genes)\n            for feat in record.features:\n                # Skip or format depending on the feature type\n                if feat.type in ignored_gene_types:\n                    continue\n                if feat.type in transcript_types:\n                    feat = self.transcript_gene(feat)\n                elif feat.type == \"CDS\":\n                    feat = self.cds_gene(feat)\n                elif feat.type in (\"mobile_genetic_element\", \"transposable_element\"):\n                    feat = self.format_mobile_element(feat)\n\n                # Normalize the gene structure\n                if feat.type in allowed_gene_types:\n                    feat = self.normalize_gene(feat, fail_types)\n                elif feat.type in allowed_non_gene_types:\n                    pass\n                else:\n                    fail_types[\"gene=\" + feat.type] = 1\n                    message = f\"Unsupported feature type: {feat.type} (for {feat.id})\"\n                    print(message)\n                    if skip_unrecognized:\n                        del feat\n                        continue\n\n                new_record.features.append(feat)\n            self.records.append(new_record)\n\n        if fail_types and not skip_unrecognized:\n            fail_errors = \"\\n   \".join(fail_types.keys())\n            raise GFFParserError(f\"Unrecognized types found:\\n   {fail_errors}\")\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/process/#src.ensembl.io.genomio.gff3.process.GFFSimplifier.transcript_for_gene","title":"<code>transcript_for_gene(gene)</code>","text":"<p>Returns a transcript, from a gene without one.</p> Source code in <code>src/ensembl/io/genomio/gff3/process.py</code> <pre><code>def transcript_for_gene(self, gene: SeqFeature) -&gt; SeqFeature:\n    \"\"\"Returns a transcript, from a gene without one.\"\"\"\n\n    transcript = SeqFeature(gene.location, type=\"transcript\")\n    transcript.qualifiers[\"source\"] = gene.qualifiers[\"source\"]\n    transcript.sub_features = []\n\n    return transcript\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/process/#src.ensembl.io.genomio.gff3.process.GFFSimplifier.transcript_gene","title":"<code>transcript_gene(ncrna)</code>","text":"<p>Create a gene for lone transcripts: 'gene' for tRNA/rRNA, and 'ncRNA' for all others</p> <p>Parameters:</p> Name Type Description Default <code>ncrna</code> <code>SeqFeature</code> <p>the transcript for which we want to create a gene.</p> required <p>Returns:</p> Type Description <code>SeqFeature</code> <p>The gene that contains the transcript.</p> Source code in <code>src/ensembl/io/genomio/gff3/process.py</code> <pre><code>def transcript_gene(self, ncrna: SeqFeature) -&gt; SeqFeature:\n    \"\"\"Create a gene for lone transcripts: 'gene' for tRNA/rRNA, and 'ncRNA' for all others\n\n    Args:\n        ncrna: the transcript for which we want to create a gene.\n\n    Returns:\n        The gene that contains the transcript.\n\n    \"\"\"\n    new_type = \"ncRNA_gene\"\n    if ncrna.type in (\"tRNA\", \"rRNA\"):\n        new_type = \"gene\"\n    print(f\"Put the transcript {ncrna.type} in a {new_type} parent feature\")\n    gene = SeqFeature(ncrna.location, type=new_type)\n    gene.qualifiers[\"source\"] = ncrna.qualifiers[\"source\"]\n    gene.sub_features = [ncrna]\n    gene.id = ncrna.id\n\n    return gene\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/process/#src.ensembl.io.genomio.gff3.process.GFFSimplifier.valid_id","title":"<code>valid_id(name)</code>","text":"<p>Check that the format of a stable id is valid.</p> Source code in <code>src/ensembl/io/genomio/gff3/process.py</code> <pre><code>def valid_id(self, name: str) -&gt; bool:\n    \"\"\"Check that the format of a stable id is valid.\"\"\"\n\n    if not self.validate_gene_id:\n        return True\n\n    min_length = self.min_id_length\n\n    # Trna (from tRNAscan)\n    if re.search(r\"^Trna\", name):\n        print(f\"Stable id is a Trna from tRNA-scan: {name}\")\n        return False\n\n    # Coordinates\n    if re.search(r\"^.+:\\d+..\\d+\", name):\n        print(f\"Stable id is a coordinate: {name}\")\n        return False\n\n    # Special characters\n    if re.search(r\"[ |]\", name):\n        print(f\"Stable id contains special characters: {name}\")\n        return False\n\n    # Min length\n    if len(name) &lt; min_length:\n        print(f\"Stable id is too short (&lt;{min_length}) {name}\")\n        return False\n\n    return True\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/process/#src.ensembl.io.genomio.gff3.process.Records","title":"<code>Records</code>","text":"<p>             Bases: <code>list</code></p> <p>List of GFF3 SeqRecords.</p> Source code in <code>src/ensembl/io/genomio/gff3/process.py</code> <pre><code>class Records(list):\n    \"\"\"List of GFF3 SeqRecords.\"\"\"\n\n    def to_gff(self, out_gff_path: PathLike) -&gt; None:\n        \"\"\"Print out the current list of records in a GFF3 file.\n\n        Args:\n            out_gff_path: Path to GFF3 file where to write the records.\n        \"\"\"\n        with Path(out_gff_path).open(\"w\") as out_gff_fh:\n            GFF.write(self, out_gff_fh)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/process/#src.ensembl.io.genomio.gff3.process.Records.to_gff","title":"<code>to_gff(out_gff_path)</code>","text":"<p>Print out the current list of records in a GFF3 file.</p> <p>Parameters:</p> Name Type Description Default <code>out_gff_path</code> <code>PathLike</code> <p>Path to GFF3 file where to write the records.</p> required Source code in <code>src/ensembl/io/genomio/gff3/process.py</code> <pre><code>def to_gff(self, out_gff_path: PathLike) -&gt; None:\n    \"\"\"Print out the current list of records in a GFF3 file.\n\n    Args:\n        out_gff_path: Path to GFF3 file where to write the records.\n    \"\"\"\n    with Path(out_gff_path).open(\"w\") as out_gff_fh:\n        GFF.write(self, out_gff_fh)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/gff3/process/#src.ensembl.io.genomio.gff3.process.main","title":"<code>main()</code>","text":"<p>Main script entry-point.</p> Source code in <code>src/ensembl/io/genomio/gff3/process.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main script entry-point.\"\"\"\n    parser = ArgumentParser(\n        description=(\n            \"Standardize the gene model representation of a GFF3 file, and extract the functional \"\n            \"annotation in a separate file.\"\n        )\n    )\n    parser.add_argument_src_path(\"--in_gff_path\", required=True, help=\"Input GFF3 file\")\n    parser.add_argument_src_path(\"--genome_data\", required=True, help=\"Genome JSON file\")\n    parser.add_argument(\n        \"--make_missing_stable_ids\", action=\"store_true\", help=\"Generate stable IDs when missing or invalid\"\n    )\n    parser.add_argument_dst_path(\"--out_gff_path\", default=Path(\"gene_models.gff3\"), help=\"Output GFF3 file\")\n    parser.add_argument_dst_path(\n        \"--out_func_path\",\n        default=Path(\"functional_annotation.json\"),\n        help=\"Output functional annotation JSON file\",\n    )\n    args = parser.parse_args()\n\n    # Merge multiline gene features in a separate file\n    interim_gff_path = Path(f\"{args.in_gff_path}_INTERIM_MERGE\")\n    merger = GFFGeneMerger()\n    merged_genes = merger.merge(args.in_gff_path, interim_gff_path)\n    num_merged_genes = len(merged_genes)\n    in_gff_path = args.in_gff_path\n    # If there are split genes, decide to merge, or just die\n    if num_merged_genes &gt; 0:\n        # Report the list of merged genes in case something does not look right\n        print(f\"{num_merged_genes} genes merged:\\n\" + \"\\n\".join(merged_genes))\n        # Use the GFF with the merged genes for the next part\n        in_gff_path = interim_gff_path\n\n    # Load GFF3 data and write a simpler version that follows our specifications as well as a\n    # functional annotation JSON file\n    gff_data = GFFSimplifier(args.genome_data, args.make_missing_stable_ids)\n    gff_data.simpler_gff3(in_gff_path)\n    gff_data.records.to_gff(args.out_gff_path)\n    gff_data.annotations.to_json(args.out_func_path)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/","title":"manifest","text":"<p>Manifest files handling module.</p>"},{"location":"reference/ensembl/io/genomio/manifest/check_integrity/","title":"check_integrity","text":"<p>Compare the genomic data in a DNA fasta file, seq_region json, gene models GFF3 and peptide fasta to ensure their contents are in sync.</p>"},{"location":"reference/ensembl/io/genomio/manifest/check_integrity/#src.ensembl.io.genomio.manifest.check_integrity.IntegrityTool","title":"<code>IntegrityTool</code>","text":"<p>Check the integrity of sequence and annotation files in the genome</p> Source code in <code>src/ensembl/io/genomio/manifest/check_integrity.py</code> <pre><code>class IntegrityTool:\n    \"\"\"Check the integrity of sequence and annotation files in the genome\"\"\"\n\n    def __init__(self, manifest_file: Path, brc_mode: bool = False, ignore_final_stops: bool = False) -&gt; None:\n        self.manifest = Manifest(manifest_file)\n        self.brc_mode = False\n        self.set_brc_mode(brc_mode)\n        self.ignore_final_stops = False\n        self.set_ignore_final_stops(ignore_final_stops)\n        self.errors: List[str] = []\n\n    def add_errors(self, *args: str) -&gt; None:\n        \"\"\"Store the given errors in the list.\"\"\"\n        for error in args:\n            self.errors += error\n\n    def check_integrity(self):\n        \"\"\"Load files listed in the manifest.json and check the integrity.\n        Check if the files are correct by verifying the MD5 hash.\n        Check if translation, functional annotation and sequence region ids\n        and lengths are consistent with the information in gff.\n        Compare sequence length from fasta_dna file to seq_region.json metadata.\n        \"\"\"\n\n        errors = []\n\n        # Load the manifest integrity counts\n        manifest = self.manifest\n        manifest.prepare_integrity_data()\n\n        genome = manifest.genome\n        seq_regions = manifest.seq_regions\n\n        dna = manifest.get_lengths(\"dna_sequences\")\n        pep = manifest.get_lengths(\"peptide_sequences\")\n        seq_lengths = manifest.get_lengths(\"seq_regions\")\n        seq_circular = manifest.get_circular(\"seq_regions\")\n\n        agp_seqr = manifest.get_lengths(\"agp\")\n\n        # Then, run the checks\n        self._check_genome(genome)\n\n        # Check gff3\n        if manifest.has_lengths(\"gff_genes\"):\n            gff_genes = manifest.get_lengths(\"gff_genes\")\n            gff_seq_regions = manifest.get_lengths(\"gff_seq_regions\")\n            gff_translations = manifest.get_lengths(\"gff_translations\")\n            gff_all_translations = manifest.get_lengths(\"gff_all_translations\")\n            gff_transposable_elements = manifest.get_lengths(\"gff_transposable_elements\")\n\n            ann_genes = manifest.get_lengths(\"ann_genes\")\n            ann_translations = manifest.get_lengths(\"ann_translations\")\n            ann_transposable_elements = manifest.get_lengths(\"ann_transposable_elements\")\n\n            # Check fasta_pep.fa integrity\n            # The sequence length and id retrieved from the fasta_pep file\n            # and compared to the translated CDS id and length in the gff\n            # We don't compare the peptide lengths because of seqedits\n            if pep:\n                tr_errors = self.check_lengths(\n                    pep, gff_translations, \"Fasta translations vs gff\", special_diff=True\n                )\n                if len(tr_errors) &gt; 0:\n                    # The pseudo CDSs are included in this check\n                    # Pseudo CDSs are not translated, if the pseudo translation ids are not ignored\n                    # in the gff it will give an error\n                    tr_errors = self.check_lengths(\n                        pep,\n                        gff_all_translations,\n                        \"Fasta translations vs gff (include pseudo CDS)\",\n                        special_diff=True,\n                    )\n                    self.add_errors(*tr_errors)\n\n            # Check functional_annotation.json integrity\n            # Gene ids, translated CDS ids and translated CDSs\n            # including pseudogenes are compared to the gff\n            if ann_genes:\n                errors += self.check_ids(ann_genes, gff_genes, \"Gene ids metadata vs gff\")\n                tr_errors = self.check_ids(\n                    ann_translations, gff_translations, \"Translation ids metadata vs gff\"\n                )\n                if len(tr_errors) &gt; 0:\n                    tr_errors = self.check_ids(\n                        ann_translations,\n                        gff_all_translations,\n                        \"Translation ids metadata vs gff (include pseudo CDS)\",\n                    )\n                self.add_errors(*tr_errors)\n                errors += self.check_ids(\n                    ann_transposable_elements,\n                    gff_transposable_elements,\n                    \"TE ids metadata vs gff\",\n                )\n\n            # Check the seq.json intregrity\n            # Compare the length and id retrieved from seq.json to the gff\n            if seq_regions:\n                self.check_seq_region_lengths(\n                    seq_lengths, gff_seq_regions, \"Seq_regions metadata vs gff\", seq_circular\n                )\n\n        # Check fasta dna and seq_region integrity\n        if dna and seq_regions:\n            self.check_seq_region_lengths(seq_lengths, dna, \"seq_regions json vs dna\")\n\n        # Check agp and seq_region integrity\n        if agp_seqr and seq_lengths:\n            self.check_seq_region_lengths(seq_lengths, agp_seqr, \"seq_regions json vs agps\")\n\n        if errors:\n            errors_str = \"\\n\".join(errors)\n            raise InvalidIntegrityError(f\"Integrity test failed:\\n{errors_str}\")\n\n    def set_brc_mode(self, brc_mode: bool) -&gt; None:\n        \"\"\"Set brc mode for this tool and the manifest.\"\"\"\n        self.brc_mode = brc_mode\n        self.manifest.brc_mode = brc_mode\n\n    def set_ignore_final_stops(self, ignore_final_stops: bool) -&gt; None:\n        \"\"\"Set ignore_final_stops (when calculating peptide length) for this tool and the manifest.\"\"\"\n        self.ignore_final_stops = ignore_final_stops\n        self.manifest.ignore_final_stops = ignore_final_stops\n\n    def _check_genome(self, genome: Dict) -&gt; None:\n        \"\"\"Check if the accession is correct in genome.json.\"\"\"\n        if genome:\n            if \"assembly\" in genome:\n                genome_ass = genome[\"assembly\"]\n                if \"accession\" in genome_ass:\n                    genome_acc = genome_ass[\"accession\"]\n                    if not re.match(r\"GC[AF]_\\d{9}(\\.\\d+)?\", genome_acc):\n                        self.add_errors(f\"Genome assembly accession is wrong: '{genome_acc}'\")\n\n    def check_ids(self, list1, list2, name):\n        \"\"\"Compare the ids in list1 and list2.\n\n        Args:\n            list1: dict containing sequence ids retrieved from functional.json.\n            list2: dict containing length and id in the retrieved from the gff.\n            name:  string\n\n        Return:\n            Error if the ids in functional.json and gff do not match.\n        \"\"\"\n\n        only1 = []\n        only2 = []\n        common = []\n\n        for item_id in list1:\n            if item_id in list2:\n                common.append(item_id)\n            else:\n                only1.append(item_id)\n        for item_id in list2:\n            if item_id not in common:\n                only2.append(item_id)\n\n        errors = []\n        if common:\n            print(f\"{len(common)} common elements in {name}\")\n        if only1:\n            errors.append(f\"{len(only1)} only in first list in {name} (first: {only1[0]})\")\n        if only2:\n            errors.append(f\"{len(only2)} only in second list in {name} (first: {only2[0]})\")\n\n        return errors\n\n    def check_lengths(self, list1, list2, name, allowed_len_diff=None, special_diff=False):\n        \"\"\"Check the difference in ids and length between list1 and list2.\n            There are a few special cases here where we allow a certain asymmetry\n            by changing the values of the arguments.\n\n        Args:\n            list1: dict containing length and id of the sequence from fasta files.\n            list2: dict containing length and id in the retrieved from the gff.\n            name:  string\n\n        allowed_len_diff : None to to not accept differences in length between list1 and list2.\n            The value can be changed based on how much difference in sequence length we are wanting to accept.\n\n        special_diff: set as False when no special length difference is expected between the lists.\n                    This can be changed if we want to report common sequences with 1 BP difference.\n\n        Returns:\n            Error if there is a difference in length or ids between the lists.\n        \"\"\"\n\n        # check list diffferences, checks if abs(values diff) &lt; allowed_len_diff\n\n        set1 = frozenset(list1)\n        set2 = frozenset(list2)\n        list1_2 = list(set1 - set2)\n        list2_1 = list(set2 - set1)\n\n        errors = []\n        if len(list1_2) &gt; 0:\n            errors.append(f\"{name}: {len(list1_2)} from the first list only (i.e. {list1_2[0]})\")\n        if len(list2_1) &gt; 0:\n            errors.append(f\"{name}: {len(list2_1)} from the second list only (i.e. {list2_1[0]})\")\n\n        common_len = 0\n        if allowed_len_diff is None:\n            common_len = len(set1 &amp; set2)\n        else:\n            # check for the sequence length difference\n            diff_len_list = []\n            diff_len_special_list = []\n            for e in set1 &amp; set2:\n                dl12 = list1[e] - list2[e]\n                if abs(dl12) &lt;= allowed_len_diff:\n                    common_len += 1\n                else:\n                    _dlist = diff_len_list\n                    # Special case: 1 AA /BP shorter,\n                    #   so assuming the stop codon is not included in the CDS (when it should be)\n                    if dl12 == 1 and special_diff:\n                        _dlist = diff_len_special_list\n                    _dlist.append(f\"{e}: {list1[e]}, {list2[e]}\")\n            if diff_len_special_list:\n                errors.append(\n                    (\n                        f\"{len(diff_len_special_list)} common elements with one BP/AA length diff for {name}\"\n                        f\"(e.g. {diff_len_special_list[0]})\"\n                    )\n                )\n            if diff_len_list:\n                errors.append(\n                    (\n                        f\"{len(diff_len_list)} common elements with length diff for {name}\"\n                        f\"(e.g. {diff_len_list[0]})\"\n                    )\n                )\n        if common_len &gt; 0:\n            print(f\"{common_len} common elements between lists for {name}\", file=sys.stderr)\n\n        return errors\n\n    def check_seq_region_lengths(\n        self,\n        seqrs: Dict[str, Any],\n        feats: Dict[str, Any],\n        name: str,\n        circular: Optional[Dict[str, Any]] = None,\n    ) -&gt; None:\n        \"\"\"Check the integrity of seq_region.json file by comparing the length of the sequence\n            to fasta files and the gff.\n\n            Seq_region file is in json format containing the metadata of the sequence.\n            It contains sequence id, length, location and the synonyms for the sequence name\n            from different sources.\n\n        Args:\n            seqs: Sequence name and length retrieved from seq_region.json file.\n            feats: Sequence name and length retrieved from the fasta and gff file.\n            name: Name of the check to show in the logs.\n            circular: Whether any sequence is circular.\n\n        Returns:\n            Error if there are common sequences with difference in ids\n            and if the sequences are not consistent in the files.\n        \"\"\"\n        comp = self._compare_seqs(seqrs, feats, circular)\n\n        common = comp[\"common\"]\n        diff = comp[\"diff\"]\n        diff_circular = comp[\"diff_circular\"]\n        only_seqr = comp[\"only_seqr\"]\n        only_feat = comp[\"only_feat\"]\n\n        if common:\n            print(f\"{len(common)} common elements in {name}\")\n        if diff_circular:\n            example = diff_circular[0]\n            print(f\"{len(diff_circular)} differences for circular elements in {name} (e.g. {example})\")\n        if diff:\n            self.add_errors(f\"{len(diff)} common elements with higher length in {name} (e.g. {diff[0]})\")\n        if only_seqr:\n            # Not an error!\n            print(f\"{len(only_seqr)} only in seq_region list in {name} (first: {only_seqr[0]})\")\n        if only_feat:\n            self.add_errors(f\"{len(only_feat)} only in second list in {name} (first: {only_feat[0]})\")\n\n    def _compare_seqs(\n        self, seqrs: Dict[str, Any], feats: Dict[str, Any], circular: Optional[Dict[str, Any]] = None\n    ) -&gt; Dict[str, List[str]]:\n        \"\"\"Give the intersection and other comparison between two groups of sequences.\n\n        Args:\n            seqs: Sequence name and length retrieved from seq_region.json file.\n            feats: Sequence name and length retrieved from the fasta and gff file.\n            circular: Whether any sequence is circular.\n\n        Returns: Dict with 5 stats:\n            common: Common elements.\n            only_seqr: Elements only in the first one.\n            only_feat: Elements only in the second one.\n            diff: Elements that differ.\n            diff_circular: Elements that differ in a circular sequence.\n\n        \"\"\"\n        comp: Dict[str, List[str]] = {\n            \"common\": [],\n            \"only_seqr\": [],\n            \"only_feat\": [],\n            \"diff\": [],\n            \"diff_circular\": [],\n        }\n\n        for seq_id in seqrs:\n            if seq_id in feats:\n                # Check that feature is within the seq_region length\n                if feats[seq_id] &gt; seqrs[seq_id]:\n                    diff_str = f\"{seq_id}: {seqrs[seq_id]} vs {feats[seq_id]}\"\n                    if circular and circular.get(seq_id, False):\n                        comp[\"diff_circular\"].append(diff_str)\n                    else:\n                        comp[\"diff\"].append(diff_str)\n                else:\n                    comp[\"common\"].append(seq_id)\n            else:\n                comp[\"only_seqr\"].append(seq_id)\n\n        for seq_id in feats:\n            if (\n                seq_id not in comp[\"common\"]\n                and seq_id not in comp[\"diff\"]\n                and seq_id not in comp[\"diff_circular\"]\n            ):\n                comp[\"only_feat\"].append(seq_id)\n\n        return comp\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/check_integrity/#src.ensembl.io.genomio.manifest.check_integrity.IntegrityTool.add_errors","title":"<code>add_errors(*args)</code>","text":"<p>Store the given errors in the list.</p> Source code in <code>src/ensembl/io/genomio/manifest/check_integrity.py</code> <pre><code>def add_errors(self, *args: str) -&gt; None:\n    \"\"\"Store the given errors in the list.\"\"\"\n    for error in args:\n        self.errors += error\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/check_integrity/#src.ensembl.io.genomio.manifest.check_integrity.IntegrityTool.check_ids","title":"<code>check_ids(list1, list2, name)</code>","text":"<p>Compare the ids in list1 and list2.</p> <p>Parameters:</p> Name Type Description Default <code>list1</code> <p>dict containing sequence ids retrieved from functional.json.</p> required <code>list2</code> <p>dict containing length and id in the retrieved from the gff.</p> required <code>name</code> <p>string</p> required Return <p>Error if the ids in functional.json and gff do not match.</p> Source code in <code>src/ensembl/io/genomio/manifest/check_integrity.py</code> <pre><code>def check_ids(self, list1, list2, name):\n    \"\"\"Compare the ids in list1 and list2.\n\n    Args:\n        list1: dict containing sequence ids retrieved from functional.json.\n        list2: dict containing length and id in the retrieved from the gff.\n        name:  string\n\n    Return:\n        Error if the ids in functional.json and gff do not match.\n    \"\"\"\n\n    only1 = []\n    only2 = []\n    common = []\n\n    for item_id in list1:\n        if item_id in list2:\n            common.append(item_id)\n        else:\n            only1.append(item_id)\n    for item_id in list2:\n        if item_id not in common:\n            only2.append(item_id)\n\n    errors = []\n    if common:\n        print(f\"{len(common)} common elements in {name}\")\n    if only1:\n        errors.append(f\"{len(only1)} only in first list in {name} (first: {only1[0]})\")\n    if only2:\n        errors.append(f\"{len(only2)} only in second list in {name} (first: {only2[0]})\")\n\n    return errors\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/check_integrity/#src.ensembl.io.genomio.manifest.check_integrity.IntegrityTool.check_integrity","title":"<code>check_integrity()</code>","text":"<p>Load files listed in the manifest.json and check the integrity. Check if the files are correct by verifying the MD5 hash. Check if translation, functional annotation and sequence region ids and lengths are consistent with the information in gff. Compare sequence length from fasta_dna file to seq_region.json metadata.</p> Source code in <code>src/ensembl/io/genomio/manifest/check_integrity.py</code> <pre><code>def check_integrity(self):\n    \"\"\"Load files listed in the manifest.json and check the integrity.\n    Check if the files are correct by verifying the MD5 hash.\n    Check if translation, functional annotation and sequence region ids\n    and lengths are consistent with the information in gff.\n    Compare sequence length from fasta_dna file to seq_region.json metadata.\n    \"\"\"\n\n    errors = []\n\n    # Load the manifest integrity counts\n    manifest = self.manifest\n    manifest.prepare_integrity_data()\n\n    genome = manifest.genome\n    seq_regions = manifest.seq_regions\n\n    dna = manifest.get_lengths(\"dna_sequences\")\n    pep = manifest.get_lengths(\"peptide_sequences\")\n    seq_lengths = manifest.get_lengths(\"seq_regions\")\n    seq_circular = manifest.get_circular(\"seq_regions\")\n\n    agp_seqr = manifest.get_lengths(\"agp\")\n\n    # Then, run the checks\n    self._check_genome(genome)\n\n    # Check gff3\n    if manifest.has_lengths(\"gff_genes\"):\n        gff_genes = manifest.get_lengths(\"gff_genes\")\n        gff_seq_regions = manifest.get_lengths(\"gff_seq_regions\")\n        gff_translations = manifest.get_lengths(\"gff_translations\")\n        gff_all_translations = manifest.get_lengths(\"gff_all_translations\")\n        gff_transposable_elements = manifest.get_lengths(\"gff_transposable_elements\")\n\n        ann_genes = manifest.get_lengths(\"ann_genes\")\n        ann_translations = manifest.get_lengths(\"ann_translations\")\n        ann_transposable_elements = manifest.get_lengths(\"ann_transposable_elements\")\n\n        # Check fasta_pep.fa integrity\n        # The sequence length and id retrieved from the fasta_pep file\n        # and compared to the translated CDS id and length in the gff\n        # We don't compare the peptide lengths because of seqedits\n        if pep:\n            tr_errors = self.check_lengths(\n                pep, gff_translations, \"Fasta translations vs gff\", special_diff=True\n            )\n            if len(tr_errors) &gt; 0:\n                # The pseudo CDSs are included in this check\n                # Pseudo CDSs are not translated, if the pseudo translation ids are not ignored\n                # in the gff it will give an error\n                tr_errors = self.check_lengths(\n                    pep,\n                    gff_all_translations,\n                    \"Fasta translations vs gff (include pseudo CDS)\",\n                    special_diff=True,\n                )\n                self.add_errors(*tr_errors)\n\n        # Check functional_annotation.json integrity\n        # Gene ids, translated CDS ids and translated CDSs\n        # including pseudogenes are compared to the gff\n        if ann_genes:\n            errors += self.check_ids(ann_genes, gff_genes, \"Gene ids metadata vs gff\")\n            tr_errors = self.check_ids(\n                ann_translations, gff_translations, \"Translation ids metadata vs gff\"\n            )\n            if len(tr_errors) &gt; 0:\n                tr_errors = self.check_ids(\n                    ann_translations,\n                    gff_all_translations,\n                    \"Translation ids metadata vs gff (include pseudo CDS)\",\n                )\n            self.add_errors(*tr_errors)\n            errors += self.check_ids(\n                ann_transposable_elements,\n                gff_transposable_elements,\n                \"TE ids metadata vs gff\",\n            )\n\n        # Check the seq.json intregrity\n        # Compare the length and id retrieved from seq.json to the gff\n        if seq_regions:\n            self.check_seq_region_lengths(\n                seq_lengths, gff_seq_regions, \"Seq_regions metadata vs gff\", seq_circular\n            )\n\n    # Check fasta dna and seq_region integrity\n    if dna and seq_regions:\n        self.check_seq_region_lengths(seq_lengths, dna, \"seq_regions json vs dna\")\n\n    # Check agp and seq_region integrity\n    if agp_seqr and seq_lengths:\n        self.check_seq_region_lengths(seq_lengths, agp_seqr, \"seq_regions json vs agps\")\n\n    if errors:\n        errors_str = \"\\n\".join(errors)\n        raise InvalidIntegrityError(f\"Integrity test failed:\\n{errors_str}\")\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/check_integrity/#src.ensembl.io.genomio.manifest.check_integrity.IntegrityTool.check_lengths","title":"<code>check_lengths(list1, list2, name, allowed_len_diff=None, special_diff=False)</code>","text":"<p>Check the difference in ids and length between list1 and list2.     There are a few special cases here where we allow a certain asymmetry     by changing the values of the arguments.</p> <p>Parameters:</p> Name Type Description Default <code>list1</code> <p>dict containing length and id of the sequence from fasta files.</p> required <code>list2</code> <p>dict containing length and id in the retrieved from the gff.</p> required <code>name</code> <p>string</p> required None to to not accept differences in length between list1 and list2. <p>The value can be changed based on how much difference in sequence length we are wanting to accept.</p> set as False when no special length difference is expected between the lists. <p>This can be changed if we want to report common sequences with 1 BP difference.</p> <p>Returns:</p> Type Description <p>Error if there is a difference in length or ids between the lists.</p> Source code in <code>src/ensembl/io/genomio/manifest/check_integrity.py</code> <pre><code>def check_lengths(self, list1, list2, name, allowed_len_diff=None, special_diff=False):\n    \"\"\"Check the difference in ids and length between list1 and list2.\n        There are a few special cases here where we allow a certain asymmetry\n        by changing the values of the arguments.\n\n    Args:\n        list1: dict containing length and id of the sequence from fasta files.\n        list2: dict containing length and id in the retrieved from the gff.\n        name:  string\n\n    allowed_len_diff : None to to not accept differences in length between list1 and list2.\n        The value can be changed based on how much difference in sequence length we are wanting to accept.\n\n    special_diff: set as False when no special length difference is expected between the lists.\n                This can be changed if we want to report common sequences with 1 BP difference.\n\n    Returns:\n        Error if there is a difference in length or ids between the lists.\n    \"\"\"\n\n    # check list diffferences, checks if abs(values diff) &lt; allowed_len_diff\n\n    set1 = frozenset(list1)\n    set2 = frozenset(list2)\n    list1_2 = list(set1 - set2)\n    list2_1 = list(set2 - set1)\n\n    errors = []\n    if len(list1_2) &gt; 0:\n        errors.append(f\"{name}: {len(list1_2)} from the first list only (i.e. {list1_2[0]})\")\n    if len(list2_1) &gt; 0:\n        errors.append(f\"{name}: {len(list2_1)} from the second list only (i.e. {list2_1[0]})\")\n\n    common_len = 0\n    if allowed_len_diff is None:\n        common_len = len(set1 &amp; set2)\n    else:\n        # check for the sequence length difference\n        diff_len_list = []\n        diff_len_special_list = []\n        for e in set1 &amp; set2:\n            dl12 = list1[e] - list2[e]\n            if abs(dl12) &lt;= allowed_len_diff:\n                common_len += 1\n            else:\n                _dlist = diff_len_list\n                # Special case: 1 AA /BP shorter,\n                #   so assuming the stop codon is not included in the CDS (when it should be)\n                if dl12 == 1 and special_diff:\n                    _dlist = diff_len_special_list\n                _dlist.append(f\"{e}: {list1[e]}, {list2[e]}\")\n        if diff_len_special_list:\n            errors.append(\n                (\n                    f\"{len(diff_len_special_list)} common elements with one BP/AA length diff for {name}\"\n                    f\"(e.g. {diff_len_special_list[0]})\"\n                )\n            )\n        if diff_len_list:\n            errors.append(\n                (\n                    f\"{len(diff_len_list)} common elements with length diff for {name}\"\n                    f\"(e.g. {diff_len_list[0]})\"\n                )\n            )\n    if common_len &gt; 0:\n        print(f\"{common_len} common elements between lists for {name}\", file=sys.stderr)\n\n    return errors\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/check_integrity/#src.ensembl.io.genomio.manifest.check_integrity.IntegrityTool.check_seq_region_lengths","title":"<code>check_seq_region_lengths(seqrs, feats, name, circular=None)</code>","text":"<p>Check the integrity of seq_region.json file by comparing the length of the sequence     to fasta files and the gff.</p> <pre><code>Seq_region file is in json format containing the metadata of the sequence.\nIt contains sequence id, length, location and the synonyms for the sequence name\nfrom different sources.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>seqs</code> <p>Sequence name and length retrieved from seq_region.json file.</p> required <code>feats</code> <code>Dict[str, Any]</code> <p>Sequence name and length retrieved from the fasta and gff file.</p> required <code>name</code> <code>str</code> <p>Name of the check to show in the logs.</p> required <code>circular</code> <code>Optional[Dict[str, Any]]</code> <p>Whether any sequence is circular.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Error if there are common sequences with difference in ids</p> <code>None</code> <p>and if the sequences are not consistent in the files.</p> Source code in <code>src/ensembl/io/genomio/manifest/check_integrity.py</code> <pre><code>def check_seq_region_lengths(\n    self,\n    seqrs: Dict[str, Any],\n    feats: Dict[str, Any],\n    name: str,\n    circular: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n    \"\"\"Check the integrity of seq_region.json file by comparing the length of the sequence\n        to fasta files and the gff.\n\n        Seq_region file is in json format containing the metadata of the sequence.\n        It contains sequence id, length, location and the synonyms for the sequence name\n        from different sources.\n\n    Args:\n        seqs: Sequence name and length retrieved from seq_region.json file.\n        feats: Sequence name and length retrieved from the fasta and gff file.\n        name: Name of the check to show in the logs.\n        circular: Whether any sequence is circular.\n\n    Returns:\n        Error if there are common sequences with difference in ids\n        and if the sequences are not consistent in the files.\n    \"\"\"\n    comp = self._compare_seqs(seqrs, feats, circular)\n\n    common = comp[\"common\"]\n    diff = comp[\"diff\"]\n    diff_circular = comp[\"diff_circular\"]\n    only_seqr = comp[\"only_seqr\"]\n    only_feat = comp[\"only_feat\"]\n\n    if common:\n        print(f\"{len(common)} common elements in {name}\")\n    if diff_circular:\n        example = diff_circular[0]\n        print(f\"{len(diff_circular)} differences for circular elements in {name} (e.g. {example})\")\n    if diff:\n        self.add_errors(f\"{len(diff)} common elements with higher length in {name} (e.g. {diff[0]})\")\n    if only_seqr:\n        # Not an error!\n        print(f\"{len(only_seqr)} only in seq_region list in {name} (first: {only_seqr[0]})\")\n    if only_feat:\n        self.add_errors(f\"{len(only_feat)} only in second list in {name} (first: {only_feat[0]})\")\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/check_integrity/#src.ensembl.io.genomio.manifest.check_integrity.IntegrityTool.set_brc_mode","title":"<code>set_brc_mode(brc_mode)</code>","text":"<p>Set brc mode for this tool and the manifest.</p> Source code in <code>src/ensembl/io/genomio/manifest/check_integrity.py</code> <pre><code>def set_brc_mode(self, brc_mode: bool) -&gt; None:\n    \"\"\"Set brc mode for this tool and the manifest.\"\"\"\n    self.brc_mode = brc_mode\n    self.manifest.brc_mode = brc_mode\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/check_integrity/#src.ensembl.io.genomio.manifest.check_integrity.IntegrityTool.set_ignore_final_stops","title":"<code>set_ignore_final_stops(ignore_final_stops)</code>","text":"<p>Set ignore_final_stops (when calculating peptide length) for this tool and the manifest.</p> Source code in <code>src/ensembl/io/genomio/manifest/check_integrity.py</code> <pre><code>def set_ignore_final_stops(self, ignore_final_stops: bool) -&gt; None:\n    \"\"\"Set ignore_final_stops (when calculating peptide length) for this tool and the manifest.\"\"\"\n    self.ignore_final_stops = ignore_final_stops\n    self.manifest.ignore_final_stops = ignore_final_stops\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/check_integrity/#src.ensembl.io.genomio.manifest.check_integrity.InvalidIntegrityError","title":"<code>InvalidIntegrityError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>When a file integrity check fails</p> Source code in <code>src/ensembl/io/genomio/manifest/check_integrity.py</code> <pre><code>class InvalidIntegrityError(Exception):\n    \"\"\"When a file integrity check fails\"\"\"\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/check_integrity/#src.ensembl.io.genomio.manifest.check_integrity.Manifest","title":"<code>Manifest</code>","text":"<p>Representation of the manifest and its files.</p> Source code in <code>src/ensembl/io/genomio/manifest/check_integrity.py</code> <pre><code>class Manifest:\n    \"\"\"Representation of the manifest and its files.\"\"\"\n\n    def __init__(self, manifest_path: PathLike) -&gt; None:\n        self.manifest_files = self.get_manifest(manifest_path)\n        self.genome: Dict[str, Any] = {}\n        self.seq_regions: Dict[str, Any] = {}\n\n        self.lengths: Dict[str, Lengths] = {\n            \"dna_sequences\": {},\n            \"peptide_sequences\": {},\n            \"seq_region_levels\": {},\n            \"annotations\": {},\n            \"agp\": {},\n            \"gff3_seq_regions\": {},\n            \"gff3_genes\": {},\n            \"gff3_translations\": {},\n            \"gff3_all_translations\": {},\n            \"gff3_transposable_elements\": {},\n            \"ann_genes\": {},\n            \"ann_translations\": {},\n            \"ann_transposable_elements\": {},\n            \"seq_regions\": {},\n        }\n\n        self.circular: Dict[str, Lengths] = {\n            \"seq_regions\": {},\n        }\n\n        self.errors: List[str] = []\n\n        self.ignore_final_stops = False\n        self.brc_mode = False\n\n    def has_lengths(self, name: str) -&gt; bool:\n        \"\"\"Check if a given name has lengths records.\"\"\"\n        if name in self.lengths:\n            return True\n        return False\n\n    def get_lengths(self, name: str) -&gt; Dict[str, Any]:\n        \"\"\"Returns a dict associating IDs with their length from a given file name.\"\"\"\n        if name in self.lengths:\n            return self.lengths[name]\n        raise KeyError(f\"No length available for key {name}\")\n\n    def get_circular(self, name: str) -&gt; Dict[str, Any]:\n        \"\"\"Returns a dict associating IDs with their is_circular flag from a given file name.\"\"\"\n        if name in self.circular:\n            return self.circular[name]\n        raise KeyError(f\"No length available for key {name}\")\n\n    def _add_error(self, error: str) -&gt; None:\n        self.errors.append(error)\n\n    def get_manifest(self, manifest_path: PathLike) -&gt; Dict[str, Any]:\n        \"\"\"Load the content of a manifest file.\n\n        Returns:\n            Dict: Content of the manifest file.\n        \"\"\"\n        manifest_path = Path(manifest_path)\n        with manifest_path.open(\"r\") as manifest_fh:\n            manifest = json.load(manifest_fh)\n\n            # Use dir name from the manifest\n            for name in manifest:\n                if \"file\" in manifest[name]:\n                    file_path = manifest_path.parent / manifest[name][\"file\"]\n                    # check if the md5sum is correct\n                    md5sum = manifest[name][\"md5sum\"]\n                    self._check_md5sum(file_path, md5sum)\n\n                    manifest[name] = file_path\n                else:\n                    for f in manifest[name]:\n                        if \"file\" in manifest[name][f]:\n                            file_path = manifest_path.parent / manifest[name][f][\"file\"]\n                            # check if the md5sum is correct\n                            md5sum = manifest[name][f][\"md5sum\"]\n                            self._check_md5sum(file_path, md5sum)\n\n                            manifest[name][f] = file_path\n            return manifest\n\n    def _check_md5sum(self, file_path: Path, md5sum: str) -&gt; None:\n        \"\"\"Verify the integrity of the files in manifest.json.\n\n        An MD5 hash is generated using the path provided which is then compared to the hash in manifest.json.\n        Errors are stored in ``self.errors``.\n\n        Args:\n            file_path: Path to a genome file.\n            md5sum: MD5 hash for the files.\n        \"\"\"\n\n        with file_path.open(\"rb\") as f:\n            bytes_obj = f.read()\n            readable_hash = hashlib.md5(bytes_obj).hexdigest()\n            if readable_hash != md5sum:\n                raise InvalidIntegrityError(f\"Invalid md5 checksum for {file_path}\")\n\n    def prepare_integrity_data(self) -&gt; None:\n        \"\"\"Read all the files and keep a record (IDs and their lengths)\n        for each cases to be compared later.\n        \"\"\"\n        # First, get the Data\n        if \"gff3\" in self.manifest_files:\n            print(\"Got a gff\")\n            self.get_gff3(self.manifest_files[\"gff3\"])\n        if \"fasta_dna\" in self.manifest_files:\n            print(\"Got a fasta dna\")\n            # Verify if the length and id for the sequence is unique\n            self.lengths[\"dna_sequences\"] = self.get_fasta_lengths(self.manifest_files[\"fasta_dna\"])\n        if \"fasta_pep\" in self.manifest_files:\n            print(\"Got a fasta pep\")\n            # Verify if the length and id for the sequence is unique\n            self.lengths[\"peptide_sequences\"] = self.get_fasta_lengths(\n                self.manifest_files[\"fasta_pep\"], ignore_final_stops=self.ignore_final_stops\n            )\n        if \"seq_region\" in self.manifest_files:\n            print(\"Got a seq_regions\")\n            seq_regions = get_json(Path(self.manifest_files[\"seq_region\"]))\n            seqr_seqlevel = {}\n            seq_lengths = {}\n            seq_circular = {}\n            # Store the length as int\n            for seq in seq_regions:\n                seq_lengths[seq[\"name\"]] = int(seq[\"length\"])\n                seq_circular[seq[\"name\"]] = seq.get(\"circular\", False)\n                if seq[\"coord_system_level\"] == \"contig\":\n                    seqr_seqlevel[seq[\"name\"]] = int(seq[\"length\"])\n            self.lengths[\"seq_regions\"] = seq_lengths\n            self.circular[\"seq_regions\"] = seq_circular\n            self.seq_regions = seq_regions\n        if \"functional_annotation\" in self.manifest_files:\n            print(\"Got a func_anns\")\n            self.get_functional_annotation(self.manifest_files[\"functional_annotation\"])\n        if \"agp\" in self.manifest_files:\n            print(\"Got agp files\")\n            self.lengths[\"agp\"] = self.get_agp_seq_regions(self.manifest_files[\"agp\"])\n        if \"genome\" in self.manifest_files:\n            print(\"Got a genome\")\n            self.lengths[\"genome\"] = get_json(Path(self.manifest_files[\"genome\"]))\n\n    def get_fasta_lengths(self, fasta_path, ignore_final_stops=False):\n        \"\"\"Check if the fasta files have the correct ids and no stop codon.\n\n        Args:\n            fasta_path: Path to fasta_dna and fasta_pep files.\n\n        Returns:\n            Error if any empty ids, non-unique ids or stop codons are found in the fasta files.\n        \"\"\"\n\n        data = {}\n        non_unique = {}\n        non_unique_count = 0\n        empty_id_count = 0\n        contains_stop_codon = 0\n        for rec in SeqIO.parse(fasta_path, \"fasta\"):\n            # Flag empty ids\n            if rec.id == \"\":\n                empty_id_count += 1\n            else:\n                # Flag redundant ids\n                if rec.id in data:\n                    non_unique[rec.id] = 1\n                    non_unique_count += 1\n                # Store sequence id and length\n                data[rec.id] = len(rec.seq)\n                stops = rec.seq.count(\"*\")\n                if stops &gt; 1:\n                    contains_stop_codon += 1\n                elif stops == 1:\n                    if not rec.seq.endswith(\"*\") or not ignore_final_stops:\n                        contains_stop_codon += 1\n\n        if empty_id_count &gt; 0:\n            self._add_error(f\"{empty_id_count} sequences with empty ids in {fasta_path}\")\n        if non_unique_count &gt; 0:\n            self._add_error(f\"{non_unique_count} non unique sequence ids in {fasta_path}\")\n        if contains_stop_codon &gt; 0:\n            self._add_error(f\"{contains_stop_codon} sequences with stop codons in {fasta_path}\")\n        return data\n\n    def get_functional_annotation(self, json_path: Path) -&gt; None:\n        \"\"\"Load the functional annotation file to retrieve the gene_id and translation id.\n            A functional annotation file contains information about a gene.\n            The functional annotation file is stored in a json format containing\n            the description, id and object type (eg: \"gene\", \"transcript\", \"translation\").\n\n        Args:\n            json_path: Path to functional_annotation.json.\n\n        Returns:\n            dict with gene and translation ids.\n        \"\"\"\n\n        # Load the json file\n        with open(json_path) as json_file:\n            data = json.load(json_file)\n\n        # Get gene ids and translation ids\n        genes = {}\n        translations = {}\n        transposons = {}\n\n        for item in data:\n            if item[\"object_type\"] == \"gene\":\n                genes[item[\"id\"]] = 1\n            elif item[\"object_type\"] == \"translation\":\n                translations[item[\"id\"]] = 1\n            if item[\"object_type\"] == \"transposable_element\":\n                transposons[item[\"id\"]] = 1\n\n        stats = {\n            \"ann_genes\": genes,\n            \"ann_translations\": translations,\n            \"ann_transposable_elements\": transposons,\n        }\n        self.lengths = {**self.lengths, **stats}\n\n    def get_gff3(self, gff3_path: Path) -&gt; None:\n        \"\"\"A GFF parser is used to retrieve information in the GFF file such as\n           gene and CDS ids and their corresponding lengths.\n\n        Args:\n            gff3_path: Path to gff3 file.\n        \"\"\"\n\n        seqs: Lengths = {}\n        genes: Lengths = {}\n        peps: Lengths = {}\n        all_peps: Lengths = {}\n        tes: Lengths = {}\n\n        with open_gz_file(gff3_path) as gff3_handle:\n            gff = GFF.parse(gff3_handle)\n            for seq in gff:\n                seqs[seq.id] = len(seq.seq)\n\n                for feat in seq.features:\n                    feat_length = abs(feat.location.end - feat.location.start)\n                    # Store gene id and length\n                    if feat.type in [\"gene\", \"ncRNA_gene\", \"pseudogene\"]:\n                        self._retrieve_gff_gene_lengths(feat, genes, peps, all_peps)\n                    if feat.type == \"transposable_element\":\n                        tes[feat.id] = feat_length\n\n        stats: Dict[str, Lengths] = {\n            \"gff3_seq_regions\": seqs,\n            \"gff3_genes\": genes,\n            \"gff3_translations\": peps,\n            \"gff3_all_translations\": all_peps,\n            \"gff3_transposable_elements\": tes,\n        }\n        self.lengths = {**self.lengths, **stats}\n\n    def _retrieve_gff_gene_lengths(\n        self, feat: SeqFeature, genes: Lengths, peps: Lengths, all_peps: Lengths\n    ) -&gt; None:\n        \"\"\"Record genes and peptides lengths from a feature.\n\n        Args:\n            feat : Gene feature to check.\n            genes: Record of genes lengths to update.\n            peps: Record of peptides lengths to update.\n            all_peps: Record of all peptides lengths to update (include pseudogenes).\n\n        \"\"\"\n        gene_id = feat.id\n        if not self.brc_mode:\n            gene_id = gene_id.replace(\"gene:\", \"\")\n        genes[gene_id] = abs(feat.location.end - feat.location.start)\n        # Get CDS id and length\n        for feat2 in feat.sub_features:\n            if feat2.type in (\"mRNA\", \"pseudogenic_transcript\"):\n                length = {}\n                for feat3 in feat2.sub_features:\n                    if feat3.type == \"CDS\":\n                        pep_id = feat3.id\n                        if not self.brc_mode:\n                            pep_id = pep_id.replace(\"CDS:\", \"\")\n                        if pep_id not in length:\n                            length[pep_id] = 0\n                        length[pep_id] += abs(feat3.location.end - feat3.location.start)\n                for pep_id, pep_length in length.items():\n                    # Store length for translations, add pseudo translations separately\n                    pep_length = floor(pep_length / 3) - 1\n                    if feat.type != \"pseudogene\":\n                        peps[pep_id] = pep_length\n                    all_peps[pep_id] = pep_length\n\n    def get_agp_seq_regions(self, agp_dict):\n        \"\"\"AGP files describe the assembly of larger sequence objects using smaller objects.\n            Eg: describes the assembly of scaffolds from contigs.\n\n        Args:\n            agp_dict: dict containing the information about the sequence.\n\n        Note:\n            AGP file is only used in the older builds, not used for current processing.\n        \"\"\"\n\n        seqr = {}\n        for agp in agp_dict:\n            agp_path = agp_dict[agp]\n\n            with open(agp_path, \"r\") as agph:\n                for line in agph:\n                    (\n                        asm_id,\n                        _,  # asm_start\n                        asm_end,\n                        _,  # asm_part\n                        typ,\n                        cmp_id,\n                        _,  # cmp_start\n                        cmp_end,\n                        _,  # cmp_strand\n                    ) = line.split(\"\\t\")\n                    # Ignore WGS contig\n                    if typ != \"W\":\n                        continue\n\n                    # Assembled seq length\n                    if asm_id not in seqr or seqr[asm_id] &lt; int(asm_end):\n                        seqr[asm_id] = int(asm_end)\n\n                    # Composite seq length\n                    if cmp_id not in seqr or seqr[cmp_id] &lt; int(cmp_end):\n                        seqr[cmp_id] = int(cmp_end)\n\n        return seqr\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/check_integrity/#src.ensembl.io.genomio.manifest.check_integrity.Manifest.get_agp_seq_regions","title":"<code>get_agp_seq_regions(agp_dict)</code>","text":"<p>AGP files describe the assembly of larger sequence objects using smaller objects.     Eg: describes the assembly of scaffolds from contigs.</p> <p>Parameters:</p> Name Type Description Default <code>agp_dict</code> <p>dict containing the information about the sequence.</p> required Note <p>AGP file is only used in the older builds, not used for current processing.</p> Source code in <code>src/ensembl/io/genomio/manifest/check_integrity.py</code> <pre><code>def get_agp_seq_regions(self, agp_dict):\n    \"\"\"AGP files describe the assembly of larger sequence objects using smaller objects.\n        Eg: describes the assembly of scaffolds from contigs.\n\n    Args:\n        agp_dict: dict containing the information about the sequence.\n\n    Note:\n        AGP file is only used in the older builds, not used for current processing.\n    \"\"\"\n\n    seqr = {}\n    for agp in agp_dict:\n        agp_path = agp_dict[agp]\n\n        with open(agp_path, \"r\") as agph:\n            for line in agph:\n                (\n                    asm_id,\n                    _,  # asm_start\n                    asm_end,\n                    _,  # asm_part\n                    typ,\n                    cmp_id,\n                    _,  # cmp_start\n                    cmp_end,\n                    _,  # cmp_strand\n                ) = line.split(\"\\t\")\n                # Ignore WGS contig\n                if typ != \"W\":\n                    continue\n\n                # Assembled seq length\n                if asm_id not in seqr or seqr[asm_id] &lt; int(asm_end):\n                    seqr[asm_id] = int(asm_end)\n\n                # Composite seq length\n                if cmp_id not in seqr or seqr[cmp_id] &lt; int(cmp_end):\n                    seqr[cmp_id] = int(cmp_end)\n\n    return seqr\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/check_integrity/#src.ensembl.io.genomio.manifest.check_integrity.Manifest.get_circular","title":"<code>get_circular(name)</code>","text":"<p>Returns a dict associating IDs with their is_circular flag from a given file name.</p> Source code in <code>src/ensembl/io/genomio/manifest/check_integrity.py</code> <pre><code>def get_circular(self, name: str) -&gt; Dict[str, Any]:\n    \"\"\"Returns a dict associating IDs with their is_circular flag from a given file name.\"\"\"\n    if name in self.circular:\n        return self.circular[name]\n    raise KeyError(f\"No length available for key {name}\")\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/check_integrity/#src.ensembl.io.genomio.manifest.check_integrity.Manifest.get_fasta_lengths","title":"<code>get_fasta_lengths(fasta_path, ignore_final_stops=False)</code>","text":"<p>Check if the fasta files have the correct ids and no stop codon.</p> <p>Parameters:</p> Name Type Description Default <code>fasta_path</code> <p>Path to fasta_dna and fasta_pep files.</p> required <p>Returns:</p> Type Description <p>Error if any empty ids, non-unique ids or stop codons are found in the fasta files.</p> Source code in <code>src/ensembl/io/genomio/manifest/check_integrity.py</code> <pre><code>def get_fasta_lengths(self, fasta_path, ignore_final_stops=False):\n    \"\"\"Check if the fasta files have the correct ids and no stop codon.\n\n    Args:\n        fasta_path: Path to fasta_dna and fasta_pep files.\n\n    Returns:\n        Error if any empty ids, non-unique ids or stop codons are found in the fasta files.\n    \"\"\"\n\n    data = {}\n    non_unique = {}\n    non_unique_count = 0\n    empty_id_count = 0\n    contains_stop_codon = 0\n    for rec in SeqIO.parse(fasta_path, \"fasta\"):\n        # Flag empty ids\n        if rec.id == \"\":\n            empty_id_count += 1\n        else:\n            # Flag redundant ids\n            if rec.id in data:\n                non_unique[rec.id] = 1\n                non_unique_count += 1\n            # Store sequence id and length\n            data[rec.id] = len(rec.seq)\n            stops = rec.seq.count(\"*\")\n            if stops &gt; 1:\n                contains_stop_codon += 1\n            elif stops == 1:\n                if not rec.seq.endswith(\"*\") or not ignore_final_stops:\n                    contains_stop_codon += 1\n\n    if empty_id_count &gt; 0:\n        self._add_error(f\"{empty_id_count} sequences with empty ids in {fasta_path}\")\n    if non_unique_count &gt; 0:\n        self._add_error(f\"{non_unique_count} non unique sequence ids in {fasta_path}\")\n    if contains_stop_codon &gt; 0:\n        self._add_error(f\"{contains_stop_codon} sequences with stop codons in {fasta_path}\")\n    return data\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/check_integrity/#src.ensembl.io.genomio.manifest.check_integrity.Manifest.get_functional_annotation","title":"<code>get_functional_annotation(json_path)</code>","text":"<p>Load the functional annotation file to retrieve the gene_id and translation id.     A functional annotation file contains information about a gene.     The functional annotation file is stored in a json format containing     the description, id and object type (eg: \"gene\", \"transcript\", \"translation\").</p> <p>Parameters:</p> Name Type Description Default <code>json_path</code> <code>Path</code> <p>Path to functional_annotation.json.</p> required <p>Returns:</p> Type Description <code>None</code> <p>dict with gene and translation ids.</p> Source code in <code>src/ensembl/io/genomio/manifest/check_integrity.py</code> <pre><code>def get_functional_annotation(self, json_path: Path) -&gt; None:\n    \"\"\"Load the functional annotation file to retrieve the gene_id and translation id.\n        A functional annotation file contains information about a gene.\n        The functional annotation file is stored in a json format containing\n        the description, id and object type (eg: \"gene\", \"transcript\", \"translation\").\n\n    Args:\n        json_path: Path to functional_annotation.json.\n\n    Returns:\n        dict with gene and translation ids.\n    \"\"\"\n\n    # Load the json file\n    with open(json_path) as json_file:\n        data = json.load(json_file)\n\n    # Get gene ids and translation ids\n    genes = {}\n    translations = {}\n    transposons = {}\n\n    for item in data:\n        if item[\"object_type\"] == \"gene\":\n            genes[item[\"id\"]] = 1\n        elif item[\"object_type\"] == \"translation\":\n            translations[item[\"id\"]] = 1\n        if item[\"object_type\"] == \"transposable_element\":\n            transposons[item[\"id\"]] = 1\n\n    stats = {\n        \"ann_genes\": genes,\n        \"ann_translations\": translations,\n        \"ann_transposable_elements\": transposons,\n    }\n    self.lengths = {**self.lengths, **stats}\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/check_integrity/#src.ensembl.io.genomio.manifest.check_integrity.Manifest.get_gff3","title":"<code>get_gff3(gff3_path)</code>","text":"<p>A GFF parser is used to retrieve information in the GFF file such as    gene and CDS ids and their corresponding lengths.</p> <p>Parameters:</p> Name Type Description Default <code>gff3_path</code> <code>Path</code> <p>Path to gff3 file.</p> required Source code in <code>src/ensembl/io/genomio/manifest/check_integrity.py</code> <pre><code>def get_gff3(self, gff3_path: Path) -&gt; None:\n    \"\"\"A GFF parser is used to retrieve information in the GFF file such as\n       gene and CDS ids and their corresponding lengths.\n\n    Args:\n        gff3_path: Path to gff3 file.\n    \"\"\"\n\n    seqs: Lengths = {}\n    genes: Lengths = {}\n    peps: Lengths = {}\n    all_peps: Lengths = {}\n    tes: Lengths = {}\n\n    with open_gz_file(gff3_path) as gff3_handle:\n        gff = GFF.parse(gff3_handle)\n        for seq in gff:\n            seqs[seq.id] = len(seq.seq)\n\n            for feat in seq.features:\n                feat_length = abs(feat.location.end - feat.location.start)\n                # Store gene id and length\n                if feat.type in [\"gene\", \"ncRNA_gene\", \"pseudogene\"]:\n                    self._retrieve_gff_gene_lengths(feat, genes, peps, all_peps)\n                if feat.type == \"transposable_element\":\n                    tes[feat.id] = feat_length\n\n    stats: Dict[str, Lengths] = {\n        \"gff3_seq_regions\": seqs,\n        \"gff3_genes\": genes,\n        \"gff3_translations\": peps,\n        \"gff3_all_translations\": all_peps,\n        \"gff3_transposable_elements\": tes,\n    }\n    self.lengths = {**self.lengths, **stats}\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/check_integrity/#src.ensembl.io.genomio.manifest.check_integrity.Manifest.get_lengths","title":"<code>get_lengths(name)</code>","text":"<p>Returns a dict associating IDs with their length from a given file name.</p> Source code in <code>src/ensembl/io/genomio/manifest/check_integrity.py</code> <pre><code>def get_lengths(self, name: str) -&gt; Dict[str, Any]:\n    \"\"\"Returns a dict associating IDs with their length from a given file name.\"\"\"\n    if name in self.lengths:\n        return self.lengths[name]\n    raise KeyError(f\"No length available for key {name}\")\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/check_integrity/#src.ensembl.io.genomio.manifest.check_integrity.Manifest.get_manifest","title":"<code>get_manifest(manifest_path)</code>","text":"<p>Load the content of a manifest file.</p> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict[str, Any]</code> <p>Content of the manifest file.</p> Source code in <code>src/ensembl/io/genomio/manifest/check_integrity.py</code> <pre><code>def get_manifest(self, manifest_path: PathLike) -&gt; Dict[str, Any]:\n    \"\"\"Load the content of a manifest file.\n\n    Returns:\n        Dict: Content of the manifest file.\n    \"\"\"\n    manifest_path = Path(manifest_path)\n    with manifest_path.open(\"r\") as manifest_fh:\n        manifest = json.load(manifest_fh)\n\n        # Use dir name from the manifest\n        for name in manifest:\n            if \"file\" in manifest[name]:\n                file_path = manifest_path.parent / manifest[name][\"file\"]\n                # check if the md5sum is correct\n                md5sum = manifest[name][\"md5sum\"]\n                self._check_md5sum(file_path, md5sum)\n\n                manifest[name] = file_path\n            else:\n                for f in manifest[name]:\n                    if \"file\" in manifest[name][f]:\n                        file_path = manifest_path.parent / manifest[name][f][\"file\"]\n                        # check if the md5sum is correct\n                        md5sum = manifest[name][f][\"md5sum\"]\n                        self._check_md5sum(file_path, md5sum)\n\n                        manifest[name][f] = file_path\n        return manifest\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/check_integrity/#src.ensembl.io.genomio.manifest.check_integrity.Manifest.has_lengths","title":"<code>has_lengths(name)</code>","text":"<p>Check if a given name has lengths records.</p> Source code in <code>src/ensembl/io/genomio/manifest/check_integrity.py</code> <pre><code>def has_lengths(self, name: str) -&gt; bool:\n    \"\"\"Check if a given name has lengths records.\"\"\"\n    if name in self.lengths:\n        return True\n    return False\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/check_integrity/#src.ensembl.io.genomio.manifest.check_integrity.Manifest.prepare_integrity_data","title":"<code>prepare_integrity_data()</code>","text":"<p>Read all the files and keep a record (IDs and their lengths) for each cases to be compared later.</p> Source code in <code>src/ensembl/io/genomio/manifest/check_integrity.py</code> <pre><code>def prepare_integrity_data(self) -&gt; None:\n    \"\"\"Read all the files and keep a record (IDs and their lengths)\n    for each cases to be compared later.\n    \"\"\"\n    # First, get the Data\n    if \"gff3\" in self.manifest_files:\n        print(\"Got a gff\")\n        self.get_gff3(self.manifest_files[\"gff3\"])\n    if \"fasta_dna\" in self.manifest_files:\n        print(\"Got a fasta dna\")\n        # Verify if the length and id for the sequence is unique\n        self.lengths[\"dna_sequences\"] = self.get_fasta_lengths(self.manifest_files[\"fasta_dna\"])\n    if \"fasta_pep\" in self.manifest_files:\n        print(\"Got a fasta pep\")\n        # Verify if the length and id for the sequence is unique\n        self.lengths[\"peptide_sequences\"] = self.get_fasta_lengths(\n            self.manifest_files[\"fasta_pep\"], ignore_final_stops=self.ignore_final_stops\n        )\n    if \"seq_region\" in self.manifest_files:\n        print(\"Got a seq_regions\")\n        seq_regions = get_json(Path(self.manifest_files[\"seq_region\"]))\n        seqr_seqlevel = {}\n        seq_lengths = {}\n        seq_circular = {}\n        # Store the length as int\n        for seq in seq_regions:\n            seq_lengths[seq[\"name\"]] = int(seq[\"length\"])\n            seq_circular[seq[\"name\"]] = seq.get(\"circular\", False)\n            if seq[\"coord_system_level\"] == \"contig\":\n                seqr_seqlevel[seq[\"name\"]] = int(seq[\"length\"])\n        self.lengths[\"seq_regions\"] = seq_lengths\n        self.circular[\"seq_regions\"] = seq_circular\n        self.seq_regions = seq_regions\n    if \"functional_annotation\" in self.manifest_files:\n        print(\"Got a func_anns\")\n        self.get_functional_annotation(self.manifest_files[\"functional_annotation\"])\n    if \"agp\" in self.manifest_files:\n        print(\"Got agp files\")\n        self.lengths[\"agp\"] = self.get_agp_seq_regions(self.manifest_files[\"agp\"])\n    if \"genome\" in self.manifest_files:\n        print(\"Got a genome\")\n        self.lengths[\"genome\"] = get_json(Path(self.manifest_files[\"genome\"]))\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/check_integrity/#src.ensembl.io.genomio.manifest.check_integrity.main","title":"<code>main()</code>","text":"<p>Main entrypoint.</p> Source code in <code>src/ensembl/io/genomio/manifest/check_integrity.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entrypoint.\"\"\"\n    parser = ArgumentParser(\n        description=\"Compare the genomic data between the files present in a manifest file.\"\n    )\n    parser.add_argument_src_path(\"--manifest_file\", required=True, help=\"Manifest file for the data to check\")\n    parser.add_argument(\"--brc_mode\", action=\"store_true\", help=\"Enable BRC mode\")\n    parser.add_argument(\n        \"--ignore_final_stops\", action=\"store_true\", help=\"Ignore final stop when calculating peptide length\"\n    )\n    args = parser.parse_args()\n\n    inspector = IntegrityTool(**vars(args))\n    inspector.check_integrity()\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/compute_stats/","title":"compute_stats","text":"<p>Compute stats from the current genome files associated with the manifest.</p>"},{"location":"reference/ensembl/io/genomio/manifest/compute_stats/#src.ensembl.io.genomio.manifest.compute_stats.BiotypeCounter","title":"<code>BiotypeCounter</code>","text":"<p>A counter for a given biotype, given a list of features.</p> Source code in <code>src/ensembl/io/genomio/manifest/compute_stats.py</code> <pre><code>class BiotypeCounter:\n    \"\"\"A counter for a given biotype, given a list of features.\"\"\"\n\n    def __init__(self, count: int = 0, ids: Optional[Set[str]] = None, example: Optional[str] = None) -&gt; None:\n        self.count: int = count\n        if ids is None:\n            ids = set()\n        self.ids: Set[str] = ids\n        if example is None:\n            example = \"\"\n        self.example: str = example\n\n    def add_id(self, feature_id: str) -&gt; None:\n        \"\"\"Add a feature to the counter.\n\n        Args:\n            feature_id (str): Feature id to add.\n        \"\"\"\n        self.count += 1\n        self.ids.add(feature_id)\n\n    def unique_count(self) -&gt; int:\n        \"\"\"Total number feature ids added to the counter so far.\n\n        Returns:\n            int: number of features in the counter.\n        \"\"\"\n        return len(self.ids)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/compute_stats/#src.ensembl.io.genomio.manifest.compute_stats.BiotypeCounter.add_id","title":"<code>add_id(feature_id)</code>","text":"<p>Add a feature to the counter.</p> <p>Parameters:</p> Name Type Description Default <code>feature_id</code> <code>str</code> <p>Feature id to add.</p> required Source code in <code>src/ensembl/io/genomio/manifest/compute_stats.py</code> <pre><code>def add_id(self, feature_id: str) -&gt; None:\n    \"\"\"Add a feature to the counter.\n\n    Args:\n        feature_id (str): Feature id to add.\n    \"\"\"\n    self.count += 1\n    self.ids.add(feature_id)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/compute_stats/#src.ensembl.io.genomio.manifest.compute_stats.BiotypeCounter.unique_count","title":"<code>unique_count()</code>","text":"<p>Total number feature ids added to the counter so far.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>number of features in the counter.</p> Source code in <code>src/ensembl/io/genomio/manifest/compute_stats.py</code> <pre><code>def unique_count(self) -&gt; int:\n    \"\"\"Total number feature ids added to the counter so far.\n\n    Returns:\n        int: number of features in the counter.\n    \"\"\"\n    return len(self.ids)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/compute_stats/#src.ensembl.io.genomio.manifest.compute_stats.StatsError","title":"<code>StatsError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Raised when stats could not be computed.</p> Source code in <code>src/ensembl/io/genomio/manifest/compute_stats.py</code> <pre><code>class StatsError(Exception):\n    \"\"\"Raised when stats could not be computed.\"\"\"\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/compute_stats/#src.ensembl.io.genomio.manifest.compute_stats.manifest_stats","title":"<code>manifest_stats</code>","text":"<p>Representation of the statistics of the set of files listed in the manifest file provided.</p> Source code in <code>src/ensembl/io/genomio/manifest/compute_stats.py</code> <pre><code>class manifest_stats:\n    \"\"\"Representation of the statistics of the set of files listed in the manifest file provided.\"\"\"\n\n    def __init__(self, manifest_dir: str, accession: Optional[str], datasets_bin: Optional[str]):\n        self.manifest = f\"{manifest_dir}/manifest.json\"\n        self.accession: Optional[str] = accession\n        self.error = False\n        if datasets_bin is None:\n            datasets_bin = \"datasets\"\n        self.datasets_bin = datasets_bin\n        self.manifest_parent = manifest_dir\n        self.check_ncbi = False\n\n    def run(self, stats_path: PathLike):\n        \"\"\"Compute stats in the files and output a stats.txt file in the same folder.\n\n        Raises:\n            StatsError: Could not compute some stats.\n        \"\"\"\n        manifest = self.get_manifest()\n\n        stats = []\n        if self.accession is not None:\n            stats.append(self.accession)\n\n        # Compute the stats from the GFF3 file\n        if \"gff3\" in manifest:\n            stats += self.get_gff3_stats(Path(manifest[\"gff3\"]))\n\n        # Compute the stats from the seq_region file\n        if \"seq_region\" in manifest:\n            stats += self.get_seq_region_stats(Path(manifest[\"seq_region\"]))\n\n        # Print out the stats in a separate file\n        with Path(stats_path).open(\"w\") as stats_out:\n            stats_out.write(\"\\n\".join(stats))\n\n        # Die if there were errors in stats comparison\n        if self.error:\n            raise StatsError(f\"Stats count errors, check the file {stats_path}\")\n\n    def get_manifest(self) -&gt; Dict:\n        \"\"\"Get the files metadata from the manifest json file.\n\n        Returns:\n            Dict: A representation of the manifest json data.\n        \"\"\"\n        with open(self.manifest) as f_json:\n            manifest = json.load(f_json)\n            manifest_root = self.manifest_parent\n\n        # Use dir name from the manifest\n        for name in manifest:\n            if \"file\" in manifest[name]:\n                file_name = manifest[name][\"file\"]\n                file_name = f\"{manifest_root}/{file_name}\"\n                manifest[name] = file_name\n            else:\n                for f in manifest[name]:\n                    if \"file\" in manifest[name][f]:\n                        file_name = manifest[name][f][\"file\"]\n                        file_name = manifest_root, file_name\n                        manifest[name][f] = file_name\n\n        return manifest\n\n    def get_seq_region_stats(self, seq_region_path: Path) -&gt; List[str]:\n        \"\"\"Compute stats from the seq_region json file.\n\n        Args:\n            seq_region_path (Path): the seq_region json file.\n\n        Returns:\n            List[str]: Stats from the seq_regions.\n        \"\"\"\n        with seq_region_path.open(\"r\") as json_file:\n            seq_regions = json.load(json_file)\n\n        # Get basic data\n        coord_systems: Dict[str, List[int]] = {}\n        circular = 0\n        locations = []\n        codon_tables = []\n        for seqr in seq_regions:\n            # Get readable seq_region name:\n            # either use a Genbank synonym, or just the provided seq_region name\n            genbank = \"synonyms\" in seqr and [x for x in seqr[\"synonyms\"] if x[\"source\"] == \"GenBank\"]\n            seqr_name = genbank and genbank[0][\"name\"] or seqr[\"name\"]\n\n            # Record the lengths of the elements of each coord_system\n            coord_level = seqr[\"coord_system_level\"]\n            if coord_level not in coord_systems:\n                coord_systems[coord_level] = []\n            coord_systems[coord_level].append(seqr[\"length\"])\n\n            # Additional metadata records to count\n            if \"circular\" in seqr:\n                circular += 1\n            if \"codon_table\" in seqr:\n                codon_tables.append(f\"{seqr_name} = {seqr['codon_table']}\")\n            if \"location\" in seqr:\n                locations.append(f\"{seqr_name} = {seqr['location']}\")\n\n        # Stats\n        stats: List[str] = []\n        stats.append(seq_region_path.name)\n        stats += self.coord_systems_stats(coord_systems)\n        stats += self.seq_region_special_stats(circular, locations, codon_tables)\n        stats.append(\"\\n\")\n        return stats\n\n    def coord_systems_stats(self, coord_systems: Dict[str, List[int]]) -&gt; List[str]:\n        \"\"\"For each coord_system compute various stats:\n            - number of sequences\n            - sequence length sum, minimum, maximum, mean\n\n        Args:\n            coord_systems: Coordinate system dictionary of lengths.\n\n        Returns:\n            A list with the computed statistics in a printable format.\n        \"\"\"\n        stats: List[str] = []\n        stats.append(f\"Total coord_systems {len(coord_systems)}\")\n        for coord_name, lengths in coord_systems.items():\n            stats.append(f\"\\nCoord_system: {coord_name}\")\n\n            stat_counts: Dict[str, Union[int, float]] = {\n                \"Number of sequences\": len(lengths),\n                \"Sequence length sum\": sum(lengths),\n                \"Sequence length minimum\": min(lengths),\n                \"Sequence length maximum\": max(lengths),\n                \"Sequence length mean\": mean(lengths),\n            }\n\n            for name, count in stat_counts.items():\n                if isinstance(count, int):\n                    stats.append(f\"{count: 9d}\\t{name}\")\n                else:\n                    stats.append(f\"{count: 9f}\\t{name}\")\n        return stats\n\n    def seq_region_special_stats(\n        self,\n        circular: int = 0,\n        locations: Optional[List[str]] = None,\n        codon_tables: Optional[List[str]] = None,\n    ) -&gt; List[str]:\n        \"\"\"Prepare stats in case there are circular regions, specific locations and codon_tables.\n                stats.append(f\"{count: 9f}\\t{name}\")\n\n        Args:\n            circular: Number of circular regions. Defaults to 0.\n            locations: The regions and their location. Defaults to None.\n            codon_tables: The regions and their codon_table. Defaults to None.\n\n        Returns:\n            A list with the computed statistics in a printable format.\n        \"\"\"\n        stats: List[str] = []\n        if circular or locations or codon_tables:\n            stats.append(\"\\nSpecial\")\n            if circular:\n                stats.append(f\"{circular: 9d}\\tcircular sequences\")\n            if locations is not None:\n                stats.append(f\"{len(locations): 9d} sequences with location\")\n                for loc in locations:\n                    stats.append(f\"\\t\\t\\t{loc}\")\n            if codon_tables:\n                stats.append(f\"{len(codon_tables): 9d} sequences with codon_table\")\n                for table in codon_tables:\n                    stats.append(f\"\\t\\t\\t{table}\")\n        return stats\n\n    def get_gff3_stats(self, gff3_path: Path) -&gt; List[str]:\n        \"\"\"Extract the gene models from the GFF3 file and compute stats.\n\n        Args:\n            gff3_path (Path): the GFF3 file.\n\n        Returns:\n            List: Stats from the gene model.\n        \"\"\"\n\n        biotypes = self.count_biotypes(gff3_path)\n        # Compile final stats\n        stats = self.biotypes_stats(biotypes)\n        stats += self.check_ncbi_stats(biotypes)\n        return stats\n\n    def count_biotypes(self, gff3_path: Path) -&gt; Dict[str, BiotypeCounter]:\n        \"\"\"Count the biotypes in a GFF3 file.\n\n        Args:\n            gff3_path: Path to the GFF3 file.\n\n        Returns:\n            Dictionary of biotype counters.\n        \"\"\"\n\n        biotypes: Dict[str, BiotypeCounter] = {}\n\n        with open_gz_file(gff3_path) as gff3_handle:\n            for rec in GFF.parse(gff3_handle):\n                for feat1 in rec.features:\n                    # Check if the gene contains proteins (CDSs),\n                    # and keep a count of all hierarchies (e.g. gene-mRNA-CDS)\n                    is_protein = False\n                    for feat2 in feat1.sub_features:\n                        if feat2.type == \"mRNA\":\n                            types2 = {f.type for f in feat2.sub_features}\n                            if \"CDS\" in types2:\n                                is_protein = True\n                        manifest_stats.increment_biotype(biotypes, feat2.id, f\"{feat1.type}-{feat2.type}\")\n                        for feat3 in feat2.sub_features:\n                            if feat3.type == \"exon\":\n                                continue\n                            manifest_stats.increment_biotype(\n                                biotypes, feat3.id, f\"{feat1.type}-{feat2.type}-{feat3.type}\"\n                            )\n\n                    # Main categories counts\n                    if feat1.type == \"pseudogene\":\n                        manifest_stats.increment_biotype(biotypes, feat1.id, \"pseudogene\")\n                    elif is_protein:\n                        manifest_stats.increment_biotype(biotypes, feat1.id, f\"PROT_{feat1.type}\")\n                    else:\n                        # Special case, undefined gene-transcript\n                        if (\n                            feat1.type == \"gene\"\n                            and feat1.sub_features\n                            and feat1.sub_features[0].type == \"transcript\"\n                        ):\n                            manifest_stats.increment_biotype(biotypes, feat1.id, \"OTHER\")\n                        else:\n                            manifest_stats.increment_biotype(biotypes, feat1.id, f\"NONPROT_{feat1.type}\")\n\n                    # Total\n                    if feat1.type in (\"gene\", \"pseudogene\"):\n                        manifest_stats.increment_biotype(biotypes, feat1.id, \"ALL_GENES\")\n        return biotypes\n\n    def biotypes_stats(self, biotypes: Dict[str, BiotypeCounter]) -&gt; List[str]:\n        \"\"\"Prepare biotype stats in order of their name.\n\n        Args:\n            biotypes: Biotypes counters.\n\n        Returns:\n            A list with the computed statistics in a printable format.\n        \"\"\"\n        sorted_biotypes = {}\n        for name in sorted(biotypes.keys()):\n            data: BiotypeCounter = biotypes[name]\n            sorted_biotypes[name] = data\n\n        stats = [\n            f\"{data.unique_count():&gt;9}\\t{biotype:&lt;20}\\tID = {data.example}\"\n            for (biotype, data) in sorted_biotypes.items()\n        ]\n        return stats\n\n    def check_ncbi_stats(self, biotypes: Dict[str, BiotypeCounter]) -&gt; List[str]:\n        \"\"\"Use the dataset tool from NCBI to get stats and compare with what we have\"\"\"\n        stats: List[str] = []\n        if not self.check_ncbi:\n            return stats\n\n        if self.accession is None:\n            return stats\n\n        accession: str = self.accession\n\n        datasets_bin = self.datasets_bin\n        if not which(datasets_bin):\n            return stats\n\n        # Get the dataset summary from NCBI\n        command = [datasets_bin, \"summary\", \"genome\", \"accession\", accession]\n        result_out = subprocess.run(command, stdout=subprocess.PIPE, check=True)\n        result = json.loads(result_out.stdout)\n\n        # Get stats\n        if \"reports\" in result:\n            genome = result[\"reports\"][0]\n            if \"annotation_info\" in genome and \"stats\" in genome[\"annotation_info\"]:\n                ncbi_stats = genome[\"annotation_info\"][\"stats\"]\n\n                if \"gene_counts\" in ncbi_stats:\n                    counts = ncbi_stats[\"gene_counts\"]\n                    stats = self.compare_ncbi_counts(biotypes, counts)\n        return stats\n\n    def compare_ncbi_counts(self, biotypes: Dict[str, BiotypeCounter], ncbi: Dict) -&gt; List[str]:\n        \"\"\"Compare specific gene stats from NCBI\"\"\"\n        stats: List[str] = []\n\n        maps = [\n            [\"total\", \"ALL_GENES\"],\n            [\"protein_coding\", \"PROT_gene\"],\n            [\"pseudogene\", \"pseudogene\"],\n            [\"non_coding\", \"NONPROT_gene\"],\n            [\"other\", \"OTHER\"],\n        ]\n\n        for count_map in maps:\n            ncbi_name, prep_name = count_map\n            ncbi_count = ncbi.get(ncbi_name, 0)\n            preped: Optional[BiotypeCounter] = biotypes.get(prep_name)\n            prep_count = 0\n            if preped is not None:\n                prep_count = preped.count\n\n            if prep_count != ncbi_count:\n                diff = prep_count - ncbi_count\n                stats.append(f\"DIFF gene count for {count_map}: {prep_count} - {ncbi_count} = {diff}\")\n                self.error = True\n            else:\n                stats.append(f\"Same count for {count_map}: {prep_count}\")\n\n        return stats\n\n    @staticmethod\n    def increment_biotype(biotypes: Dict[str, BiotypeCounter], feature_id: str, feature_biotype: str) -&gt; None:\n        \"\"\"Add the feature to their respective biotype counter.\n\n        Args:\n            biotypes (Dict[str, BiotypeCounter]): All current biotypes, with their counter.\n            feature_id (str): Feature id to be counted.\n            feature_biotype (str): The biotype of the feature.\n        \"\"\"\n        if feature_biotype not in biotypes:\n            biotypes[feature_biotype] = BiotypeCounter(example=feature_id)\n        biotypes[feature_biotype].add_id(feature_id)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/compute_stats/#src.ensembl.io.genomio.manifest.compute_stats.manifest_stats.biotypes_stats","title":"<code>biotypes_stats(biotypes)</code>","text":"<p>Prepare biotype stats in order of their name.</p> <p>Parameters:</p> Name Type Description Default <code>biotypes</code> <code>Dict[str, BiotypeCounter]</code> <p>Biotypes counters.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list with the computed statistics in a printable format.</p> Source code in <code>src/ensembl/io/genomio/manifest/compute_stats.py</code> <pre><code>def biotypes_stats(self, biotypes: Dict[str, BiotypeCounter]) -&gt; List[str]:\n    \"\"\"Prepare biotype stats in order of their name.\n\n    Args:\n        biotypes: Biotypes counters.\n\n    Returns:\n        A list with the computed statistics in a printable format.\n    \"\"\"\n    sorted_biotypes = {}\n    for name in sorted(biotypes.keys()):\n        data: BiotypeCounter = biotypes[name]\n        sorted_biotypes[name] = data\n\n    stats = [\n        f\"{data.unique_count():&gt;9}\\t{biotype:&lt;20}\\tID = {data.example}\"\n        for (biotype, data) in sorted_biotypes.items()\n    ]\n    return stats\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/compute_stats/#src.ensembl.io.genomio.manifest.compute_stats.manifest_stats.check_ncbi_stats","title":"<code>check_ncbi_stats(biotypes)</code>","text":"<p>Use the dataset tool from NCBI to get stats and compare with what we have</p> Source code in <code>src/ensembl/io/genomio/manifest/compute_stats.py</code> <pre><code>def check_ncbi_stats(self, biotypes: Dict[str, BiotypeCounter]) -&gt; List[str]:\n    \"\"\"Use the dataset tool from NCBI to get stats and compare with what we have\"\"\"\n    stats: List[str] = []\n    if not self.check_ncbi:\n        return stats\n\n    if self.accession is None:\n        return stats\n\n    accession: str = self.accession\n\n    datasets_bin = self.datasets_bin\n    if not which(datasets_bin):\n        return stats\n\n    # Get the dataset summary from NCBI\n    command = [datasets_bin, \"summary\", \"genome\", \"accession\", accession]\n    result_out = subprocess.run(command, stdout=subprocess.PIPE, check=True)\n    result = json.loads(result_out.stdout)\n\n    # Get stats\n    if \"reports\" in result:\n        genome = result[\"reports\"][0]\n        if \"annotation_info\" in genome and \"stats\" in genome[\"annotation_info\"]:\n            ncbi_stats = genome[\"annotation_info\"][\"stats\"]\n\n            if \"gene_counts\" in ncbi_stats:\n                counts = ncbi_stats[\"gene_counts\"]\n                stats = self.compare_ncbi_counts(biotypes, counts)\n    return stats\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/compute_stats/#src.ensembl.io.genomio.manifest.compute_stats.manifest_stats.compare_ncbi_counts","title":"<code>compare_ncbi_counts(biotypes, ncbi)</code>","text":"<p>Compare specific gene stats from NCBI</p> Source code in <code>src/ensembl/io/genomio/manifest/compute_stats.py</code> <pre><code>def compare_ncbi_counts(self, biotypes: Dict[str, BiotypeCounter], ncbi: Dict) -&gt; List[str]:\n    \"\"\"Compare specific gene stats from NCBI\"\"\"\n    stats: List[str] = []\n\n    maps = [\n        [\"total\", \"ALL_GENES\"],\n        [\"protein_coding\", \"PROT_gene\"],\n        [\"pseudogene\", \"pseudogene\"],\n        [\"non_coding\", \"NONPROT_gene\"],\n        [\"other\", \"OTHER\"],\n    ]\n\n    for count_map in maps:\n        ncbi_name, prep_name = count_map\n        ncbi_count = ncbi.get(ncbi_name, 0)\n        preped: Optional[BiotypeCounter] = biotypes.get(prep_name)\n        prep_count = 0\n        if preped is not None:\n            prep_count = preped.count\n\n        if prep_count != ncbi_count:\n            diff = prep_count - ncbi_count\n            stats.append(f\"DIFF gene count for {count_map}: {prep_count} - {ncbi_count} = {diff}\")\n            self.error = True\n        else:\n            stats.append(f\"Same count for {count_map}: {prep_count}\")\n\n    return stats\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/compute_stats/#src.ensembl.io.genomio.manifest.compute_stats.manifest_stats.coord_systems_stats","title":"<code>coord_systems_stats(coord_systems)</code>","text":"For each coord_system compute various stats <ul> <li>number of sequences</li> <li>sequence length sum, minimum, maximum, mean</li> </ul> <p>Parameters:</p> Name Type Description Default <code>coord_systems</code> <code>Dict[str, List[int]]</code> <p>Coordinate system dictionary of lengths.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list with the computed statistics in a printable format.</p> Source code in <code>src/ensembl/io/genomio/manifest/compute_stats.py</code> <pre><code>def coord_systems_stats(self, coord_systems: Dict[str, List[int]]) -&gt; List[str]:\n    \"\"\"For each coord_system compute various stats:\n        - number of sequences\n        - sequence length sum, minimum, maximum, mean\n\n    Args:\n        coord_systems: Coordinate system dictionary of lengths.\n\n    Returns:\n        A list with the computed statistics in a printable format.\n    \"\"\"\n    stats: List[str] = []\n    stats.append(f\"Total coord_systems {len(coord_systems)}\")\n    for coord_name, lengths in coord_systems.items():\n        stats.append(f\"\\nCoord_system: {coord_name}\")\n\n        stat_counts: Dict[str, Union[int, float]] = {\n            \"Number of sequences\": len(lengths),\n            \"Sequence length sum\": sum(lengths),\n            \"Sequence length minimum\": min(lengths),\n            \"Sequence length maximum\": max(lengths),\n            \"Sequence length mean\": mean(lengths),\n        }\n\n        for name, count in stat_counts.items():\n            if isinstance(count, int):\n                stats.append(f\"{count: 9d}\\t{name}\")\n            else:\n                stats.append(f\"{count: 9f}\\t{name}\")\n    return stats\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/compute_stats/#src.ensembl.io.genomio.manifest.compute_stats.manifest_stats.count_biotypes","title":"<code>count_biotypes(gff3_path)</code>","text":"<p>Count the biotypes in a GFF3 file.</p> <p>Parameters:</p> Name Type Description Default <code>gff3_path</code> <code>Path</code> <p>Path to the GFF3 file.</p> required <p>Returns:</p> Type Description <code>Dict[str, BiotypeCounter]</code> <p>Dictionary of biotype counters.</p> Source code in <code>src/ensembl/io/genomio/manifest/compute_stats.py</code> <pre><code>def count_biotypes(self, gff3_path: Path) -&gt; Dict[str, BiotypeCounter]:\n    \"\"\"Count the biotypes in a GFF3 file.\n\n    Args:\n        gff3_path: Path to the GFF3 file.\n\n    Returns:\n        Dictionary of biotype counters.\n    \"\"\"\n\n    biotypes: Dict[str, BiotypeCounter] = {}\n\n    with open_gz_file(gff3_path) as gff3_handle:\n        for rec in GFF.parse(gff3_handle):\n            for feat1 in rec.features:\n                # Check if the gene contains proteins (CDSs),\n                # and keep a count of all hierarchies (e.g. gene-mRNA-CDS)\n                is_protein = False\n                for feat2 in feat1.sub_features:\n                    if feat2.type == \"mRNA\":\n                        types2 = {f.type for f in feat2.sub_features}\n                        if \"CDS\" in types2:\n                            is_protein = True\n                    manifest_stats.increment_biotype(biotypes, feat2.id, f\"{feat1.type}-{feat2.type}\")\n                    for feat3 in feat2.sub_features:\n                        if feat3.type == \"exon\":\n                            continue\n                        manifest_stats.increment_biotype(\n                            biotypes, feat3.id, f\"{feat1.type}-{feat2.type}-{feat3.type}\"\n                        )\n\n                # Main categories counts\n                if feat1.type == \"pseudogene\":\n                    manifest_stats.increment_biotype(biotypes, feat1.id, \"pseudogene\")\n                elif is_protein:\n                    manifest_stats.increment_biotype(biotypes, feat1.id, f\"PROT_{feat1.type}\")\n                else:\n                    # Special case, undefined gene-transcript\n                    if (\n                        feat1.type == \"gene\"\n                        and feat1.sub_features\n                        and feat1.sub_features[0].type == \"transcript\"\n                    ):\n                        manifest_stats.increment_biotype(biotypes, feat1.id, \"OTHER\")\n                    else:\n                        manifest_stats.increment_biotype(biotypes, feat1.id, f\"NONPROT_{feat1.type}\")\n\n                # Total\n                if feat1.type in (\"gene\", \"pseudogene\"):\n                    manifest_stats.increment_biotype(biotypes, feat1.id, \"ALL_GENES\")\n    return biotypes\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/compute_stats/#src.ensembl.io.genomio.manifest.compute_stats.manifest_stats.get_gff3_stats","title":"<code>get_gff3_stats(gff3_path)</code>","text":"<p>Extract the gene models from the GFF3 file and compute stats.</p> <p>Parameters:</p> Name Type Description Default <code>gff3_path</code> <code>Path</code> <p>the GFF3 file.</p> required <p>Returns:</p> Name Type Description <code>List</code> <code>List[str]</code> <p>Stats from the gene model.</p> Source code in <code>src/ensembl/io/genomio/manifest/compute_stats.py</code> <pre><code>def get_gff3_stats(self, gff3_path: Path) -&gt; List[str]:\n    \"\"\"Extract the gene models from the GFF3 file and compute stats.\n\n    Args:\n        gff3_path (Path): the GFF3 file.\n\n    Returns:\n        List: Stats from the gene model.\n    \"\"\"\n\n    biotypes = self.count_biotypes(gff3_path)\n    # Compile final stats\n    stats = self.biotypes_stats(biotypes)\n    stats += self.check_ncbi_stats(biotypes)\n    return stats\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/compute_stats/#src.ensembl.io.genomio.manifest.compute_stats.manifest_stats.get_manifest","title":"<code>get_manifest()</code>","text":"<p>Get the files metadata from the manifest json file.</p> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>A representation of the manifest json data.</p> Source code in <code>src/ensembl/io/genomio/manifest/compute_stats.py</code> <pre><code>def get_manifest(self) -&gt; Dict:\n    \"\"\"Get the files metadata from the manifest json file.\n\n    Returns:\n        Dict: A representation of the manifest json data.\n    \"\"\"\n    with open(self.manifest) as f_json:\n        manifest = json.load(f_json)\n        manifest_root = self.manifest_parent\n\n    # Use dir name from the manifest\n    for name in manifest:\n        if \"file\" in manifest[name]:\n            file_name = manifest[name][\"file\"]\n            file_name = f\"{manifest_root}/{file_name}\"\n            manifest[name] = file_name\n        else:\n            for f in manifest[name]:\n                if \"file\" in manifest[name][f]:\n                    file_name = manifest[name][f][\"file\"]\n                    file_name = manifest_root, file_name\n                    manifest[name][f] = file_name\n\n    return manifest\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/compute_stats/#src.ensembl.io.genomio.manifest.compute_stats.manifest_stats.get_seq_region_stats","title":"<code>get_seq_region_stats(seq_region_path)</code>","text":"<p>Compute stats from the seq_region json file.</p> <p>Parameters:</p> Name Type Description Default <code>seq_region_path</code> <code>Path</code> <p>the seq_region json file.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: Stats from the seq_regions.</p> Source code in <code>src/ensembl/io/genomio/manifest/compute_stats.py</code> <pre><code>def get_seq_region_stats(self, seq_region_path: Path) -&gt; List[str]:\n    \"\"\"Compute stats from the seq_region json file.\n\n    Args:\n        seq_region_path (Path): the seq_region json file.\n\n    Returns:\n        List[str]: Stats from the seq_regions.\n    \"\"\"\n    with seq_region_path.open(\"r\") as json_file:\n        seq_regions = json.load(json_file)\n\n    # Get basic data\n    coord_systems: Dict[str, List[int]] = {}\n    circular = 0\n    locations = []\n    codon_tables = []\n    for seqr in seq_regions:\n        # Get readable seq_region name:\n        # either use a Genbank synonym, or just the provided seq_region name\n        genbank = \"synonyms\" in seqr and [x for x in seqr[\"synonyms\"] if x[\"source\"] == \"GenBank\"]\n        seqr_name = genbank and genbank[0][\"name\"] or seqr[\"name\"]\n\n        # Record the lengths of the elements of each coord_system\n        coord_level = seqr[\"coord_system_level\"]\n        if coord_level not in coord_systems:\n            coord_systems[coord_level] = []\n        coord_systems[coord_level].append(seqr[\"length\"])\n\n        # Additional metadata records to count\n        if \"circular\" in seqr:\n            circular += 1\n        if \"codon_table\" in seqr:\n            codon_tables.append(f\"{seqr_name} = {seqr['codon_table']}\")\n        if \"location\" in seqr:\n            locations.append(f\"{seqr_name} = {seqr['location']}\")\n\n    # Stats\n    stats: List[str] = []\n    stats.append(seq_region_path.name)\n    stats += self.coord_systems_stats(coord_systems)\n    stats += self.seq_region_special_stats(circular, locations, codon_tables)\n    stats.append(\"\\n\")\n    return stats\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/compute_stats/#src.ensembl.io.genomio.manifest.compute_stats.manifest_stats.increment_biotype","title":"<code>increment_biotype(biotypes, feature_id, feature_biotype)</code>  <code>staticmethod</code>","text":"<p>Add the feature to their respective biotype counter.</p> <p>Parameters:</p> Name Type Description Default <code>biotypes</code> <code>Dict[str, BiotypeCounter]</code> <p>All current biotypes, with their counter.</p> required <code>feature_id</code> <code>str</code> <p>Feature id to be counted.</p> required <code>feature_biotype</code> <code>str</code> <p>The biotype of the feature.</p> required Source code in <code>src/ensembl/io/genomio/manifest/compute_stats.py</code> <pre><code>@staticmethod\ndef increment_biotype(biotypes: Dict[str, BiotypeCounter], feature_id: str, feature_biotype: str) -&gt; None:\n    \"\"\"Add the feature to their respective biotype counter.\n\n    Args:\n        biotypes (Dict[str, BiotypeCounter]): All current biotypes, with their counter.\n        feature_id (str): Feature id to be counted.\n        feature_biotype (str): The biotype of the feature.\n    \"\"\"\n    if feature_biotype not in biotypes:\n        biotypes[feature_biotype] = BiotypeCounter(example=feature_id)\n    biotypes[feature_biotype].add_id(feature_id)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/compute_stats/#src.ensembl.io.genomio.manifest.compute_stats.manifest_stats.run","title":"<code>run(stats_path)</code>","text":"<p>Compute stats in the files and output a stats.txt file in the same folder.</p> <p>Raises:</p> Type Description <code>StatsError</code> <p>Could not compute some stats.</p> Source code in <code>src/ensembl/io/genomio/manifest/compute_stats.py</code> <pre><code>def run(self, stats_path: PathLike):\n    \"\"\"Compute stats in the files and output a stats.txt file in the same folder.\n\n    Raises:\n        StatsError: Could not compute some stats.\n    \"\"\"\n    manifest = self.get_manifest()\n\n    stats = []\n    if self.accession is not None:\n        stats.append(self.accession)\n\n    # Compute the stats from the GFF3 file\n    if \"gff3\" in manifest:\n        stats += self.get_gff3_stats(Path(manifest[\"gff3\"]))\n\n    # Compute the stats from the seq_region file\n    if \"seq_region\" in manifest:\n        stats += self.get_seq_region_stats(Path(manifest[\"seq_region\"]))\n\n    # Print out the stats in a separate file\n    with Path(stats_path).open(\"w\") as stats_out:\n        stats_out.write(\"\\n\".join(stats))\n\n    # Die if there were errors in stats comparison\n    if self.error:\n        raise StatsError(f\"Stats count errors, check the file {stats_path}\")\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/compute_stats/#src.ensembl.io.genomio.manifest.compute_stats.manifest_stats.seq_region_special_stats","title":"<code>seq_region_special_stats(circular=0, locations=None, codon_tables=None)</code>","text":"<p>Prepare stats in case there are circular regions, specific locations and codon_tables.         stats.append(f\"{count: 9f}      {name}\")</p> <p>Parameters:</p> Name Type Description Default <code>circular</code> <code>int</code> <p>Number of circular regions. Defaults to 0.</p> <code>0</code> <code>locations</code> <code>Optional[List[str]]</code> <p>The regions and their location. Defaults to None.</p> <code>None</code> <code>codon_tables</code> <code>Optional[List[str]]</code> <p>The regions and their codon_table. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list with the computed statistics in a printable format.</p> Source code in <code>src/ensembl/io/genomio/manifest/compute_stats.py</code> <pre><code>def seq_region_special_stats(\n    self,\n    circular: int = 0,\n    locations: Optional[List[str]] = None,\n    codon_tables: Optional[List[str]] = None,\n) -&gt; List[str]:\n    \"\"\"Prepare stats in case there are circular regions, specific locations and codon_tables.\n            stats.append(f\"{count: 9f}\\t{name}\")\n\n    Args:\n        circular: Number of circular regions. Defaults to 0.\n        locations: The regions and their location. Defaults to None.\n        codon_tables: The regions and their codon_table. Defaults to None.\n\n    Returns:\n        A list with the computed statistics in a printable format.\n    \"\"\"\n    stats: List[str] = []\n    if circular or locations or codon_tables:\n        stats.append(\"\\nSpecial\")\n        if circular:\n            stats.append(f\"{circular: 9d}\\tcircular sequences\")\n        if locations is not None:\n            stats.append(f\"{len(locations): 9d} sequences with location\")\n            for loc in locations:\n                stats.append(f\"\\t\\t\\t{loc}\")\n        if codon_tables:\n            stats.append(f\"{len(codon_tables): 9d} sequences with codon_table\")\n            for table in codon_tables:\n                stats.append(f\"\\t\\t\\t{table}\")\n    return stats\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/compute_stats/#src.ensembl.io.genomio.manifest.compute_stats.main","title":"<code>main()</code>","text":"<p>Main entrypoint.</p> Source code in <code>src/ensembl/io/genomio/manifest/compute_stats.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entrypoint.\"\"\"\n    parser = ArgumentParser(\n        description=\"Compute stats from the current genome files associated with the manifest.\"\n    )\n    parser.add_argument_src_path(\n        \"--manifest_dir\", required=True, help=\"Manifest directory where 'manifest.json' file is located\"\n    )\n    parser.add_argument(\"--accession\", help=\"Sequence accession ID to compare stats with NCBI\")\n    parser.add_argument(\"--datasets_bin\", help=\"Datasets bin status\")\n    parser.add_argument_dst_path(\"--stats_file\", help=\"Output file with the stats\")\n    args = parser.parse_args()\n\n    mstats = manifest_stats(args.manifest_dir, args.accession, args.datasets_bin)\n    if args.accession is not None:\n        mstats.check_ncbi = True\n    stats_file = args.stats_file if args.stats_file is not None else args.manifest_dir / \"stats.txt\"\n    mstats.run(stats_file)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/generate/","title":"generate","text":"<p>Creates a manifest file in a folder depending on the file names ends.</p>"},{"location":"reference/ensembl/io/genomio/manifest/generate/#src.ensembl.io.genomio.manifest.generate.ManifestMaker","title":"<code>ManifestMaker</code>","text":"<p>Given a directory with genomic files, create a manifest json file for them.</p> Source code in <code>src/ensembl/io/genomio/manifest/generate.py</code> <pre><code>class ManifestMaker:\n    \"\"\"Given a directory with genomic files, create a manifest json file for them.\"\"\"\n\n    same_names = {\n        \"gff3\",\n        \"fasta_dna\",\n        \"fasta_pep\",\n        \"functional_annotation\",\n        \"genome\",\n        \"seq_attrib\",\n        \"seq_region\",\n        \"agp\",\n        \"events\",\n    }\n    alias_names = {\n        \"gene_models\": \"gff3\",\n        \"dna\": \"fasta_dna\",\n        \"pep\": \"fasta_pep\",\n    }\n    names = {name: name for name in same_names}\n    names = {**names, **alias_names}\n\n    def __init__(self, manifest_dir: Path) -&gt; None:\n        self.dir = manifest_dir\n\n    def create_manifest(self) -&gt; Path:\n        \"\"\"Create the manifest file.\"\"\"\n        manifest_data = self.get_files_checksums()\n        manifest_path = self.dir / \"manifest.json\"\n        with manifest_path.open(\"w\") as json_out:\n            json_out.write(json.dumps(manifest_data, sort_keys=True, indent=4))\n        return manifest_path\n\n    def get_files_checksums(self):\n        \"\"\"Compute the checksum of all the files in the directory.\"\"\"\n        manifest_files = {}\n        for subfile in self.dir.iterdir():\n            used_file = False\n            if subfile.is_dir():\n                print(\"Can't create manifest for subdirectory\")\n                continue\n\n            for name, standard_name in self.names.items():\n                if subfile.stem.endswith(name):\n                    used_file = True\n                    md5 = self._get_md5sum(subfile)\n                    file_obj = {\"file\": subfile.name, \"md5sum\": md5}\n                    if standard_name in manifest_files:\n                        if isinstance(manifest_files[standard_name], list):\n                            manifest_files[standard_name].append(file_obj)\n                        else:\n                            # Convert to a list\n                            manifest_files[standard_name] = [manifest_files[standard_name], file_obj]\n\n                    else:\n                        manifest_files[standard_name] = file_obj\n                    break\n\n            if not used_file:\n                print(f\"File {subfile} was not included in the manifest\")\n\n        return manifest_files\n\n    @staticmethod\n    def _get_md5sum(file_path: Path) -&gt; str:\n        with file_path.open(\"rb\") as f:\n            data_bytes = f.read()\n            return hashlib.md5(data_bytes).hexdigest()\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/generate/#src.ensembl.io.genomio.manifest.generate.ManifestMaker.create_manifest","title":"<code>create_manifest()</code>","text":"<p>Create the manifest file.</p> Source code in <code>src/ensembl/io/genomio/manifest/generate.py</code> <pre><code>def create_manifest(self) -&gt; Path:\n    \"\"\"Create the manifest file.\"\"\"\n    manifest_data = self.get_files_checksums()\n    manifest_path = self.dir / \"manifest.json\"\n    with manifest_path.open(\"w\") as json_out:\n        json_out.write(json.dumps(manifest_data, sort_keys=True, indent=4))\n    return manifest_path\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/generate/#src.ensembl.io.genomio.manifest.generate.ManifestMaker.get_files_checksums","title":"<code>get_files_checksums()</code>","text":"<p>Compute the checksum of all the files in the directory.</p> Source code in <code>src/ensembl/io/genomio/manifest/generate.py</code> <pre><code>def get_files_checksums(self):\n    \"\"\"Compute the checksum of all the files in the directory.\"\"\"\n    manifest_files = {}\n    for subfile in self.dir.iterdir():\n        used_file = False\n        if subfile.is_dir():\n            print(\"Can't create manifest for subdirectory\")\n            continue\n\n        for name, standard_name in self.names.items():\n            if subfile.stem.endswith(name):\n                used_file = True\n                md5 = self._get_md5sum(subfile)\n                file_obj = {\"file\": subfile.name, \"md5sum\": md5}\n                if standard_name in manifest_files:\n                    if isinstance(manifest_files[standard_name], list):\n                        manifest_files[standard_name].append(file_obj)\n                    else:\n                        # Convert to a list\n                        manifest_files[standard_name] = [manifest_files[standard_name], file_obj]\n\n                else:\n                    manifest_files[standard_name] = file_obj\n                break\n\n        if not used_file:\n            print(f\"File {subfile} was not included in the manifest\")\n\n    return manifest_files\n</code></pre>"},{"location":"reference/ensembl/io/genomio/manifest/generate/#src.ensembl.io.genomio.manifest.generate.main","title":"<code>main()</code>","text":"<p>Main entrypoint.</p> Source code in <code>src/ensembl/io/genomio/manifest/generate.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entrypoint.\"\"\"\n    parser = ArgumentParser(\n        description=\"Compare the genomic data between the files present in a manifest file.\"\n    )\n    parser.add_argument_dst_path(\n        \"--manifest_dir\", required=True, help=\"Folder where to create a manifest file\"\n    )\n    args = parser.parse_args()\n\n    maker = ManifestMaker(args.manifest_dir)\n    maker.create_manifest()\n</code></pre>"},{"location":"reference/ensembl/io/genomio/schemas/","title":"schemas","text":"<p>Handling and verification of different format schemas module.</p>"},{"location":"reference/ensembl/io/genomio/schemas/json/","title":"json","text":"<p>Handling and verification of JSON schemas module.</p>"},{"location":"reference/ensembl/io/genomio/schemas/json/factory/","title":"factory","text":"<p>Generates one JSON file per metadata type inside <code>manifest</code>, including the manifest itself.</p>"},{"location":"reference/ensembl/io/genomio/schemas/json/factory/#src.ensembl.io.genomio.schemas.json.factory.json_schema_factory","title":"<code>json_schema_factory(manifest_dir, metadata_types, output_dir)</code>","text":"<p>Generates one JSON file per metadata type inside <code>manifest</code>, including \"manifest.json\" itself.</p> <p>Each JSON file will have the file name of the metadata type, e.g. \"seq_region.json\".</p> <p>Parameters:</p> Name Type Description Default <code>manifest_dir</code> <code>PathLike</code> <p>Path to the folder with the manifest JSON file to check.</p> required <code>metadata_types</code> <code>List[str]</code> <p>Metadata types to extract from <code>manifest</code> as JSON files.</p> required <code>output_dir</code> <code>PathLike</code> <p>Path to the folder where to generate the JSON files.</p> required Source code in <code>src/ensembl/io/genomio/schemas/json/factory.py</code> <pre><code>def json_schema_factory(manifest_dir: PathLike, metadata_types: List[str], output_dir: PathLike) -&gt; None:\n    \"\"\"Generates one JSON file per metadata type inside `manifest`, including \"manifest.json\" itself.\n\n    Each JSON file will have the file name of the metadata type, e.g. \"seq_region.json\".\n\n    Args:\n        manifest_dir: Path to the folder with the manifest JSON file to check.\n        metadata_types: Metadata types to extract from `manifest` as JSON files.\n        output_dir: Path to the folder where to generate the JSON files.\n\n    \"\"\"\n    manifest_path = Path(manifest_dir, \"manifest.json\")\n    with manifest_path.open() as manifest_file:\n        content = json.load(manifest_file)\n        shutil.copyfile(manifest_path, Path(output_dir, \"manifest.json\"))\n        json_files = {}\n        # Use dir name from the manifest\n        for name in content:\n            if \"file\" in content[name]:\n                file_name = content[name][\"file\"]\n                json_files[name] = manifest_path.parent / file_name\n            else:\n                for key in content[name]:\n                    if \"file\" in content[name][key]:\n                        file_name = content[name][key][\"file\"]\n                        json_files[name] = {key: manifest_path.parent / file_name}\n        # Check the other JSON schemas\n        for metadata_key in metadata_types:\n            if metadata_key in json_files:\n                if isinstance(json_files[metadata_key], dict):\n                    for key, filepath in json_files[metadata_key].items():\n                        shutil.copyfile(filepath, Path(output_dir, f\"{metadata_key}_{key}.json\"))\n                else:\n                    shutil.copyfile(json_files[metadata_key], Path(output_dir, f\"{metadata_key}.json\"))\n</code></pre>"},{"location":"reference/ensembl/io/genomio/schemas/json/factory/#src.ensembl.io.genomio.schemas.json.factory.main","title":"<code>main()</code>","text":"<p>Main script entry-point.</p> Source code in <code>src/ensembl/io/genomio/schemas/json/factory.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main script entry-point.\"\"\"\n    parser = ArgumentParser(\n        description=\"Generates one JSON file per metadata type in the provided manifest, including itself.\"\n    )\n    parser.add_argument_src_path(\n        \"--manifest_dir\", required=True, help=\"Folder containing the 'manifest.json' file to check\"\n    )\n    parser.add_argument(\n        \"--metadata_types\", required=True, nargs=\"+\", metavar=\"TYPE\", help=\"Metadata types to extract\"\n    )\n    parser.add_argument_dst_path(\n        \"--output_dir\", default=Path.cwd(), help=\"Folder to store the produced files\"\n    )\n    args = parser.parse_args()\n\n    json_schema_factory(**vars(args))\n</code></pre>"},{"location":"reference/ensembl/io/genomio/schemas/json/validate/","title":"validate","text":"<p>Validates a JSON file with the provided JSON schema.</p>"},{"location":"reference/ensembl/io/genomio/schemas/json/validate/#src.ensembl.io.genomio.schemas.json.validate.json_schema_validator","title":"<code>json_schema_validator(json_file, json_schema)</code>","text":"<p>Validates a JSON file with the provided JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>json_file</code> <code>PathLike</code> <p>Path to the JSON file to check.</p> required <code>json_schema</code> <code>PathLike</code> <p>JSON schema to validate <code>json_file</code> against.</p> required Example <p>check_json_schema --json_file  --json_schema  Source code in <code>src/ensembl/io/genomio/schemas/json/validate.py</code> <pre><code>def json_schema_validator(json_file: PathLike, json_schema: PathLike) -&gt; None:\n    \"\"\"Validates a JSON file with the provided JSON schema.\n\n    Args:\n        json_file: Path to the JSON file to check.\n        json_schema: JSON schema to validate `json_file` against.\n\n    Example:\n        check_json_schema --json_file &lt;JSON_FILE&gt; --json_schema &lt;JSON_SCHEMA&gt;\n    \"\"\"\n\n    # Open IO for JSON files and validate it\n    with Path(json_file).open(\"r\") as fh:\n        content = json.load(fh)\n    with Path(json_schema).open(\"r\") as fh:\n        schema = json.load(fh)\n    jsonschema.validate(instance=content, schema=schema)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/schemas/json/validate/#src.ensembl.io.genomio.schemas.json.validate.main","title":"<code>main()</code>","text":"<p>Main script entry-point.</p> Source code in <code>src/ensembl/io/genomio/schemas/json/validate.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main script entry-point.\"\"\"\n    parser = ArgumentParser(description=\"Validates a JSON file against a JSON schema\")\n    parser.add_argument_src_path(\"--json_file\", required=True, help=\"JSON file to check\")\n    parser.add_argument_src_path(\"--json_schema\", required=True, help=\"JSON schema to validate against\")\n    args = parser.parse_args()\n\n    json_schema_validator(**vars(args))\n</code></pre>"},{"location":"reference/ensembl/io/genomio/seq_region/","title":"seq_region","text":"<p>Sequence regions handling module.</p>"},{"location":"reference/ensembl/io/genomio/seq_region/dump/","title":"dump","text":"<p>Fetch all the sequence regions from a core database and print them in JSON format.</p>"},{"location":"reference/ensembl/io/genomio/seq_region/dump/#src.ensembl.io.genomio.seq_region.dump.MapFormatError","title":"<code>MapFormatError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Error when parsing the db map file.</p> Source code in <code>src/ensembl/io/genomio/seq_region/dump.py</code> <pre><code>class MapFormatError(Exception):\n    \"\"\"Error when parsing the db map file.\"\"\"\n</code></pre>"},{"location":"reference/ensembl/io/genomio/seq_region/dump/#src.ensembl.io.genomio.seq_region.dump.add_attribs","title":"<code>add_attribs(seq_region, attrib_dict)</code>","text":"<p>Map seq_regions attribs to a specific name and type defined below.</p> <p>Parameters:</p> Name Type Description Default <code>seq_region</code> <code>Dict</code> <p>A seq_region dict to modify.</p> required <code>attrib_dict</code> <code>Dict</code> <p>The attribs for this seq_region.</p> required Source code in <code>src/ensembl/io/genomio/seq_region/dump.py</code> <pre><code>def add_attribs(seq_region: Dict, attrib_dict: Dict) -&gt; None:\n    \"\"\"Map seq_regions attribs to a specific name and type defined below.\n\n    Args:\n        seq_region (Dict): A seq_region dict to modify.\n        attrib_dict (Dict): The attribs for this seq_region.\n    \"\"\"\n    bool_attribs = {\n        \"circular_seq\": \"circular\",\n        \"non_ref\": \"non_ref\",\n    }\n    int_attribs = {\n        \"codon_table\": \"codon_table\",\n    }\n    string_attribs = {\n        \"BRC4_seq_region_name\": \"BRC4_seq_region_name\",\n        \"EBI_seq_region_name\": \"EBI_seq_region_name\",\n        \"coord_system_tag\": \"coord_system_level\",\n        \"sequence_location\": \"location\",\n    }\n\n    for name, key in bool_attribs.items():\n        value = attrib_dict.get(name)\n        if value:\n            seq_region[key] = bool(value)\n\n    for name, key in int_attribs.items():\n        value = attrib_dict.get(name)\n        if value:\n            seq_region[key] = int(value)\n\n    for name, key in string_attribs.items():\n        value = attrib_dict.get(name)\n        if value:\n            seq_region[key] = str(value)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/seq_region/dump/#src.ensembl.io.genomio.seq_region.dump.get_attribs","title":"<code>get_attribs(seq_region)</code>","text":"<p>Given a seq_region, extract the attribs as value-source items.</p> <p>Parameters:</p> Name Type Description Default <code>seq_region</code> <code>SeqRegion</code> <p>The seq_region from which the attribs are extracted.</p> required <p>Returns:</p> Name Type Description <code>List</code> <code>List</code> <p>All attribs as a dict with 'value' and 'source' keys.</p> Source code in <code>src/ensembl/io/genomio/seq_region/dump.py</code> <pre><code>def get_attribs(seq_region: SeqRegion) -&gt; List:\n    \"\"\"Given a seq_region, extract the attribs as value-source items.\n\n    Args:\n        seq_region (SeqRegion): The seq_region from which the attribs are extracted.\n\n    Returns:\n        List: All attribs as a dict with 'value' and 'source' keys.\n    \"\"\"\n    attribs = seq_region.seq_region_attrib\n    atts = []\n    if attribs:\n        for attrib in attribs:\n            att_obj = {\"value\": attrib.value, \"source\": attrib.attrib_type.code}\n            atts.append(att_obj)\n    return atts\n</code></pre>"},{"location":"reference/ensembl/io/genomio/seq_region/dump/#src.ensembl.io.genomio.seq_region.dump.get_coord_systems","title":"<code>get_coord_systems(session)</code>","text":"<p>Retrieve the coord_system metadata from the current core.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session for the current core.</p> required <p>Returns:</p> Type Description <code>List[CoordSystem]</code> <p>List[CoordSystem]: All coord_systems in the core.</p> Source code in <code>src/ensembl/io/genomio/seq_region/dump.py</code> <pre><code>def get_coord_systems(session: Session) -&gt; List[CoordSystem]:\n    \"\"\"Retrieve the coord_system metadata from the current core.\n\n    Args:\n        session (Session): Session for the current core.\n\n    Returns:\n        List[CoordSystem]: All coord_systems in the core.\n    \"\"\"\n    coord_systems: List[CoordSystem] = []\n    coord_stmt = select(CoordSystem).filter(CoordSystem.attrib.like(\"%default_version%\"))\n    for row in session.execute(coord_stmt).unique().all():\n        coord_systems.append(row[0])\n    return coord_systems\n</code></pre>"},{"location":"reference/ensembl/io/genomio/seq_region/dump/#src.ensembl.io.genomio.seq_region.dump.get_external_db_map","title":"<code>get_external_db_map(map_file)</code>","text":"<p>Class method, set up the map for all SeqRegion objects</p> Source code in <code>src/ensembl/io/genomio/seq_region/dump.py</code> <pre><code>def get_external_db_map(map_file: Path) -&gt; Dict:\n    \"\"\"Class method, set up the map for all SeqRegion objects\"\"\"\n    db_map: Dict[str, str] = {}\n    with map_file.open(\"r\") as map_fh:\n        for line in map_fh:\n            line = line.rstrip()\n            if line.startswith(\"#\") or line.startswith(\" \") or line == \"\":\n                continue\n            parts = line.split(\"\\t\")\n            if not parts[0] or not parts[1]:\n                raise MapFormatError(f\"External db file is not formatted correctly for: {line}\")\n            db_map[parts[1]] = parts[0]\n    return db_map\n</code></pre>"},{"location":"reference/ensembl/io/genomio/seq_region/dump/#src.ensembl.io.genomio.seq_region.dump.get_karyotype","title":"<code>get_karyotype(seq_region)</code>","text":"<p>Given a seq_region, extract the karyotype bands.</p> <p>Parameters:</p> Name Type Description Default <code>seq_region</code> <code>SeqRegion</code> <p>The seq_region from which the karyotype bands are extracted.</p> required <p>Returns:</p> Name Type Description <code>List</code> <code>List</code> <p>All karyotype bands as a dict with values 'start', 'end', 'name' 'stain', 'structure'.</p> Source code in <code>src/ensembl/io/genomio/seq_region/dump.py</code> <pre><code>def get_karyotype(seq_region: SeqRegion) -&gt; List:\n    \"\"\"Given a seq_region, extract the karyotype bands.\n\n    Args:\n        seq_region (SeqRegion): The seq_region from which the karyotype bands are extracted.\n\n    Returns:\n        List: All karyotype bands as a dict with values 'start', 'end', 'name' 'stain', 'structure'.\n    \"\"\"\n    bands = seq_region.karyotype\n    kars = []\n    if bands:\n        for band in bands:\n            kar = {\"start\": band.seq_region_start, \"end\": band.seq_region_end}\n            if band.band:\n                kar[\"name\"] = band.band\n            if band.stain:\n                kar[\"stain\"] = band.stain\n                structure = KARYOTYPE_STRUCTURE.get(band.stain, \"\")\n                if structure:\n                    kar[\"structure\"] = structure\n            kars.append(kar)\n\n    kars = sorted(kars, key=lambda kar: kar[\"name\"])\n    return kars\n</code></pre>"},{"location":"reference/ensembl/io/genomio/seq_region/dump/#src.ensembl.io.genomio.seq_region.dump.get_seq_regions","title":"<code>get_seq_regions(session, external_db_map)</code>","text":"<p>Returns all the sequence regions from the current core database.</p> <p>Include synonyms, attribs and karyotypes. Only the top level sequences are exported.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session from the current core.</p> required <code>external_db_map</code> <code>dict</code> <p>Mapping of external_db names for the synonyms.</p> required Source code in <code>src/ensembl/io/genomio/seq_region/dump.py</code> <pre><code>def get_seq_regions(session: Session, external_db_map: dict) -&gt; List[SeqRegion]:\n    \"\"\"Returns all the sequence regions from the current core database.\n\n    Include synonyms, attribs and karyotypes. Only the top level sequences are exported.\n\n    Args:\n        session (Session): Session from the current core.\n        external_db_map (dict): Mapping of external_db names for the synonyms.\n\n    \"\"\"\n    coord_systems = get_coord_systems(session)\n    seq_regions = []\n\n    for coord_system in coord_systems:\n        # print(f\"Dump coord {coord_system.name}\")\n        seqr_stmt = (\n            select(SeqRegion)\n            .where(SeqRegion.coord_system_id == coord_system.coord_system_id)\n            .options(\n                joinedload(SeqRegion.seq_region_synonym).joinedload(SeqRegionSynonym.external_db),\n                joinedload(SeqRegion.seq_region_attrib).joinedload(SeqRegionAttrib.attrib_type),\n                joinedload(SeqRegion.karyotype),\n            )\n        )\n        for row in session.execute(seqr_stmt).unique().all():\n            seqr: SeqRegion = row[0]\n            seq_region: Dict[str, Any] = {}\n            seq_region = {\"name\": seqr.name, \"length\": seqr.length}\n            synonyms = get_synonyms(seqr, external_db_map)\n            if synonyms:\n                seq_region[\"synonyms\"] = synonyms\n\n            attribs = get_attribs(seqr)\n            if attribs:\n                attrib_dict = {attrib[\"source\"]: attrib[\"value\"] for attrib in attribs}\n                if \"toplevel\" not in attrib_dict:\n                    continue\n                add_attribs(seq_region, attrib_dict)\n            else:\n                # Skip seq_region without attribs, not toplevel\n                continue\n\n            karyotype = get_karyotype(seqr)\n            if karyotype:\n                seq_region[\"karyotype_bands\"] = karyotype\n\n            if \"coord_system_level\" not in seq_region:\n                seq_region[\"coord_system_level\"] = coord_system.name\n\n            seq_regions.append(seq_region)\n\n    seq_regions = sorted(seq_regions, key=lambda seqr: (seqr[\"coord_system_level\"], seqr[\"name\"]))\n    return seq_regions\n</code></pre>"},{"location":"reference/ensembl/io/genomio/seq_region/dump/#src.ensembl.io.genomio.seq_region.dump.get_synonyms","title":"<code>get_synonyms(seq_region, external_db_map)</code>","text":"<p>Get all synonyms for a given seq_region. Use the mapping for synonym source names.</p> <p>Parameters:</p> Name Type Description Default <code>seq_region</code> <code>SeqRegion</code> <p>Seq_region from which the synonyms are extracted.</p> required <code>external_db_map</code> <code>dict</code> <p>To map the synonym source names.</p> required <p>Returns:</p> Name Type Description <code>List</code> <code>List</code> <p>All synonyms as a dict with 'name' and 'source' keys.</p> Source code in <code>src/ensembl/io/genomio/seq_region/dump.py</code> <pre><code>def get_synonyms(seq_region: SeqRegion, external_db_map: dict) -&gt; List:\n    \"\"\"Get all synonyms for a given seq_region. Use the mapping for synonym source names.\n\n    Args:\n        seq_region (SeqRegion): Seq_region from which the synonyms are extracted.\n        external_db_map (dict): To map the synonym source names.\n\n    Returns:\n        List: All synonyms as a dict with 'name' and 'source' keys.\n    \"\"\"\n    synonyms = seq_region.seq_region_synonym\n    syns = []\n    if synonyms:\n        for syn in synonyms:\n            if syn.external_db:\n                source = syn.external_db.db_name\n                if source in external_db_map:\n                    source = external_db_map[source]\n                syn_obj = {\"name\": syn.synonym, \"source\": source}\n            else:\n                syn_obj = {\"name\": syn.synonym}\n            syns.append(syn_obj)\n\n    syns = sorted(syns, key=lambda syn: (syn[\"name\"], syn.get(\"source\", \"\")))\n    return syns\n</code></pre>"},{"location":"reference/ensembl/io/genomio/seq_region/dump/#src.ensembl.io.genomio.seq_region.dump.main","title":"<code>main()</code>","text":"<p>Main script entry-point.</p> Source code in <code>src/ensembl/io/genomio/seq_region/dump.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main script entry-point.\"\"\"\n    parser = ArgumentParser(\n        description=\"Fetch all the sequence regions from a core database and print them in JSON format.\"\n    )\n    parser.add_server_arguments(include_database=True)\n    parser.add_argument_src_path(\n        \"--external_db_map\", default=DEFAULT_MAP.resolve(), help=\"File with external_db mapping\"\n    )\n    args = parser.parse_args()\n\n    dbc = DBConnection(args.url)\n\n    external_map_path = Path(args.external_db_map)\n    external_map = get_external_db_map(external_map_path)\n\n    with dbc.session_scope() as session:\n        seq_regions = get_seq_regions(session, external_map)\n\n    print(json.dumps(seq_regions, indent=2))\n</code></pre>"},{"location":"reference/ensembl/io/genomio/seq_region/prepare/","title":"prepare","text":"<p>Construct a seq_region metadata file from INSDC files.</p>"},{"location":"reference/ensembl/io/genomio/seq_region/prepare/#src.ensembl.io.genomio.seq_region.prepare.UnknownMetadata","title":"<code>UnknownMetadata</code>","text":"<p>             Bases: <code>Exception</code></p> <p>If a metadata if not supported or recognized.</p> Source code in <code>src/ensembl/io/genomio/seq_region/prepare.py</code> <pre><code>class UnknownMetadata(Exception):\n    \"\"\"If a metadata if not supported or recognized.\"\"\"\n</code></pre>"},{"location":"reference/ensembl/io/genomio/seq_region/prepare/#src.ensembl.io.genomio.seq_region.prepare.add_insdc_seq_region_name","title":"<code>add_insdc_seq_region_name(seq_regions, synonym_sources=None)</code>","text":"<p>Returns the list of sequence regions with their corresponding INSDC sequence region names.</p> <p>\"BRC4_seq_region_name\" and \"EBI_seq_region_name\" fields are added to each sequence region: the former will contain the corresponding INSDC name whilst the latter will contain the current name.</p> <p>Parameters:</p> Name Type Description Default <code>seq_regions</code> <code>List[SeqRegion]</code> <p>Sequence regions.</p> required <code>synonym_sources</code> <code>Optional[List[str]]</code> <p>Synonym sources to use for the BRC4 name, in order of preference.</p> <code>None</code> <p>Raises:</p> Type Description <code>UnknownMetadata</code> <p>If no synonym name is found for a sequence region.</p> Source code in <code>src/ensembl/io/genomio/seq_region/prepare.py</code> <pre><code>def add_insdc_seq_region_name(\n    seq_regions: List[SeqRegion], synonym_sources: Optional[List[str]] = None\n) -&gt; List[SeqRegion]:\n    \"\"\"Returns the list of sequence regions with their corresponding INSDC sequence region names.\n\n    \"BRC4_seq_region_name\" and \"EBI_seq_region_name\" fields are added to each sequence region: the\n    former will contain the corresponding INSDC name whilst the latter will contain the current name.\n\n    Args:\n        seq_regions: Sequence regions.\n        synonym_sources: Synonym sources to use for the BRC4 name, in order of preference.\n\n    Raises:\n        UnknownMetadata: If no synonym name is found for a sequence region.\n\n    \"\"\"\n    if synonym_sources is None:\n        synonym_sources = SYNONYM_RESOURCES\n    new_seq_regions = []\n    for seqr in seq_regions:\n        names = {synonym[\"source\"]: synonym[\"name\"] for synonym in seqr.get(\"synonyms\", [])}\n        # Choose the synonym to use as the BRC name, using the first valid name from the source list\n        for source_name in synonym_sources:\n            if source_name in names:\n                brc_name = names[source_name]\n                break\n        else:\n            raise UnknownMetadata(f'Cannot set BRC4 sequence region name for {seqr[\"name\"]}')\n        brc_name = brc_name.partition(\".\")[0]\n        seqr[\"BRC4_seq_region_name\"] = brc_name\n        seqr[\"EBI_seq_region_name\"] = seqr[\"name\"]\n        new_seq_regions.append(seqr)\n    return new_seq_regions\n</code></pre>"},{"location":"reference/ensembl/io/genomio/seq_region/prepare/#src.ensembl.io.genomio.seq_region.prepare.add_mitochondrial_codon_table","title":"<code>add_mitochondrial_codon_table(seq_regions, taxon_id)</code>","text":"<p>Adds the mitochondrial codon table to each sequence region (when missing) based on the taxon ID.</p> <p>If no mitochondrial genetic code can be found for the given taxon ID nothing will be changed.</p> <p>Parameters:</p> Name Type Description Default <code>seq_regions</code> <code>List[SeqRegion]</code> <p>Sequence regions.</p> required <code>taxon_id</code> <code>int</code> <p>The species taxon ID.</p> required Source code in <code>src/ensembl/io/genomio/seq_region/prepare.py</code> <pre><code>def add_mitochondrial_codon_table(seq_regions: List[SeqRegion], taxon_id: int) -&gt; None:\n    \"\"\"Adds the mitochondrial codon table to each sequence region (when missing) based on the taxon ID.\n\n    If no mitochondrial genetic code can be found for the given taxon ID nothing will be changed.\n\n    Args:\n        seq_regions: Sequence regions.\n        taxon_id: The species taxon ID.\n\n    \"\"\"\n    url = f\"https://www.ebi.ac.uk/ena/data/taxonomy/v1/taxon/tax-id/{str(taxon_id)}\"\n    response = requests.get(url, headers={\"Content-Type\": \"application/json\"}, timeout=60)\n    decoded = response.json()\n    if \"mitochondrialGeneticCode\" not in decoded:\n        print(f\"No mitochondria genetic code found for taxon {taxon_id}\")\n    else:\n        genetic_code = int(decoded[\"mitochondrialGeneticCode\"])\n        for seqr in seq_regions:\n            location = seqr.get(\"location\", \"\")\n            # Do not overwrite any existing mitochondrial codon table\n            if (location == \"mitochondrial_chromosome\") and (\"codon_table\" not in seqr):\n                seqr[\"codon_table\"] = genetic_code\n</code></pre>"},{"location":"reference/ensembl/io/genomio/seq_region/prepare/#src.ensembl.io.genomio.seq_region.prepare.add_translation_table","title":"<code>add_translation_table(seq_regions, location_codon=None)</code>","text":"<p>Adds the translation codon table to each sequence region (when missing) based on its location.</p> <p>Parameters:</p> Name Type Description Default <code>seq_regions</code> <code>List[SeqRegion]</code> <p>Sequence regions.</p> required <code>location_codon</code> <code>Optional[Dict[str, int]]</code> <p>Map of known codon tables for known locations.</p> <code>None</code> Source code in <code>src/ensembl/io/genomio/seq_region/prepare.py</code> <pre><code>def add_translation_table(\n    seq_regions: List[SeqRegion], location_codon: Optional[Dict[str, int]] = None\n) -&gt; None:\n    \"\"\"Adds the translation codon table to each sequence region (when missing) based on its location.\n\n    Args:\n        seq_regions: Sequence regions.\n        location_codon: Map of known codon tables for known locations.\n\n    \"\"\"\n    if location_codon is None:\n        location_codon = LOCATION_CODON\n    for seqr in seq_regions:\n        # Do not overwrite any existing codon table\n        if (\"codon_table\" not in seqr) and (\"location\" in seqr) and (seqr[\"location\"] in location_codon):\n            seqr[\"codon_table\"] = location_codon[seqr[\"location\"]]\n</code></pre>"},{"location":"reference/ensembl/io/genomio/seq_region/prepare/#src.ensembl.io.genomio.seq_region.prepare.exclude_seq_regions","title":"<code>exclude_seq_regions(seq_regions, to_exclude)</code>","text":"<p>Returns the list of sequence regions with the ones from the exclusion list removed.</p> <p>Parameters:</p> Name Type Description Default <code>seq_regions</code> <code>List[SeqRegion]</code> <p>Sequence regions.</p> required <code>to_exclude</code> <code>List[str]</code> <p>Sequence region names to exclude.</p> required Source code in <code>src/ensembl/io/genomio/seq_region/prepare.py</code> <pre><code>def exclude_seq_regions(seq_regions: List[SeqRegion], to_exclude: List[str]) -&gt; List[SeqRegion]:\n    \"\"\"Returns the list of sequence regions with the ones from the exclusion list removed.\n\n    Args:\n        seq_regions: Sequence regions.\n        to_exclude: Sequence region names to exclude.\n\n    \"\"\"\n    filtered_seq_regions = []\n    for seqr in seq_regions:\n        if (\"name\" in seqr) and (seqr[\"name\"] in to_exclude):\n            print(f'Remove seq_region {seqr[\"name\"]}')\n        else:\n            filtered_seq_regions.append(seqr)\n    return filtered_seq_regions\n</code></pre>"},{"location":"reference/ensembl/io/genomio/seq_region/prepare/#src.ensembl.io.genomio.seq_region.prepare.get_codon_table","title":"<code>get_codon_table(record)</code>","text":"<p>Returns the codon table number from a given a GenBank sequence record (if present).</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>SeqRecord</code> <p>GenBank sequence record.</p> required Source code in <code>src/ensembl/io/genomio/seq_region/prepare.py</code> <pre><code>def get_codon_table(record: SeqRecord) -&gt; Optional[int]:\n    \"\"\"Returns the codon table number from a given a GenBank sequence record (if present).\n\n    Args:\n        record: GenBank sequence record.\n\n    \"\"\"\n    table_number = None\n    for feat in record.features:\n        if \"transl_table\" in feat.qualifiers:\n            table_number = int(feat.qualifiers[\"transl_table\"][0])\n            break\n    return table_number\n</code></pre>"},{"location":"reference/ensembl/io/genomio/seq_region/prepare/#src.ensembl.io.genomio.seq_region.prepare.get_gbff_seq_regions","title":"<code>get_gbff_seq_regions(gbff_path)</code>","text":"<p>Returns the sequence regions found in the GBFF file (if any).</p> <p>Parameters:</p> Name Type Description Default <code>gbff_path</code> <code>PathLike</code> <p>Path to GBFF file.</p> required <p>Returns:</p> Type Description <code>SeqRegionDict</code> <p>A dict of SeqRegions, with their name as the key.</p> Source code in <code>src/ensembl/io/genomio/seq_region/prepare.py</code> <pre><code>def get_gbff_seq_regions(gbff_path: PathLike) -&gt; SeqRegionDict:\n    \"\"\"Returns the sequence regions found in the GBFF file (if any).\n\n    Args:\n        gbff_path: Path to GBFF file.\n\n    Returns:\n        A dict of SeqRegions, with their name as the key.\n\n    \"\"\"\n    seq_regions = {}\n    with open_gz_file(gbff_path) as gbff_file:\n        for record in SeqIO.parse(gbff_file, \"genbank\"):\n            seqr: SeqRegion = {\"length\": len(record.seq)}\n            # Is the seq_region circular?\n            annotations = record.annotations\n            if (\"topology\" in annotations) and (annotations[\"topology\"] == \"circular\"):\n                seqr[\"circular\"] = True\n            # Is there a genetic code defined?\n            codon_table = get_codon_table(record)\n            if codon_table is not None:\n                seqr[\"codon_table\"] = codon_table\n            # Is it an organelle?\n            location = get_organelle(record)\n            if location is not None:\n                seqr[\"location\"] = location\n            # Is there a comment stating the Genbank record this is based on?\n            genbank_id = get_genbank_id(record)\n            if genbank_id is not None:\n                seqr[\"synonyms\"] = [{\"source\": \"INSDC\", \"name\": genbank_id}]\n            # Store the seq_region\n            seq_regions[record.id] = seqr\n    return seq_regions\n</code></pre>"},{"location":"reference/ensembl/io/genomio/seq_region/prepare/#src.ensembl.io.genomio.seq_region.prepare.get_genbank_id","title":"<code>get_genbank_id(record)</code>","text":"<p>Returns the GenBank accession from a given sequence record (if present).</p> <p>Only useful for RefSeq sequence records, where the GenBank accession is stored in a comment.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>SeqRecord</code> <p>Sequence record.</p> required Source code in <code>src/ensembl/io/genomio/seq_region/prepare.py</code> <pre><code>def get_genbank_id(record: SeqRecord) -&gt; Optional[str]:\n    \"\"\"Returns the GenBank accession from a given sequence record (if present).\n\n    Only useful for RefSeq sequence records, where the GenBank accession is stored in a comment.\n\n    Args:\n        record: Sequence record.\n\n    \"\"\"\n    genbank_id = None\n    if \"comment\" in record.annotations:\n        comment = record.annotations[\"comment\"]\n        comment = re.sub(r\"[ \\n\\r]+\", \" \", comment)\n        match = re.search(r\"The reference sequence was derived from ([^\\.]+)\\.\", comment)\n        if match:\n            genbank_id = match.group(1)\n    return genbank_id\n</code></pre>"},{"location":"reference/ensembl/io/genomio/seq_region/prepare/#src.ensembl.io.genomio.seq_region.prepare.get_organelle","title":"<code>get_organelle(record, molecule_location=None)</code>","text":"<p>Returns the organelle location from the given GenBank record (if present).</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>SeqRecord</code> <p>GenBank sequence record.</p> required <code>molecule_location</code> <code>Optional[Dict]</code> <p>Map of sequence type to SO location.</p> <code>None</code> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the location is not part of the controlled vocabulary.</p> Source code in <code>src/ensembl/io/genomio/seq_region/prepare.py</code> <pre><code>def get_organelle(record: SeqRecord, molecule_location: Optional[Dict] = None) -&gt; Optional[str]:\n    \"\"\"Returns the organelle location from the given GenBank record (if present).\n\n    Args:\n        record: GenBank sequence record.\n        molecule_location: Map of sequence type to SO location.\n\n    Raises:\n        KeyError: If the location is not part of the controlled vocabulary.\n\n    \"\"\"\n    if molecule_location is None:\n        molecule_location = MOLECULE_LOCATION\n    location = None\n    for feat in record.features:\n        if \"organelle\" in feat.qualifiers:\n            organelle = str(feat.qualifiers[\"organelle\"][0])\n            if not organelle:\n                break\n            # Remove plastid prefix\n            with_prefix = re.match(r\"^(plastid|mitochondrion):(.+)$\", organelle)\n            if with_prefix:\n                organelle = with_prefix[2]\n            # Get controlled name\n            try:\n                location = molecule_location[organelle]\n            except KeyError as exc:\n                raise UnknownMetadata(f\"Unrecognized sequence location: {organelle}\") from exc\n            break\n    return location\n</code></pre>"},{"location":"reference/ensembl/io/genomio/seq_region/prepare/#src.ensembl.io.genomio.seq_region.prepare.get_report_regions","title":"<code>get_report_regions(report_path, is_refseq)</code>","text":"<p>Returns the sequence regions found in the report file.</p> <p>Parameters:</p> Name Type Description Default <code>report_path</code> <code>PathLike</code> <p>Path to the sequence regions report file.</p> required <code>is_refseq</code> <code>bool</code> <p>True if the source of the report is RefSeq, false if INSDC.</p> required <p>Returns:</p> Type Description <code>SeqRegionDict</code> <p>A dict of SeqRegions, with their name as the key.</p> Source code in <code>src/ensembl/io/genomio/seq_region/prepare.py</code> <pre><code>def get_report_regions(report_path: PathLike, is_refseq: bool) -&gt; SeqRegionDict:\n    \"\"\"Returns the sequence regions found in the report file.\n\n    Args:\n        report_path: Path to the sequence regions report file.\n        is_refseq: True if the source of the report is RefSeq, false if INSDC.\n\n    Returns:\n        A dict of SeqRegions, with their name as the key.\n\n    \"\"\"\n    # Get the report in a CSV format\n    report_csv = report_to_csv(report_path)[0]\n    # Feed the csv string to the CSV reader\n    reader = csv.DictReader(report_csv.splitlines(), delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n    # Create the seq_regions\n    seq_regions = {}\n    for row in reader:\n        seq_region = make_seq_region(row, is_refseq)\n        if seq_region:\n            name = seq_region[\"name\"]\n            seq_regions[name] = seq_region\n    return seq_regions\n</code></pre>"},{"location":"reference/ensembl/io/genomio/seq_region/prepare/#src.ensembl.io.genomio.seq_region.prepare.main","title":"<code>main()</code>","text":"<p>Module's entry-point.</p> Source code in <code>src/ensembl/io/genomio/seq_region/prepare.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Module's entry-point.\"\"\"\n    parser = ArgumentParser(description=\"Construct a sequence region metadata file from INSDC files.\")\n    parser.add_argument_src_path(\"--genome_file\", required=True, help=\"Genome metadata JSON file\")\n    parser.add_argument_src_path(\n        \"--report_file\", required=True, help=\"INSDC/RefSeq sequences report file to parse\"\n    )\n    parser.add_argument_src_path(\"--gbff_file\", help=\"INSDC/RefSeq GBFF file to parse\")\n    parser.add_argument_dst_path(\n        \"--dst_dir\", default=Path.cwd(), help=\"Output folder for the processed sequence regions JSON file\"\n    )\n    parser.add_argument(\"--brc_mode\", action=\"store_true\", help=\"Enable BRC mode\")\n    parser.add_argument(\n        \"--to_exclude\", nargs=\"*\", metavar=\"SEQ_REGION_NAME\", help=\"Sequence region names to exclude\"\n    )\n    args = parser.parse_args()\n\n    prepare_seq_region_metadata(**vars(args))\n</code></pre>"},{"location":"reference/ensembl/io/genomio/seq_region/prepare/#src.ensembl.io.genomio.seq_region.prepare.make_seq_region","title":"<code>make_seq_region(data, is_refseq, synonym_map=None, molecule_location=None)</code>","text":"<p>Returns a sequence region from the information provided.</p> <p>An empty sequence region will be returned if not accession information is found.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict</code> <p>a dict from the report representing one line, where the key is the column name.</p> required <code>is_refseq</code> <code>bool</code> <p>True if the source is RefSeq, false if INSDC.</p> required <code>synonym_map</code> <code>Optional[Dict[str, str]]</code> <p>Map of INSDC report column names to sequence region field names.</p> <code>None</code> <code>molecule_location</code> <code>Optional[Dict[str, str]]</code> <p>Map of sequence type to SO location.</p> <code>None</code> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the sequence location is not recognised.</p> <code>UnknownMetadata</code> <p>if the sequence role is not recognised.</p> Source code in <code>src/ensembl/io/genomio/seq_region/prepare.py</code> <pre><code>def make_seq_region(\n    data: Dict,\n    is_refseq: bool,\n    synonym_map: Optional[Dict[str, str]] = None,\n    molecule_location: Optional[Dict[str, str]] = None,\n) -&gt; SeqRegion:\n    \"\"\"Returns a sequence region from the information provided.\n\n    An empty sequence region will be returned if not accession information is found.\n\n    Args:\n        data: a dict from the report representing one line, where the key is the column name.\n        is_refseq: True if the source is RefSeq, false if INSDC.\n        synonym_map: Map of INSDC report column names to sequence region field names.\n        molecule_location: Map of sequence type to SO location.\n\n    Raises:\n        KeyError: If the sequence location is not recognised.\n        UnknownMetadata: if the sequence role is not recognised.\n\n    \"\"\"\n    if synonym_map is None:\n        synonym_map = SYNONYM_MAP\n    if molecule_location is None:\n        molecule_location = MOLECULE_LOCATION\n    seq_region = {}\n    # Set accession as the sequence region name\n    src = \"RefSeq\" if is_refseq else \"GenBank\"\n    accession_id = data.get(f\"{src}-Accn\", \"\")\n    if accession_id and (accession_id != \"na\"):\n        seq_region[\"name\"] = accession_id\n    else:\n        print(f'No {src} accession ID found for {data[\"Sequence-Name\"]}')\n        return {}\n    # Add synonyms\n    synonyms = []\n    for field, source in synonym_map.items():\n        if (field in data) and (data[field].casefold() != \"na\"):\n            synonym = {\"source\": source, \"name\": data[field]}\n            synonyms.append(synonym)\n    if synonyms:\n        synonyms.sort(key=lambda x: x[\"source\"])\n        seq_region[\"synonyms\"] = synonyms\n    # Add sequence length\n    field = \"Sequence-Length\"\n    if (field in data) and (data[field].casefold() != \"na\"):\n        seq_region[\"length\"] = int(data[field])\n    # Add coordinate system and location\n    seq_role = data[\"Sequence-Role\"]\n    # Scaffold?\n    if seq_role in (\"unplaced-scaffold\", \"unlocalized-scaffold\"):\n        seq_region[\"coord_system_level\"] = \"scaffold\"\n    # Chromosome? Check location\n    elif seq_role == \"assembled-molecule\":\n        seq_region[\"coord_system_level\"] = \"chromosome\"\n        location = data[\"Assigned-Molecule-Location/Type\"].lower()\n        # Get location metadata\n        try:\n            seq_region[\"location\"] = molecule_location[location]\n        except KeyError as exc:\n            raise UnknownMetadata(f\"Unrecognized sequence location: {location}\") from exc\n    else:\n        raise UnknownMetadata(f\"Unrecognized sequence role: {seq_role}\")\n    return seq_region\n</code></pre>"},{"location":"reference/ensembl/io/genomio/seq_region/prepare/#src.ensembl.io.genomio.seq_region.prepare.merge_seq_regions","title":"<code>merge_seq_regions(left=None, right=None)</code>","text":"<p>Merges sequence regions from different sources.</p> <p>When combining two regions matching the same key, the \"right\" seq region data will take precedence over the \"left\".</p> <p>Parameters:</p> Name Type Description Default <code>left</code> <code>Optional[SeqRegionDict]</code> <p>Dictionary of sequence regions with names as keys.</p> <code>None</code> <code>right</code> <code>Optional[SeqRegionDict]</code> <p>Dictionary of sequence regions with names as keys.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[SeqRegion]</code> <p>A list of merged sequence regions, sorted by their name.</p> Source code in <code>src/ensembl/io/genomio/seq_region/prepare.py</code> <pre><code>def merge_seq_regions(\n    left: Optional[SeqRegionDict] = None, right: Optional[SeqRegionDict] = None\n) -&gt; List[SeqRegion]:\n    \"\"\"Merges sequence regions from different sources.\n\n    When combining two regions matching the same key, the \"right\" seq region data will take precedence\n    over the \"left\".\n\n    Args:\n        left: Dictionary of sequence regions with names as keys.\n        right: Dictionary of sequence regions with names as keys.\n\n    Returns:\n        A list of merged sequence regions, sorted by their name.\n\n    \"\"\"\n    if left is None:\n        left = {}\n    if right is None:\n        right = {}\n    # Get all the names\n    all_names = set(left).union(right)\n    # Create the seq_regions, merge if needed\n    seq_regions = []\n    for name in all_names:\n        left_seqr = left.get(name, {})\n        right_seqr = right.get(name, {})\n        if left_seqr and right_seqr:\n            final_seqr = {**left_seqr, **right_seqr}\n        elif left_seqr:\n            final_seqr = left_seqr\n        else:\n            final_seqr = right_seqr\n        seq_regions.append(final_seqr)\n    seq_regions.sort(key=lambda x: x[\"name\"])\n    return seq_regions\n</code></pre>"},{"location":"reference/ensembl/io/genomio/seq_region/prepare/#src.ensembl.io.genomio.seq_region.prepare.prepare_seq_region_metadata","title":"<code>prepare_seq_region_metadata(genome_file, report_file, dst_dir, gbff_file=None, brc_mode=False, to_exclude=None)</code>","text":"<p>Prepares the sequence region metadata found in the INSDC/RefSeq report and GBFF files.</p> <p>The sequence region information is loaded from both sources and combined. Elements are added/excluded as requested, and the final sequence region metadata is dumped in a JSON file that follows the schema defined in \"schemas/seq_region_schema.json\".</p> <p>Parameters:</p> Name Type Description Default <code>genome_file</code> <code>PathLike</code> <p>Genome metadata JSON file path.</p> required <code>report_file</code> <code>PathLike</code> <p>INSDC/RefSeq sequences report file path to parse.</p> required <code>dst_dir</code> <code>PathLike</code> <p>Output folder for the processed sequence regions JSON file.</p> required <code>gbff_file</code> <code>Optional[PathLike]</code> <p>INSDC/RefSeq GBFF file path to parse.</p> <code>None</code> <code>brc_mode</code> <code>bool</code> <p>Include INSDC sequence region names.</p> <code>False</code> <code>to_exclude</code> <code>Optional[List[str]]</code> <p>Sequence region names to exclude.</p> <code>None</code> Source code in <code>src/ensembl/io/genomio/seq_region/prepare.py</code> <pre><code>def prepare_seq_region_metadata(\n    genome_file: PathLike,\n    report_file: PathLike,\n    dst_dir: PathLike,\n    gbff_file: Optional[PathLike] = None,\n    brc_mode: bool = False,\n    to_exclude: Optional[List[str]] = None,\n) -&gt; None:\n    \"\"\"Prepares the sequence region metadata found in the INSDC/RefSeq report and GBFF files.\n\n    The sequence region information is loaded from both sources and combined. Elements are added/excluded\n    as requested, and the final sequence region metadata is dumped in a JSON file that follows the schema\n    defined in \"schemas/seq_region_schema.json\".\n\n    Args:\n        genome_file: Genome metadata JSON file path.\n        report_file: INSDC/RefSeq sequences report file path to parse.\n        dst_dir: Output folder for the processed sequence regions JSON file.\n        gbff_file: INSDC/RefSeq GBFF file path to parse.\n        brc_mode: Include INSDC sequence region names.\n        to_exclude: Sequence region names to exclude.\n\n    \"\"\"\n    genome_data = get_json(genome_file)\n    dst_dir = Path(dst_dir)\n    dst_dir.mkdir(parents=True, exist_ok=True)\n    # Final file name\n    metadata_type = \"seq_region\"\n    metadata_file = f\"{metadata_type}.json\"\n    final_path = dst_dir / metadata_file\n    is_refseq = genome_data[\"assembly\"][\"accession\"].startswith(\"GCF_\")\n    # Get the sequence regions from the report and gbff files, and merge them\n    report_regions = get_report_regions(report_file, is_refseq)\n    if gbff_file:\n        gbff_regions = get_gbff_seq_regions(gbff_file)\n        seq_regions = merge_seq_regions(report_regions, gbff_regions)\n    else:\n        seq_regions = list(report_regions.values())\n    # Exclude seq_regions from a list\n    if to_exclude is not None:\n        seq_regions = exclude_seq_regions(seq_regions, to_exclude)\n    # Setup the BRC4_seq_region_name\n    if brc_mode:\n        seq_regions = add_insdc_seq_region_name(seq_regions)\n    # Add translation and mitochondrial codon tables\n    add_translation_table(seq_regions)\n    add_mitochondrial_codon_table(seq_regions, genome_data[\"species\"][\"taxonomy_id\"])\n    # Print out the file\n    print_json(final_path, seq_regions)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/seq_region/prepare/#src.ensembl.io.genomio.seq_region.prepare.report_to_csv","title":"<code>report_to_csv(report_path)</code>","text":"<p>Returns an assembly report as a CSV string.</p> <p>Parameters:</p> Name Type Description Default <code>report_path</code> <code>PathLike</code> <p>path to a seq_region file from INSDC/RefSeq</p> required <p>Returns:</p> Type Description <code>Tuple[str, Dict]</code> <p>The data as a string in CSV format, and the head metadata as a dictionary.</p> Source code in <code>src/ensembl/io/genomio/seq_region/prepare.py</code> <pre><code>def report_to_csv(report_path: PathLike) -&gt; Tuple[str, Dict]:\n    \"\"\"Returns an assembly report as a CSV string.\n\n    Args:\n        report_path: path to a seq_region file from INSDC/RefSeq\n\n    Returns:\n        The data as a string in CSV format, and the head metadata as a dictionary.\n\n    \"\"\"\n    with open_gz_file(report_path) as report:\n        data = \"\"\n        metadata = {}\n        last_head = \"\"\n        for line in report:\n            # Ignore header\n            if line.startswith(\"#\"):\n                # Get metadata values if possible\n                match = re.search(\"# (.+?): (.+?)$\", line)\n                if match:\n                    metadata[match.group(1)] = match.group(2)\n                last_head = line\n            else:\n                if last_head:\n                    data += last_head[2:].strip() + \"\\n\"\n                    last_head = \"\"\n                data += line\n        return data, metadata\n</code></pre>"},{"location":"reference/ensembl/io/genomio/utils/","title":"utils","text":"<p>Utils module.</p>"},{"location":"reference/ensembl/io/genomio/utils/archive_utils/","title":"archive_utils","text":"<p>Utils to deal with archive files.</p>"},{"location":"reference/ensembl/io/genomio/utils/archive_utils/#src.ensembl.io.genomio.utils.archive_utils.extract_file","title":"<code>extract_file(src_file, dst_dir)</code>","text":"<p>Extracts the <code>src_file</code> into <code>dst_dir</code>.</p> <p>If the file is not an archive, it will be copied to <code>dst_dir</code>. <code>dst_dir</code> will be created if it does not exist.</p> <p>Parameters:</p> Name Type Description Default <code>src_file</code> <code>PathLike</code> <p>Path to the file to unpack.</p> required <code>dst_dir</code> <code>PathLike</code> <p>Path to where extract the file.</p> required Source code in <code>src/ensembl/io/genomio/utils/archive_utils.py</code> <pre><code>def extract_file(src_file: PathLike, dst_dir: PathLike) -&gt; None:\n    \"\"\"Extracts the `src_file` into `dst_dir`.\n\n    If the file is not an archive, it will be copied to `dst_dir`. `dst_dir` will be created if it\n    does not exist.\n\n    Args:\n        src_file: Path to the file to unpack.\n        dst_dir: Path to where extract the file.\n\n    \"\"\"\n    src_file = Path(src_file)\n    # Create a set of all the possible \"right-side suffixes\", e.g. \"myfile.tar.gz\" produces {\".tar.gz\", \".gz\"}\n    extensions = {\"\".join(src_file.suffixes[i:]) for i in range(0, len(src_file.suffixes))}\n    dst_dir = Path(dst_dir)\n\n    if extensions.intersection(SUPPORTED_ARCHIVE_FORMATS):\n        shutil.unpack_archive(src_file, dst_dir)\n    else:\n        # Replicate the functionality of shutil.unpack_archive() by creating `dst_dir`\n        dst_dir.mkdir(parents=True, exist_ok=True)\n        if extensions.intersection([\".gz\"]):\n            final_path = dst_dir / src_file.stem\n            with gzip.open(src_file, \"rb\") as f_in:\n                with final_path.open(\"wb\") as f_out:\n                    shutil.copyfileobj(f_in, f_out)\n        else:\n            shutil.copy(src_file, dst_dir)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/utils/archive_utils/#src.ensembl.io.genomio.utils.archive_utils.extract_file_cli","title":"<code>extract_file_cli()</code>","text":"<p>Entry-point for the <code>extract_file</code> method.</p> Source code in <code>src/ensembl/io/genomio/utils/archive_utils.py</code> <pre><code>def extract_file_cli() -&gt; None:\n    \"\"\"Entry-point for the `extract_file` method.\"\"\"\n    parser = ArgumentParser(description=\"Extract a file into a directory.\")\n    parser.add_argument_src_path(\"--src_file\", required=True, help=\"File to unpack\")\n    parser.add_argument_dst_path(\n        \"--dst_dir\", default=Path.cwd(), help=\"Output folder where to extract the file\"\n    )\n    args = parser.parse_args()\n\n    extract_file(**vars(args))\n</code></pre>"},{"location":"reference/ensembl/io/genomio/utils/archive_utils/#src.ensembl.io.genomio.utils.archive_utils.open_gz_file","title":"<code>open_gz_file(file_path)</code>","text":"<p>Yields an open file object, even if the file is compressed with gzip.</p> <p>The file is expected to contain a text, and this can be used with the usual \"with\".</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>PathLike</code> <p>A file path to open.</p> required Source code in <code>src/ensembl/io/genomio/utils/archive_utils.py</code> <pre><code>@contextmanager\ndef open_gz_file(file_path: PathLike) -&gt; Generator[TextIO, None, None]:\n    \"\"\"Yields an open file object, even if the file is compressed with gzip.\n\n    The file is expected to contain a text, and this can be used with the usual \"with\".\n\n    Args:\n        file_path: A file path to open.\n\n    \"\"\"\n    this_file = Path(file_path)\n    if this_file.suffix == \".gz\":\n        with gzip.open(this_file, \"rt\") as fh:\n            yield fh\n    else:\n        with this_file.open(\"rt\") as fh:\n            yield fh\n</code></pre>"},{"location":"reference/ensembl/io/genomio/utils/json_utils/","title":"json_utils","text":"<p>Utils to deal with JSON files.</p>"},{"location":"reference/ensembl/io/genomio/utils/json_utils/#src.ensembl.io.genomio.utils.json_utils.get_json","title":"<code>get_json(src_path, **kwargs)</code>","text":"<p>Generic data JSON loader.</p> <p>Parameters:</p> Name Type Description Default <code>src_path</code> <code>PathLike</code> <p>Path to the JSON file to load.</p> required Source code in <code>src/ensembl/io/genomio/utils/json_utils.py</code> <pre><code>def get_json(src_path: PathLike, **kwargs) -&gt; Any:\n    \"\"\"Generic data JSON loader.\n\n    Args:\n        src_path: Path to the JSON file to load.\n\n    \"\"\"\n    with Path(src_path).open(\"r\") as json_file:\n        return json.load(json_file, **kwargs)\n</code></pre>"},{"location":"reference/ensembl/io/genomio/utils/json_utils/#src.ensembl.io.genomio.utils.json_utils.print_json","title":"<code>print_json(dst_path, data, **kwargs)</code>","text":"<p>Generic data JSON dumper to a file, with keys sorted and pretty-printed with indent 4 by default.</p> <p>Parameters:</p> Name Type Description Default <code>dst_path</code> <code>PathLike</code> <p>Path to the JSON file to create.</p> required <code>data</code> <code>Any</code> <p>Any data to store into the file.</p> required Source code in <code>src/ensembl/io/genomio/utils/json_utils.py</code> <pre><code>def print_json(dst_path: PathLike, data: Any, **kwargs) -&gt; None:\n    \"\"\"Generic data JSON dumper to a file, with keys sorted and pretty-printed with indent 4 by default.\n\n    Args:\n        dst_path: Path to the JSON file to create.\n        data: Any data to store into the file.\n\n    \"\"\"\n    kwargs.setdefault(\"sort_keys\", True)\n    kwargs.setdefault(\"indent\", 4)\n    with Path(dst_path).open(\"w\") as json_file:\n        json_file.write(json.dumps(data, **kwargs))\n</code></pre>"},{"location":"reference/example/calculate/","title":"calculate","text":"<p>Do math with your own functions.</p> <p>Modules exported by this package:</p> <ul> <li><code>calculations</code>: Provide several sample math calculations.</li> </ul>"},{"location":"reference/example/calculate/calculations/","title":"calculations","text":"<p>Provide several sample math calculations.</p> <p>This module allows the user to make mathematical calculations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from calculator import calculations\n&gt;&gt;&gt; calculations.add(2, 4)\n6.0\n&gt;&gt;&gt; calculations.multiply(2.0, 4.0)\n8.0\n&gt;&gt;&gt; from calculator.calculations import divide\n&gt;&gt;&gt; divide(4.0, 2)\n2.0\n</code></pre> <p>The module contains the following functions:</p> <ul> <li><code>add(a, b)</code> - Returns the sum of two numbers.</li> <li><code>subtract(a, b)</code> - Returns the difference of two numbers.</li> <li><code>multiply(a, b)</code> - Returns the product of two numbers.</li> <li><code>divide(a, b)</code> - Returns the quotient of two numbers.</li> </ul>"},{"location":"reference/example/calculate/calculations/#src.example.calculate.calculations.add","title":"<code>add(a, b)</code>","text":"<p>Compute and return the sum of two numbers.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; add(4.0, 2.0)\n6.0\n&gt;&gt;&gt; add(4, 2)\n6.0\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Union[float, int]</code> <p>A number representing the first addend in the addition.</p> required <code>b</code> <code>Union[float, int]</code> <p>A number representing the second addend in the addition.</p> required <p>Returns:</p> Type Description <code>float</code> <p>A number representing the arithmetic sum of <code>a</code> and <code>b</code>.</p> Source code in <code>src/example/calculate/calculations.py</code> <pre><code>def add(a: Union[float, int], b: Union[float, int]) -&gt; float:\n    \"\"\"Compute and return the sum of two numbers.\n\n    Examples:\n        &gt;&gt;&gt; add(4.0, 2.0)\n        6.0\n        &gt;&gt;&gt; add(4, 2)\n        6.0\n\n    Args:\n        a: A number representing the first addend in the addition.\n        b: A number representing the second addend in the addition.\n\n    Returns:\n        A number representing the arithmetic sum of `a` and `b`.\n    \"\"\"\n    return float(a + b)\n</code></pre>"},{"location":"reference/example/calculate/calculations/#src.example.calculate.calculations.divide","title":"<code>divide(a, b)</code>","text":"<p>Compute and return the quotient of two numbers.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; divide(4.0, 2.0)\n2.0\n&gt;&gt;&gt; divide(4, 2)\n2.0\n&gt;&gt;&gt; divide(4, 0)\nTraceback (most recent call last):\n...\nZeroDivisionError: division by zero\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Union[float, int]</code> <p>A number representing the dividend in the division.</p> required <code>b</code> <code>Union[float, int]</code> <p>A number representing the divisor in the division.</p> required <p>Returns:</p> Type Description <code>float</code> <p>A number representing the quotient of <code>a</code> and <code>b</code>.</p> <p>Raises:</p> Type Description <code>ZeroDivisionError</code> <p>An error occurs when the divisor is <code>0</code>.</p> Source code in <code>src/example/calculate/calculations.py</code> <pre><code>def divide(a: Union[float, int], b: Union[float, int]) -&gt; float:\n    \"\"\"Compute and return the quotient of two numbers.\n\n    Examples:\n        &gt;&gt;&gt; divide(4.0, 2.0)\n        2.0\n        &gt;&gt;&gt; divide(4, 2)\n        2.0\n        &gt;&gt;&gt; divide(4, 0)\n        Traceback (most recent call last):\n        ...\n        ZeroDivisionError: division by zero\n\n    Args:\n        a: A number representing the dividend in the division.\n        b: A number representing the divisor in the division.\n\n    Returns:\n        A number representing the quotient of `a` and `b`.\n\n    Raises:\n        ZeroDivisionError: An error occurs when the divisor is `0`.\n    \"\"\"\n    if b == 0:\n        raise ZeroDivisionError(\"division by zero\")\n    return float(a / b)\n</code></pre>"},{"location":"reference/example/calculate/calculations/#src.example.calculate.calculations.multiply","title":"<code>multiply(a, b)</code>","text":"<p>Compute and return the product of two numbers.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; multiply(4.0, 2.0)\n8.0\n&gt;&gt;&gt; multiply(4, 2)\n8.0\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Union[float, int]</code> <p>A number representing the multiplicand in the multiplication.</p> required <code>b</code> <code>Union[float, int]</code> <p>A number representing the multiplier in the multiplication.</p> required <p>Returns:</p> Type Description <code>float</code> <p>A number representing the product of <code>a</code> and <code>b</code>.</p> Source code in <code>src/example/calculate/calculations.py</code> <pre><code>def multiply(a: Union[float, int], b: Union[float, int]) -&gt; float:\n    \"\"\"Compute and return the product of two numbers.\n\n    Examples:\n        &gt;&gt;&gt; multiply(4.0, 2.0)\n        8.0\n        &gt;&gt;&gt; multiply(4, 2)\n        8.0\n\n    Args:\n        a: A number representing the multiplicand in the multiplication.\n        b: A number representing the multiplier in the multiplication.\n\n    Returns:\n        A number representing the product of `a` and `b`.\n    \"\"\"\n    return float(a * b)\n</code></pre>"},{"location":"reference/example/calculate/calculations/#src.example.calculate.calculations.subtract","title":"<code>subtract(a, b)</code>","text":"<p>Calculate the difference of two numbers.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; subtract(4.0, 2.0)\n2.0\n&gt;&gt;&gt; subtract(4, 2)\n2.0\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Union[float, int]</code> <p>A number representing the minuend in the subtraction.</p> required <code>b</code> <code>Union[float, int]</code> <p>A number representing the subtrahend in the subtraction.</p> required <p>Returns:</p> Type Description <code>float</code> <p>A number representing the difference between <code>a</code> and <code>b</code>.</p> Source code in <code>src/example/calculate/calculations.py</code> <pre><code>def subtract(a: Union[float, int], b: Union[float, int]) -&gt; float:\n    \"\"\"Calculate the difference of two numbers.\n\n    Examples:\n        &gt;&gt;&gt; subtract(4.0, 2.0)\n        2.0\n        &gt;&gt;&gt; subtract(4, 2)\n        2.0\n\n    Args:\n        a: A number representing the minuend in the subtraction.\n        b: A number representing the subtrahend in the subtraction.\n\n    Returns:\n        A number representing the difference between `a` and `b`.\n    \"\"\"\n    return float(a - b)\n</code></pre>"},{"location":"reference/example/google_docs/","title":"google_docs","text":"<p>An example of common google doc strings</p> <p>Modules exported by this package:</p> <ul> <li><code>google_docs_eg</code>: Suite of google doc string formats</li> </ul>"},{"location":"reference/example/google_docs/google_docs_eg/","title":"google_docs_eg","text":"<p>Example Google style docstrings.</p> <p>This module demonstrates documentation as specified by the <code>Google Python Style Guide</code>_. Docstrings may extend over multiple lines. Sections are created with a section header and a colon followed by a block of indented text.</p> Example <p>Examples can be given using either the <code>Example</code> or <code>Examples</code> sections. Sections support any reStructuredText formatting, including literal blocks::</p> <pre><code>$ python example_google.py\n</code></pre> <p>Section breaks are created by resuming unindented text. Section breaks are also implicitly created anytime a new section starts.</p> <p>Attributes:</p> Name Type Description <code>module_level_variable1</code> <code>int</code> <p>Module level variables may be documented in either the <code>Attributes</code> section of the module docstring, or in an inline docstring immediately following the variable.</p> <p>Either form is acceptable, but the two should not be mixed. Choose one convention to document module level variables and be consistent with it.</p> Todo <ul> <li>For module TODOs</li> <li>You have to also use <code>sphinx.ext.todo</code> extension</li> </ul> <p>.. _Google Python Style Guide:    http://google.github.io/styleguide/pyguide.html</p>"},{"location":"reference/example/google_docs/google_docs_eg/#src.example.google_docs.google_docs_eg.module_level_variable2","title":"<code>module_level_variable2 = 98765</code>  <code>module-attribute</code>","text":"<p>int: Module level variable documented inline.</p> <p>The docstring may span multiple lines. The type may optionally be specified on the first line, separated by a colon.</p>"},{"location":"reference/example/google_docs/google_docs_eg/#src.example.google_docs.google_docs_eg.ExampleClass","title":"<code>ExampleClass</code>","text":"<p>             Bases: <code>object</code></p> <p>The summary line for a class docstring should fit on one line.</p> <p>If the class has public attributes, they may be documented here in an <code>Attributes</code> section and follow the same formatting as a function's <code>Args</code> section. Alternatively, attributes may be documented inline with the attribute's declaration (see init method below).</p> <p>Properties created with the <code>@property</code> decorator should be documented in the property's getter method.</p> <p>Attributes:</p> Name Type Description <code>attr1</code> <code>str</code> <p>Description of <code>attr1</code>.</p> <code>attr2</code> <p>obj:<code>int</code>, optional): Description of <code>attr2</code>.</p> Source code in <code>src/example/google_docs/google_docs_eg.py</code> <pre><code>class ExampleClass(object):\n    \"\"\"The summary line for a class docstring should fit on one line.\n\n    If the class has public attributes, they may be documented here\n    in an ``Attributes`` section and follow the same formatting as a\n    function's ``Args`` section. Alternatively, attributes may be documented\n    inline with the attribute's declaration (see __init__ method below).\n\n    Properties created with the ``@property`` decorator should be documented\n    in the property's getter method.\n\n    Attributes:\n        attr1 (str): Description of `attr1`.\n        attr2 (:obj:`int`, optional): Description of `attr2`.\n\n    \"\"\"\n\n    def __init__(self, param1, param2, param3):\n        \"\"\"Example of docstring on the __init__ method.\n\n        The __init__ method may be documented in either the class level\n        docstring, or as a docstring on the __init__ method itself.\n\n        Either form is acceptable, but the two should not be mixed. Choose one\n        convention to document the __init__ method and be consistent with it.\n\n        Note:\n            Do not include the `self` parameter in the ``Args`` section.\n\n        Args:\n            param1 (str): Description of `param1`.\n            param2 (:obj:`int`, optional): Description of `param2`. Multiple\n                lines are supported.\n            param3 (:obj:`list` of :obj:`str`): Description of `param3`.\n\n        \"\"\"\n        self.attr1 = param1\n        self.attr2 = param2\n        self.attr3 = param3  #: Doc comment *inline* with attribute\n\n        #: list of str: Doc comment *before* attribute, with type specified\n        self.attr4 = ['attr4']\n\n        self.attr5 = None\n        \"\"\"str: Docstring *after* attribute, with type specified.\"\"\"\n\n    @property\n    def readonly_property(self):\n        \"\"\"str: Properties should be documented in their getter method.\"\"\"\n        return 'readonly_property'\n\n    @property\n    def readwrite_property(self):\n        \"\"\":obj:`list` of :obj:`str`: Properties with both a getter and setter\n        should only be documented in their getter method.\n\n        If the setter method contains notable behavior, it should be\n        mentioned here.\n        \"\"\"\n        return ['readwrite_property']\n\n    @readwrite_property.setter\n    def readwrite_property(self, value):\n        value\n\n    def example_method(self, param1, param2):\n        \"\"\"Class methods are similar to regular functions.\n\n        Note:\n            Do not include the `self` parameter in the ``Args`` section.\n\n        Args:\n            param1: The first parameter.\n            param2: The second parameter.\n\n        Returns:\n            True if successful, False otherwise.\n\n        \"\"\"\n        return True\n\n    def __special__(self):\n        \"\"\"By default special members with docstrings are not included.\n\n        Special members are any methods or attributes that start with and\n        end with a double underscore. Any special member with a docstring\n        will be included in the output, if\n        ``napoleon_include_special_with_doc`` is set to True.\n\n        This behavior can be enabled by changing the following setting in\n        Sphinx's conf.py::\n\n            napoleon_include_special_with_doc = True\n\n        \"\"\"\n        pass\n\n    def __special_without_docstring__(self):\n        pass\n\n    def _private(self):\n        \"\"\"By default private members are not included.\n\n        Private members are any methods or attributes that start with an\n        underscore and are *not* special. By default they are not included\n        in the output.\n\n        This behavior can be changed such that private members *are* included\n        by changing the following setting in Sphinx's conf.py::\n\n            napoleon_include_private_with_doc = True\n\n        \"\"\"\n        pass\n\n    def _private_without_docstring(self):\n        pass\n</code></pre>"},{"location":"reference/example/google_docs/google_docs_eg/#src.example.google_docs.google_docs_eg.ExampleClass.attr5","title":"<code>attr5 = None</code>  <code>instance-attribute</code>","text":"<p>str: Docstring after attribute, with type specified.</p>"},{"location":"reference/example/google_docs/google_docs_eg/#src.example.google_docs.google_docs_eg.ExampleClass.readonly_property","title":"<code>readonly_property</code>  <code>property</code>","text":"<p>str: Properties should be documented in their getter method.</p>"},{"location":"reference/example/google_docs/google_docs_eg/#src.example.google_docs.google_docs_eg.ExampleClass.readwrite_property","title":"<code>readwrite_property</code>  <code>property</code> <code>writable</code>","text":"<p>:obj:<code>list</code> of :obj:<code>str</code>: Properties with both a getter and setter should only be documented in their getter method.</p> <p>If the setter method contains notable behavior, it should be mentioned here.</p>"},{"location":"reference/example/google_docs/google_docs_eg/#src.example.google_docs.google_docs_eg.ExampleClass.__init__","title":"<code>__init__(param1, param2, param3)</code>","text":"<p>Example of docstring on the init method.</p> <p>The init method may be documented in either the class level docstring, or as a docstring on the init method itself.</p> <p>Either form is acceptable, but the two should not be mixed. Choose one convention to document the init method and be consistent with it.</p> Note <p>Do not include the <code>self</code> parameter in the <code>Args</code> section.</p> <p>Parameters:</p> Name Type Description Default <code>param1</code> <code>str</code> <p>Description of <code>param1</code>.</p> required <code>param2</code> <p>obj:<code>int</code>, optional): Description of <code>param2</code>. Multiple lines are supported.</p> required <code>param3</code> <p>obj:<code>list</code> of :obj:<code>str</code>): Description of <code>param3</code>.</p> required Source code in <code>src/example/google_docs/google_docs_eg.py</code> <pre><code>def __init__(self, param1, param2, param3):\n    \"\"\"Example of docstring on the __init__ method.\n\n    The __init__ method may be documented in either the class level\n    docstring, or as a docstring on the __init__ method itself.\n\n    Either form is acceptable, but the two should not be mixed. Choose one\n    convention to document the __init__ method and be consistent with it.\n\n    Note:\n        Do not include the `self` parameter in the ``Args`` section.\n\n    Args:\n        param1 (str): Description of `param1`.\n        param2 (:obj:`int`, optional): Description of `param2`. Multiple\n            lines are supported.\n        param3 (:obj:`list` of :obj:`str`): Description of `param3`.\n\n    \"\"\"\n    self.attr1 = param1\n    self.attr2 = param2\n    self.attr3 = param3  #: Doc comment *inline* with attribute\n\n    #: list of str: Doc comment *before* attribute, with type specified\n    self.attr4 = ['attr4']\n\n    self.attr5 = None\n    \"\"\"str: Docstring *after* attribute, with type specified.\"\"\"\n</code></pre>"},{"location":"reference/example/google_docs/google_docs_eg/#src.example.google_docs.google_docs_eg.ExampleClass.__special__","title":"<code>__special__()</code>","text":"<p>By default special members with docstrings are not included.</p> <p>Special members are any methods or attributes that start with and end with a double underscore. Any special member with a docstring will be included in the output, if <code>napoleon_include_special_with_doc</code> is set to True.</p> <p>This behavior can be enabled by changing the following setting in Sphinx's conf.py::</p> <pre><code>napoleon_include_special_with_doc = True\n</code></pre> Source code in <code>src/example/google_docs/google_docs_eg.py</code> <pre><code>def __special__(self):\n    \"\"\"By default special members with docstrings are not included.\n\n    Special members are any methods or attributes that start with and\n    end with a double underscore. Any special member with a docstring\n    will be included in the output, if\n    ``napoleon_include_special_with_doc`` is set to True.\n\n    This behavior can be enabled by changing the following setting in\n    Sphinx's conf.py::\n\n        napoleon_include_special_with_doc = True\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/example/google_docs/google_docs_eg/#src.example.google_docs.google_docs_eg.ExampleClass.example_method","title":"<code>example_method(param1, param2)</code>","text":"<p>Class methods are similar to regular functions.</p> Note <p>Do not include the <code>self</code> parameter in the <code>Args</code> section.</p> <p>Parameters:</p> Name Type Description Default <code>param1</code> <p>The first parameter.</p> required <code>param2</code> <p>The second parameter.</p> required <p>Returns:</p> Type Description <p>True if successful, False otherwise.</p> Source code in <code>src/example/google_docs/google_docs_eg.py</code> <pre><code>def example_method(self, param1, param2):\n    \"\"\"Class methods are similar to regular functions.\n\n    Note:\n        Do not include the `self` parameter in the ``Args`` section.\n\n    Args:\n        param1: The first parameter.\n        param2: The second parameter.\n\n    Returns:\n        True if successful, False otherwise.\n\n    \"\"\"\n    return True\n</code></pre>"},{"location":"reference/example/google_docs/google_docs_eg/#src.example.google_docs.google_docs_eg.ExampleError","title":"<code>ExampleError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Exceptions are documented in the same way as classes.</p> <p>The init method may be documented in either the class level docstring, or as a docstring on the init method itself.</p> <p>Either form is acceptable, but the two should not be mixed. Choose one convention to document the init method and be consistent with it.</p> Note <p>Do not include the <code>self</code> parameter in the <code>Args</code> section.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>Human readable string describing the exception.</p> required <code>code</code> <p>obj:<code>int</code>, optional): Error code.</p> required <p>Attributes:</p> Name Type Description <code>msg</code> <code>str</code> <p>Human readable string describing the exception.</p> <code>code</code> <code>int</code> <p>Exception error code.</p> Source code in <code>src/example/google_docs/google_docs_eg.py</code> <pre><code>class ExampleError(Exception):\n    \"\"\"Exceptions are documented in the same way as classes.\n\n    The __init__ method may be documented in either the class level\n    docstring, or as a docstring on the __init__ method itself.\n\n    Either form is acceptable, but the two should not be mixed. Choose one\n    convention to document the __init__ method and be consistent with it.\n\n    Note:\n        Do not include the `self` parameter in the ``Args`` section.\n\n    Args:\n        msg (str): Human readable string describing the exception.\n        code (:obj:`int`, optional): Error code.\n\n    Attributes:\n        msg (str): Human readable string describing the exception.\n        code (int): Exception error code.\n\n    \"\"\"\n\n    def __init__(self, msg, code):\n        self.msg = msg\n        self.code = code\n</code></pre>"},{"location":"reference/example/google_docs/google_docs_eg/#src.example.google_docs.google_docs_eg.example_generator","title":"<code>example_generator(n)</code>","text":"<p>Generators have a <code>Yields</code> section instead of a <code>Returns</code> section.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The upper limit of the range to generate, from 0 to <code>n</code> - 1.</p> required <p>Yields:</p> Name Type Description <code>int</code> <p>The next number in the range of 0 to <code>n</code> - 1.</p> <p>Examples:</p> <p>Examples should be written in doctest format, and should illustrate how to use the function.</p> <pre><code>&gt;&gt;&gt; print([i for i in example_generator(4)])\n[0, 1, 2, 3]\n</code></pre> Source code in <code>src/example/google_docs/google_docs_eg.py</code> <pre><code>def example_generator(n):\n    \"\"\"Generators have a ``Yields`` section instead of a ``Returns`` section.\n\n    Args:\n        n (int): The upper limit of the range to generate, from 0 to `n` - 1.\n\n    Yields:\n        int: The next number in the range of 0 to `n` - 1.\n\n    Examples:\n        Examples should be written in doctest format, and should illustrate how\n        to use the function.\n\n        &gt;&gt;&gt; print([i for i in example_generator(4)])\n        [0, 1, 2, 3]\n\n    \"\"\"\n    for i in range(n):\n        yield i\n</code></pre>"},{"location":"reference/example/google_docs/google_docs_eg/#src.example.google_docs.google_docs_eg.function_with_pep484_type_annotations","title":"<code>function_with_pep484_type_annotations(param1, param2)</code>","text":"<p>Example function with PEP 484 type annotations.</p> <p>Parameters:</p> Name Type Description Default <code>param1</code> <code>int</code> <p>The first parameter.</p> required <code>param2</code> <code>str</code> <p>The second parameter.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>The return value. True for success, False otherwise.</p> Source code in <code>src/example/google_docs/google_docs_eg.py</code> <pre><code>def function_with_pep484_type_annotations(param1: int, param2: str) -&gt; bool:\n    \"\"\"Example function with PEP 484 type annotations.\n\n    Args:\n        param1: The first parameter.\n        param2: The second parameter.\n\n    Returns:\n        The return value. True for success, False otherwise.\n\n    \"\"\"\n</code></pre>"},{"location":"reference/example/google_docs/google_docs_eg/#src.example.google_docs.google_docs_eg.function_with_types_in_docstring","title":"<code>function_with_types_in_docstring(param1, param2)</code>","text":"<p>Example function with types documented in the docstring.</p> <p><code>PEP 484</code> type annotations are supported. If attribute, parameter, and return types are annotated according to <code>PEP 484</code>, they do not need to be included in the docstring:</p> <p>Parameters:</p> Name Type Description Default <code>param1</code> <code>int</code> <p>The first parameter.</p> required <code>param2</code> <code>str</code> <p>The second parameter.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>The return value. True for success, False otherwise.</p> <p>.. _PEP 484:     https://www.python.org/dev/peps/pep-0484/</p> Source code in <code>src/example/google_docs/google_docs_eg.py</code> <pre><code>def function_with_types_in_docstring(param1, param2):\n    \"\"\"Example function with types documented in the docstring.\n\n    `PEP 484`_ type annotations are supported. If attribute, parameter, and\n    return types are annotated according to `PEP 484`_, they do not need to be\n    included in the docstring:\n\n    Args:\n        param1 (int): The first parameter.\n        param2 (str): The second parameter.\n\n    Returns:\n        bool: The return value. True for success, False otherwise.\n\n    .. _PEP 484:\n        https://www.python.org/dev/peps/pep-0484/\n\n    \"\"\"\n</code></pre>"},{"location":"reference/example/google_docs/google_docs_eg/#src.example.google_docs.google_docs_eg.module_level_function","title":"<code>module_level_function(param1, param2=None, *args, **kwargs)</code>","text":"<p>This is an example of a module level function.</p> <p>Function parameters should be documented in the <code>Args</code> section. The name of each parameter is required. The type and description of each parameter is optional, but should be included if not obvious.</p> <p>If *args or **kwargs are accepted, they should be listed as <code>*args</code> and <code>**kwargs</code>.</p> <p>The format for a parameter is::</p> <pre><code>name (type): description\n    The description may span multiple lines. Following\n    lines should be indented. The \"(type)\" is optional.\n\n    Multiple paragraphs are supported in parameter\n    descriptions.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>param1</code> <code>int</code> <p>The first parameter.</p> required <code>param2</code> <p>obj:<code>str</code>, optional): The second parameter. Defaults to None. Second line of description should be indented.</p> <code>None</code> <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if successful, False otherwise.</p> <p>The return type is optional and may be specified at the beginning of</p> <p>the <code>Returns</code> section followed by a colon.</p> <p>The <code>Returns</code> section may span multiple lines and paragraphs.</p> <p>Following lines should be indented to match the first line.</p> <p>The <code>Returns</code> section supports any reStructuredText formatting,</p> <p>including literal blocks::</p> <p>{     'param1': param1,     'param2': param2 }</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>The <code>Raises</code> section is a list of all exceptions that are relevant to the interface.</p> <code>ValueError</code> <p>If <code>param2</code> is equal to <code>param1</code>.</p> Source code in <code>src/example/google_docs/google_docs_eg.py</code> <pre><code>def module_level_function(param1, param2=None, *args, **kwargs):\n    \"\"\"This is an example of a module level function.\n\n    Function parameters should be documented in the ``Args`` section. The name\n    of each parameter is required. The type and description of each parameter\n    is optional, but should be included if not obvious.\n\n    If \\*args or \\*\\*kwargs are accepted,\n    they should be listed as ``*args`` and ``**kwargs``.\n\n    The format for a parameter is::\n\n        name (type): description\n            The description may span multiple lines. Following\n            lines should be indented. The \"(type)\" is optional.\n\n            Multiple paragraphs are supported in parameter\n            descriptions.\n\n    Args:\n        param1 (int): The first parameter.\n        param2 (:obj:`str`, optional): The second parameter. Defaults to None.\n            Second line of description should be indented.\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n\n    Returns:\n        bool: True if successful, False otherwise.\n\n        The return type is optional and may be specified at the beginning of\n        the ``Returns`` section followed by a colon.\n\n        The ``Returns`` section may span multiple lines and paragraphs.\n        Following lines should be indented to match the first line.\n\n        The ``Returns`` section supports any reStructuredText formatting,\n        including literal blocks::\n\n            {\n                'param1': param1,\n                'param2': param2\n            }\n\n    Raises:\n        AttributeError: The ``Raises`` section is a list of all exceptions\n            that are relevant to the interface.\n        ValueError: If `param2` is equal to `param1`.\n\n    \"\"\"\n    if param1 == param2:\n        raise ValueError('param1 may not be equal to param2')\n    return True\n</code></pre>"}]}